{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r cluster_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_index</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_index  cluster\n",
       "0            0        3\n",
       "1            1        2\n",
       "2            2        2\n",
       "3            3        3\n",
       "4            4        2\n",
       "..         ...      ...\n",
       "95          95        4\n",
       "96          96        2\n",
       "97          97        2\n",
       "98          98        2\n",
       "99          99        2\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Requirement already satisfied: en-core-web-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl#egg=en_core_web_sm==3.0.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (3.0.0)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.50.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: setuptools in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.25.11)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "%store -r cleanDocx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/himanshusaroha/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(cleanDocx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dfObj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['[\"[\\'available online  .sciencedirect. \\\\ava...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['[\"[\\'expert systems with applications  (2018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['[\"[\\'expert systems with applications  (2018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['[\"[\\'egyptian informatics journal  (xxxx) \\\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['[\"[\\'journal  king saud university – compute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>['[\"[\\'contents lists available  sciencedirect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>['[\"[\\'decision support systems  (2020) 113280...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>['[\"[\\'engineering applications  artiﬁcial int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>['[\"[\\'user identiﬁcation  social networks\\\\th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>['[\"[\\'european journal  operational research ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0   ['[\"[\\'available online  .sciencedirect. \\\\ava...\n",
       "1   ['[\"[\\'expert systems with applications  (2018...\n",
       "2   ['[\"[\\'expert systems with applications  (2018...\n",
       "3   ['[\"[\\'egyptian informatics journal  (xxxx) \\\\...\n",
       "4   ['[\"[\\'journal  king saud university – compute...\n",
       "..                                                ...\n",
       "95  ['[\"[\\'contents lists available  sciencedirect...\n",
       "96  ['[\"[\\'decision support systems  (2020) 113280...\n",
       "97  ['[\"[\\'engineering applications  artiﬁcial int...\n",
       "98  ['[\"[\\'user identiﬁcation  social networks\\\\th...\n",
       "99  ['[\"[\\'european journal  operational research ...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfObj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dfObj1.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['available', 'online', 'sciencedirect', 'available', 'online', 'sciencedirect', 'sciencedirect', 'sciencedirect', 'procedia', 'computer', 'science', 'procedia', 'computer', 'science', 'procedia', 'computer', 'science', 'elsevier', 'locate', 'procedia', 'elsevier', 'locate', 'procedia', 'fifth', 'information', 'systems', 'international', 'conference', 'fifth', 'information', 'systems', 'international', 'conference', 'sentiment', 'analysis', 'social', 'media', 'application', 'systematic', 'sentiment', 'analysis', 'social', 'media', 'application', 'systematic', 'literature', 'review', 'literature', 'review', 'zulfadzli', 'drus', 'haliyana', 'khalid', 'zulfadzli', 'drus', 'haliyana', 'khalid', 'azman', 'hashim', 'international', 'business', 'school', 'kuala', 'lumpur', 'malaysia', 'azman', 'hashim', 'international', 'business', 'school', 'kuala', 'lumpur', 'malaysia', 'abstract', 'abstract', 'this', 'paper', 'report', 'review', 'sentiment', 'analysis', 'social', 'media', 'that', 'explored', 'methods', 'social', 'media', 'platform', 'used', 'this', 'paper', 'report', 'review', 'sentiment', 'analysis', 'social', 'media', 'that', 'explored', 'methods', 'social', 'media', 'platform', 'used', 'application', 'social', 'media', 'contain', 'large', 'amount', 'data', 'that', 'been', 'uploaded', 'users', 'form', 'text', 'videos', 'photos', 'application', 'social', 'media', 'contain', 'large', 'amount', 'data', 'that', 'been', 'uploaded', 'users', 'form', 'text', 'videos', 'photos', 'audio', 'data', 'converted', 'into', 'valuable', 'information', 'using', 'sentiment', 'analysis', 'systematic', 'review', 'studies', 'audio', 'data', 'converted', 'into', 'valuable', 'information', 'using', 'sentiment', 'analysis', 'systematic', 'review', 'studies', 'published', 'between', 'undertaken', 'using', 'following', 'trusted', 'credible', 'database', 'including', 'emerald', 'published', 'between', 'undertaken', 'using', 'following', 'trusted', 'credible', 'database', 'including', 'emerald', 'insight', 'ieee', 'xplore', 'science', 'direct', 'scopus', 'after', 'initial', 'depth', 'screening', 'paper', 'articles', 'have', 'been', 'insight', 'ieee', 'xplore', 'science', 'direct', 'scopus', 'after', 'initial', 'depth', 'screening', 'paper', 'articles', 'have', 'been', 'chosen', 'from', 'review', 'process', 'articles', 'have', 'been', 'reviewed', 'based', 'study', 'result', 'shows', 'most', 'chosen', 'from', 'review', 'process', 'articles', 'have', 'been', 'reviewed', 'based', 'study', 'result', 'shows', 'most', 'articles', 'applied', 'opinion', 'lexicon', 'method', 'analyses', 'text', 'sentiment', 'social', 'media', 'extracted', 'data', 'microblogging', 'site', 'mainly', 'articles', 'applied', 'opinion', 'lexicon', 'method', 'analyses', 'text', 'sentiment', 'social', 'media', 'extracted', 'data', 'microblogging', 'site', 'mainly', 'twitter', 'sentiment', 'analysis', 'application', 'seen', 'world', 'events', 'healthcare', 'politics', 'business', 'twitter', 'sentiment', 'analysis', 'application', 'seen', 'world', 'events', 'healthcare', 'politics', 'business', 'authors', 'published', 'elsevier', 'authors', 'published', 'elsevier', 'authors', 'published', 'elsevier', 'this', 'open', 'access', 'article', 'under', 'license', 'http', 'creativecommons', 'licenses', 'this', 'open', 'access', 'article', 'under', 'license', 'http', 'creativecommons', 'licenses', 'this', 'open', 'access', 'article', 'under', 'license', 'http', 'creativecommons', 'licenses', 'peer', 'review', 'under', 'responsibility', 'scientific', 'committee', 'fifth', 'information', 'systems', 'international', 'conference', 'peer', 'review', 'under', 'responsibility', 'scientific', 'committee', 'fifth', 'information', 'systems', 'international', 'conference', 'peer', 'review', 'under', 'responsibility', 'scientific', 'committee', 'fifth', 'information', 'systems', 'international', 'conference', 'keywords', 'sentiment', 'analysis', 'data', 'social', 'media', 'keywords', 'sentiment', 'analysis', 'data', 'social', 'media', 'introduction', 'introduction', 'emergence', 'changing', 'world', 'social', 'media', 'only', 'online', 'social', 'media', 'used', 'connect', 'emergence', 'changing', 'world', 'social', 'media', 'only', 'online', 'social', 'media', 'used', 'connect', 'share', 'information', 'their', 'personal', 'opinion', 'others', 'even', 'business', 'also', 'communicate', 'understand', 'share', 'information', 'their', 'personal', 'opinion', 'others', 'even', 'business', 'also', 'communicate', 'understand', 'improve', 'their', 'product', 'services', 'through', 'connecting', 'social', 'media', 'number', 'social', 'media', 'users', 'increases', 'improve', 'their', 'product', 'services', 'through', 'connecting', 'social', 'media', 'number', 'social', 'media', 'users', 'increases', 'every', 'estimated', 'there', 'will', 'billion', 'social', 'media', 'users', 'worldwide', 'there', 'various', 'every', 'estimated', 'there', 'will', 'billion', 'social', 'media', 'users', 'worldwide', 'there', 'various', 'corresponding', 'author', 'corresponding', 'author', 'mail', 'address', 'haliyana', 'mail', 'address', 'haliyana', 'authors', 'published', 'elsevier', 'this', 'open', 'access', 'article', 'under', 'license', 'http', 'creativecommons', 'licenses', 'authors', 'published', 'elsevier', 'peer', 'review', 'under', 'responsibility', 'scientific', 'committee', 'fifth', 'information', 'systems', 'international', 'conference', 'this', 'open', 'access', 'article', 'under', 'license', 'http', 'creativecommons', 'licenses', 'peer', 'review', 'under', 'responsibility', 'scientific', 'committee', 'fifth', 'information', 'systems', 'international', 'conference', 'authors', 'published', 'elsevier', 'this', 'open', 'access', 'article', 'under', 'license', 'http', 'creativecommons', 'licenses', 'peer', 'review', 'under', 'responsibility', 'scientific', 'committee', 'fifth', 'information', 'systems', 'international', 'conference', 'procs', 'procs', 'online', 'sciencedirect', 'zulfadzli', 'drus', 'procedia', 'computer', 'science', 'author', 'name', 'procedia', 'computer', 'science', 'author', 'name', 'procedia', 'computer', 'science', 'type', 'information', 'uploaded', 'shared', 'social', 'media', 'form', 'text', 'videos', 'photos', 'audio', 'social', 'media', 'rich', 'with', 'unprocessed', 'data', 'improvement', 'technology', 'especially', 'machine', 'learning', 'artificial', 'intelligence', 'allow', 'data', 'processed', 'converted', 'into', 'useful', 'data', 'that', 'they', 'benefit', 'most', 'business', 'organization', 'this', 'paper', 'focuses', 'provide', 'better', 'understanding', 'application', 'sentiment', 'analysis', 'social', 'media', 'platform', 'examining', 'related', 'literature', 'published', 'between', 'sentiment', 'analysis', 'approach', 'that', 'uses', 'natural', 'language', 'processing', 'extract', 'convert', 'interpret', 'opinion', 'from', 'text', 'classify', 'them', 'into', 'positive', 'negative', 'natural', 'sentiment', 'most', 'previous', 'study', 'applied', 'sentiment', 'analysis', 'into', 'product', 'movie', 'review', 'better', 'understand', 'their', 'customer', 'make', 'necessary', 'decision', 'improve', 'their', 'product', 'services', 'scholars', 'have', 'been', 'conducting', 'study', 'sentiment', 'analysis', 'since', 'last', 'decade', 'which', 'most', 'papers', 'started', 'appear', 'rapidly', 'growing', 'after', 'year', 'sentiment', 'analysis', 'divided', 'into', 'three', 'different', 'levels', 'which', 'sentence', 'level', 'document', 'level', 'feature', 'level', 'purpose', 'classify', 'opinion', 'either', 'from', 'sentence', 'document', 'features', 'into', 'positive', 'negative', 'sentiment', 'there', 'main', 'methods', 'sentiment', 'analysis', 'have', 'been', 'identified', 'which', 'machine', 'learning', 'approach', 'lexicon', 'based', 'approach', 'machine', 'learning', 'approach', 'utilized', 'algorithms', 'extract', 'detect', 'sentiment', 'from', 'data', 'while', 'lexicon', 'based', 'approach', 'works', 'counting', 'positive', 'negative', 'words', 'that', 'related', 'data', 'scholars', 'have', 'been', 'developing', 'effective', 'accurate', 'model', 'sentiment', 'analysis', 'there', 'challenge', 'arise', 'developing', 'model', 'where', 'most', 'design', 'english', 'language', 'recent', 'study', 'shows', 'that', 'there', 'sentiment', 'analysis', 'model', 'design', 'other', 'languages', 'such', 'korean', 'thailand', 'arabic', 'malay', 'portuguese', 'chinese', 'application', 'sentiment', 'analysis', 'reported', 'that', 'been', 'done', 'business', 'marketing', 'politics', 'public', 'action', 'context', 'example', 'application', 'commerce', 'voting', 'application', 'world', 'events', 'most', 'data', 'extracted', 'study', 'extracted', 'from', 'social', 'media', 'social', 'media', 'contain', 'vast', 'amount', 'data', 'from', 'online', 'users', 'information', 'product', 'services', 'place', 'events', 'which', 'makes', 'sentiment', 'analysis', 'study', 'review', 'design', 'systematic', 'review', 'undertaken', 'using', 'steps', 'guidelines', 'conducting', 'systematic', 'literature', 'review', 'management', 'first', 'start', 'defining', 'research', 'question', 'then', 'determine', 'required', 'characteristic', 'study', 'continue', 'retrieving', 'potentially', 'relevant', 'literature', 'selecting', 'pertinent', 'literature', 'then', 'synthesize', 'relevant', 'information', 'from', 'literature', 'final', 'step', 'reporting', 'result', 'review', 'provide', 'overview', 'review', 'following', 'research', 'question', 'addressed', 'uf', 'what', 'method', 'used', 'sentiment', 'analysis', 'social', 'media', 'uf', 'what', 'type', 'social', 'media', 'platform', 'used', 'applied', 'sentiment', 'analysis', 'uf', 'what', 'application', 'context', 'sentiment', 'analysis', 'social', 'media', 'retrieving', 'selecting', 'pertinent', 'literature', 'review', 'utilizing', 'five', 'reputable', 'credible', 'online', 'databases', 'that', 'published', 'literature', 'covering', 'information', 'computer', 'science', 'area', 'search', 'strings', 'keywords', 'used', 'five', 'online', 'databases', 'sentiment', 'analysis', 'social', 'media', 'facebook', 'twitter', 'total', 'articles', 'identified', 'from', 'database', 'search', 'articles', 'articles', 'identified', 'from', 'emerald', 'insight', 'results', 'identified', 'from', 'science', 'direct', 'results', 'from', 'association', 'computing', 'machinery', 'articles', 'from', 'scopus', 'articles', 'identified', 'from', 'ieee', 'then', 'screening', 'papers', 'conducted', 'based', 'inclusion', 'exclusion', 'criteria', 'screening', 'resulted', 'articles', 'consequently', 'screening', 'involved', 'reading', 'full', 'texts', 'analyzing', 'each', 'article', 'obtain', 'finalized', 'articles', 'studies', 'were', 'published', 'between', 'february', 'there', 'total', 'articles', 'selected', 'that', 'suits', 'purpose', 'this', 'review', 'data', 'from', 'paper', 'extracted', 'primary', 'study', 'findings', 'analyzed', 'integrated', 'synthesizing', 'literature', 'into', 'table', 'table', 'summary', 'reviewed', 'literature', 'author', 'title', 'method', 'tools', 'application', 'result', 'yuliyanti', 'djatna', 'sentiment', 'mining', 'community', 'lexicon', 'based', 'success', 'level', 'sukoco', 'development', 'program', 'evaluation', 'based', 'machine', 'community', 'development', 'social', 'media', 'martin', 'domingo', 'social', 'media', 'resource', 'sentiment', 'martin', 'analysis', 'airport', 'service', 'quality', 'mandsberg', 'learning', 'machine', 'learning', 'program', 'analyse', 'airport', 'service', 'quality', 'context', 'twitter', 'twitter', 'account', 'mansour', 'social', 'media', 'analysis', 'user', 'responses', 'lexicon', 'based', 'most', 'user', 'view', 'isis', 'twitter', 'terrorism', 'using', 'sentiment', 'analysis', 'threat', 'fear', 'text', 'mining', 'online', 'saragih', 'girsang', 'sentiment', 'analysis', 'customer', 'lexicon', 'based', 'evaluate', 'business', 'facebook', 'engagement', 'social', 'media', 'transport', 'performance', 'online', 'transport', 'twitter', 'comments', 'hassan', 'hussain', 'sentiment', 'analysis', 'social', 'networking', 'husain', 'sadiq', 'sites', 'data', 'using', 'machine', 'learning', 'machine', 'learning', 'find', 'depression', 'level', 'twitter', 'person', 'newsgroup', 'approach', 'measurement', 'depression', 'joyce', 'deng', 'sentiment', 'analysis', 'tweets', 'lexicon', 'based', 'calculate', 'sentiment', 'twitter', 'presidential', 'election', 'machine', 'expressed', 'compare', 'with', 'learning', 'polling', 'data', 'correlation', 'analyzing', 'sentiments', 'expressed', 'lexicon', 'based', 'analyze', 'energy', 'provider', 'twitter', 'ikoro', 'harmina', 'malik', 'batista', 'navarro', 'twitter', 'energy', 'company', 'consumers', 'social', 'media', 'content', 'sentiment', 'lexicon', 'based', 'security', 'breaches', 'twitter', 'analysis', 'consumer', 'security', 'breaches', 'company', 'sentiment', 'that', 'users', 'show', 'detected', 'early', 'stages', 'prevent', 'further', 'destruction', 'shayaa', 'social', 'media', 'sentiment', 'analysis', 'lexicon', 'based', 'negative', 'sentiment', 'score', 'multiple', 'chung', 'sulaiman', 'employment', 'malaysia', 'employment', 'jaafar', 'zakaria', 'isah', 'trundle', 'neagu', 'dong', 'bouguettaya', 'erradi', 'hadjidj', 'social', 'media', 'analysis', 'product', 'safety', 'lexicon', 'based', 'monitor', 'brand', 'order', 'facebook', 'using', 'text', 'mining', 'sentiment', 'analysis', 'machine', 'even', 'sudden', 'rise', 'comment', 'learning', 'negative', 'sentiment', 'twitter', 'sentiment', 'analysis', 'service', 'social', 'media', 'based', 'sentiment', 'analysis', 'framework', 'machine', 'learning', 'identify', 'location', 'disease', 'outbreaks', 'channel', 'social', 'media', 'twitter', 'reddit', 'instagram', 'news', 'forum', 'author', 'name', 'procedia', 'computer', 'science', 'zulfadzli', 'drus', 'procedia', 'computer', 'science', 'author', 'name', 'procedia', 'computer', 'science', 'type', 'information', 'uploaded', 'shared', 'social', 'media', 'form', 'text', 'videos', 'photos', 'audio', 'social', 'synthesizing', 'literature', 'media', 'rich', 'with', 'unprocessed', 'data', 'improvement', 'technology', 'especially', 'machine', 'learning', 'artificial', 'intelligence', 'allow', 'data', 'processed', 'converted', 'into', 'useful', 'data', 'that', 'they', 'benefit', 'most', 'business', 'organization', 'this', 'paper', 'focuses', 'provide', 'better', 'understanding', 'application', 'sentiment', 'analysis', 'social', 'media', 'platform', 'examining', 'related', 'literature', 'published', 'between', 'sentiment', 'analysis', 'approach', 'that', 'uses', 'natural', 'language', 'processing', 'extract', 'convert', 'interpret', 'opinion', 'from', 'text', 'classify', 'them', 'into', 'positive', 'negative', 'natural', 'sentiment', 'most', 'previous', 'study', 'applied', 'sentiment', 'analysis', 'into', 'product', 'movie', 'review', 'better', 'understand', 'their', 'customer', 'make', 'necessary', 'decision', 'improve', 'their', 'product', 'services', 'scholars', 'have', 'been', 'conducting', 'study', 'sentiment', 'analysis', 'since', 'last', 'decade', 'which', 'most', 'papers', 'started', 'appear', 'rapidly', 'growing', 'after', 'year', 'sentiment', 'analysis', 'divided', 'into', 'three', 'different', 'levels', 'which', 'sentence', 'level', 'document', 'level', 'feature', 'level', 'purpose', 'classify', 'opinion', 'either', 'from', 'sentence', 'document', 'features', 'into', 'positive', 'negative', 'sentiment', 'there', 'main', 'methods', 'sentiment', 'analysis', 'have', 'been', 'identified', 'which', 'machine', 'learning', 'approach', 'lexicon', 'based', 'approach', 'machine', 'learning', 'approach', 'utilized', 'algorithms', 'extract', 'detect', 'sentiment', 'from', 'data', 'while', 'lexicon', 'based', 'approach', 'works', 'counting', 'positive', 'negative', 'words', 'that', 'related', 'data', 'scholars', 'have', 'been', 'developing', 'effective', 'accurate', 'model', 'sentiment', 'analysis', 'there', 'challenge', 'arise', 'developing', 'model', 'where', 'most', 'design', 'english', 'language', 'recent', 'study', 'shows', 'that', 'there', 'sentiment', 'analysis', 'model', 'design', 'other', 'languages', 'such', 'korean', 'thailand', 'arabic', 'malay', 'portuguese', 'chinese', 'application', 'sentiment', 'analysis', 'reported', 'that', 'been', 'done', 'business', 'marketing', 'politics', 'public', 'action', 'context', 'example', 'application', 'commerce', 'voting', 'application', 'world', 'events', 'most', 'data', 'extracted', 'study', 'extracted', 'from', 'social', 'media', 'social', 'media', 'contain', 'vast', 'amount', 'data', 'from', 'online', 'users', 'information', 'product', 'services', 'place', 'events', 'which', 'makes', 'sentiment', 'analysis', 'study', 'review', 'design', 'systematic', 'review', 'undertaken', 'using', 'steps', 'guidelines', 'conducting', 'systematic', 'literature', 'review', 'management', 'first', 'start', 'defining', 'research', 'question', 'then', 'determine', 'required', 'characteristic', 'study', 'continue', 'retrieving', 'potentially', 'relevant', 'literature', 'selecting', 'pertinent', 'literature', 'then', 'synthesize', 'relevant', 'information', 'from', 'literature', 'final', 'step', 'reporting', 'result', 'review', 'provide', 'overview', 'review', 'following', 'research', 'question', 'addressed', 'uf', 'what', 'method', 'used', 'sentiment', 'analysis', 'social', 'media', 'uf', 'what', 'type', 'social', 'media', 'platform', 'used', 'applied', 'sentiment', 'analysis', 'uf', 'what', 'application', 'context', 'sentiment', 'analysis', 'social', 'media', 'retrieving', 'selecting', 'pertinent', 'literature', 'review', 'utilizing', 'five', 'reputable', 'credible', 'online', 'databases', 'that', 'published', 'literature', 'covering', 'information', 'computer', 'science', 'area', 'search', 'strings', 'keywords', 'used', 'five', 'online', 'databases', 'sentiment', 'analysis', 'social', 'media', 'facebook', 'twitter', 'total', 'articles', 'identified', 'from', 'database', 'search', 'articles', 'articles', 'identified', 'from', 'emerald', 'insight', 'results', 'identified', 'from', 'science', 'direct', 'results', 'from', 'association', 'computing', 'machinery', 'articles', 'from', 'scopus', 'articles', 'identified', 'from', 'ieee', 'then', 'screening', 'papers', 'conducted', 'based', 'inclusion', 'exclusion', 'criteria', 'screening', 'resulted', 'articles', 'consequently', 'screening', 'involved', 'reading', 'full', 'texts', 'analyzing', 'each', 'article', 'obtain', 'finalized', 'articles', 'studies', 'were', 'published', 'between', 'february', 'there', 'total', 'articles', 'selected', 'that', 'suits', 'purpose', 'this', 'review', 'data', 'from', 'paper', 'extracted', 'primary', 'study', 'findings', 'analyzed', 'integrated', 'into', 'table', 'table', 'summary', 'reviewed', 'literature', 'author', 'title', 'method', 'tools', 'application', 'result', 'yuliyanti', 'djatna', 'sukoco', 'sentiment', 'mining', 'community', 'development', 'program', 'evaluation', 'based', 'social', 'media', 'lexicon', 'based', 'machine', 'learning', 'success', 'level', 'community', 'development', 'program', 'social', 'media', 'resource', 'sentiment', 'analysis', 'airport', 'service', 'quality', 'machine', 'learning', 'analyse', 'airport', 'service', 'quality', 'context', 'twitter', 'twitter', 'account', 'martin', 'domingo', 'martin', 'mandsberg', 'mansour', 'social', 'media', 'analysis', 'user', 'responses', 'terrorism', 'using', 'sentiment', 'analysis', 'text', 'mining', 'lexicon', 'based', 'most', 'user', 'view', 'isis', 'twitter', 'threat', 'fear', 'saragih', 'girsang', 'sentiment', 'analysis', 'customer', 'engagement', 'social', 'media', 'transport', 'online', 'lexicon', 'based', 'evaluate', 'business', 'performance', 'online', 'transport', 'facebook', 'twitter', 'comments', 'machine', 'learning', 'find', 'depression', 'level', 'person', 'twitter', 'newsgroup', 'hassan', 'hussain', 'husain', 'sadiq', 'sentiment', 'analysis', 'social', 'networking', 'sites', 'data', 'using', 'machine', 'learning', 'approach', 'measurement', 'depression', 'joyce', 'deng', 'sentiment', 'analysis', 'tweets', 'presidential', 'election', 'ikoro', 'harmina', 'malik', 'batista', 'navarro', 'analyzing', 'sentiments', 'expressed', 'twitter', 'energy', 'company', 'consumers', 'social', 'media', 'content', 'sentiment', 'analysis', 'consumer', 'security', 'breaches', 'lexicon', 'based', 'lexicon', 'based', 'machine', 'learning', 'lexicon', 'based', 'calculate', 'sentiment', 'expressed', 'compare', 'with', 'polling', 'data', 'correlation', 'analyze', 'energy', 'provider', 'company', 'sentiment', 'that', 'users', 'show', 'security', 'breaches', 'detected', 'early', 'stages', 'prevent', 'further', 'destruction', 'social', 'media', 'sentiment', 'analysis', 'employment', 'malaysia', 'lexicon', 'based', 'negative', 'sentiment', 'score', 'employment', 'isah', 'trundle', 'neagu', 'social', 'media', 'analysis', 'product', 'safety', 'using', 'text', 'mining', 'sentiment', 'analysis', 'lexicon', 'based', 'machine', 'learning', 'monitor', 'brand', 'order', 'even', 'sudden', 'rise', 'negative', 'sentiment', 'facebook', 'comment', 'twitter', 'sentiment', 'analysis', 'service', 'social', 'media', 'based', 'sentiment', 'analysis', 'framework', 'machine', 'learning', 'identify', 'location', 'disease', 'outbreaks', 'twitter', 'twitter', 'twitter', 'multiple', 'channel', 'social', 'media', 'twitter', 'reddit', 'instagram', 'news', 'forum', 'shayaa', 'chung', 'sulaiman', 'jaafar', 'zakaria', 'dong', 'bouguettaya', 'erradi', 'hadjidj', 'zulfadzli', 'drus', 'procedia', 'computer', 'science', 'author', 'name', 'procedia', 'computer', 'science', 'author', 'name', 'procedia', 'computer', 'science', 'method', 'tools', 'application', 'result', 'reporting', 'result', 'author', 'akter', 'aziz', 'tareq', 'mahtab', 'islam', 'rahaman', 'chedia', 'cynthia', 'jawad', 'hodhod', 'omar', 'fatyanosa', 'bachtiar', 'poecze', 'ebster', 'strauss', 'christine', 'ramanathan', 'meyyappan', 'shahare', 'title', 'sentiment', 'analysis', 'facebook', 'group', 'using', 'lexicon', 'based', 'approach', 'machine', 'learning', 'determine', 'recent', 'trends', 'characteristics', 'people', 'food', 'habit', 'sentiment', 'analysis', 'bangladesh', 'cricket', 'with', 'support', 'vector', 'machine', 'lexicon', 'based', 'machine', 'learning', 'analyze', 'people', 'sentiment', 'expressed', 'towards', 'cricket', 'context', 'facebook', 'group', 'foodbank', 'facebook', 'group', 'bangladesh', 'cricket', 'social', 'media', 'sentiment', 'analysis', 'lexicon', 'versus', 'machine', 'learning', 'lexicon', 'based', 'sentiment', 'analysis', 'consumer', 'generated', 'content', 'facebook', 'brand', 'pages', 'elrahman', 'alotaibi', 'alshehri', 'sentiment', 'analysis', 'twitter', 'data', 'popularity', 'between', 'restaurant', 'mcdonalds', 'twitter', 'sentiment', 'analysis', 'social', 'media', 'networks', 'using', 'machine', 'learning', 'machine', 'learning', 'system', 'provide', 'insight', 'people', 'perception', 'twitter', 'classification', 'method', 'comparison', 'indonesian', 'social', 'media', 'sentiment', 'analysis', 'lexicon', 'based', 'sentiment', 'jakarta', 'governor', 'election', 'twitter', 'karamollaoglu', 'dogru', 'dorterler', 'utku', 'yıldız', 'sentiment', 'analysis', 'turkish', 'social', 'media', 'shares', 'through', 'lexicon', 'based', 'approach', 'lexicon', 'based', 'measure', 'perception', 'twitter', 'influences', 'phenomena', 'machine', 'learning', 'machine', 'learning', 'machine', 'learning', 'social', 'media', 'metrics', 'sentiment', 'analysis', 'evaluate', 'effectiveness', 'social', 'media', 'posts', 'machine', 'learning', 'optimize', 'brand', 'communication', 'understanding', 'consumer', 'feedback', 'facebook', 'page', 'youtube', 'gamers', 'twitter', 'ragini', 'anand', 'bhaskar', 'data', 'analytics', 'disaster', 'response', 'recovery', 'through', 'sentiment', 'analysis', 'lexicon', 'based', 'machine', 'learning', 'sentiment', 'towards', 'needs', 'affected', 'people', 'during', 'disaster', 'twitter', 'text', 'mining', 'sentiment', 'analysis', 'people', 'feedback', 'about', 'oman', 'tourism', 'lexicon', 'based', 'feedback', 'oman', 'tourism', 'twitter', 'sentiment', 'analysis', 'data', 'based', 'social', 'media', 'machine', 'learning', 'process', 'identify', 'emotion', 'level', 'from', 'news', 'data', 'news', 'from', 'blogs', 'vishal', 'extensive', 'study', 'sentiment', 'analysis', 'tools', 'binary', 'classification', 'tweets', 'using', 'rapid', 'miner', 'machine', 'learning', 'identify', 'efficient', 'tool', 'which', 'help', 'enterprise', 'twitter', 'suman', 'gupta', 'sharma', 'analysis', 'stock', 'price', 'flow', 'based', 'social', 'media', 'sentiments', 'machine', 'learning', 'relate', 'flow', 'stock', 'price', 'stock', 'twists', 'sentiment', 'analysis', 'method', 'used', 'social', 'media', 'based', 'papers', 'reviewed', 'paper', 'demonstrated', 'usage', 'either', 'lexicon', 'based', 'method', 'machine', 'learning', 'method', 'both', 'method', 'when', 'implementing', 'sentiment', 'analysis', 'results', 'show', 'conducting', 'sentiment', 'analysis', 'reviewed', 'paper', 'uses', 'lexicon', 'based', 'method', 'papers', 'machine', 'learning', 'papers', 'show', 'combination', 'both', 'methods', 'lexicon', 'based', 'method', 'known', 'unsupervised', 'learning', 'method', 'lexicon', 'method', 'does', 'require', 'training', 'data', 'only', 'depends', 'dictionary', 'most', 'study', 'adapted', 'sentiwordnet', 'method', 'when', 'conducting', 'sentiment', 'analysis', 'this', 'approach', 'calculated', 'based', 'occurrences', 'terms', 'text', 'data', 'with', 'other', 'positive', 'negative', 'words', 'predeveloped', 'polarity', 'lexicons', 'like', 'sentiwordnet', 'method', 'works', 'converting', 'words', 'into', 'number', 'calculated', 'using', 'term', 'frequency', 'inverse', 'document', 'frequency', 'method', 'techniques', 'rely', 'lexical', 'resources', 'effectiveness', 'whole', 'approach', 'strongly', 'depends', 'quality', 'lexical', 'resources', 'based', 'polarity', 'piece', 'text', 'obtained', 'ground', 'polarity', 'words', 'which', 'compose', 'complexity', 'natural', 'languages', 'this', 'approach', 'designed', 'cover', 'aspect', 'language', 'especially', 'when', 'comes', 'slang', 'sarcasm', 'negation', 'using', 'sentiment', 'words', 'sufficient', 'some', 'problems', 'exist', 'such', 'some', 'words', 'have', 'different', 'meaning', 'based', 'application', 'some', 'sentence', 'containing', 'sentiment', 'words', 'express', 'sentiment', 'many', 'sentences', 'without', 'sentiment', 'words', 'also', 'imply', 'opinion', 'however', 'lexicon', 'based', 'method', 'does', 'have', 'advantage', 'such', 'provides', 'simple', 'counting', 'positive', 'negative', 'words', 'flexible', 'with', 'different', 'language', 'speed', 'complete', 'analysis', 'machine', 'learning', 'method', 'falls', 'under', 'supervise', 'learning', 'method', 'requires', 'training', 'data', 'order', 'processed', 'most', 'used', 'method', 'machine', 'learning', 'method', 'naive', 'bayes', 'model', 'different', 'machine', 'learning', 'model', 'these', 'most', 'common', 'used', 'naive', 'bayes', 'successful', 'when', 'applied', 'well', 'formed', 'text', 'corpus', 'while', 'support', 'vector', 'machine', 'gives', 'good', 'performance', 'shape', 'dataset', 'nevertheless', 'machine', 'learning', 'method', 'performs', 'poorly', 'facebook', 'with', 'people', 'post', 'random', 'length', 'lots', 'spelling', 'mistake', 'requires', 'huge', 'amount', 'training', 'sample', 'order', 'adapt', 'method', 'amount', 'dataset', 'will', 'influence', 'size', 'quality', 'output', 'furthermore', 'analyzing', 'with', 'machine', 'learning', 'time', 'consuming', 'where', 'takes', 'hours', 'complex', 'machine', 'learning', 'model', 'especially', 'training', 'required', 'process', 'faster', 'with', 'smaller', 'size', 'training', 'dataset', 'leads', 'poorer', 'classification', 'accuracy', 'interestingly', 'researchers', 'argue', 'that', 'both', 'types', 'analysis', 'method', 'perform', 'very', 'similar', 'terms', 'accuracy', 'there', 'options', 'combine', 'approaches', 'mainly', 'lexicon', 'based', 'sentiment', 'classification', 'that', 'contain', 'sentiment', 'scoring', 'function', 'naive', 'bayes', 'multinomial', 'event', 'models', 'from', 'machine', 'learning', 'approach', 'predict', 'direction', 'sentiment', 'instead', 'relying', 'method', 'studies', 'have', 'proven', 'that', 'combining', 'both', 'methods', 'better', 'efficiency', 'thus', 'order', 'improve', 'outcome', 'recommended', 'combine', 'both', 'methods', 'will', 'complement', 'each', 'other', 'result', 'improved', 'compared', 'using', 'approach', 'only', 'combine', 'approach', 'valuable', 'identify', 'phenomenon', 'also', 'improve', 'handling', 'unstructured', 'data', 'type', 'social', 'media', 'platform', 'extract', 'data', 'sentiment', 'analysis', 'social', 'information', 'services', 'social', 'media', 'categorized', 'into', 'four', 'types', 'based', 'their', 'application', 'usage', 'content', 'communities', 'youtube', 'instagram', 'social', 'networking', 'facebook', 'linkedin', 'blogs', 'reddit', 'quora', 'micro', 'blogs', 'twitter', 'tumblr', 'based', 'reviewed', 'paper', 'among', 'four', 'types', 'social', 'media', 'services', 'micro', 'blogging', 'sites', 'specifically', 'twitter', 'social', 'media', 'platform', 'used', 'collect', 'information', 'user', 'opinion', 'reviewed', 'paper', 'uses', 'twitter', 'collect', 'information', 'sentiment', 'analysis', 'twitter', 'most', 'visited', 'websites', 'enables', 'users', 'post', 'interact', 'with', 'short', 'messages', 'twitter', 'also', 'express', 'their', 'opinion', 'provide', 'very', 'valuable', 'information', 'scholars', 'business', 'organization', 'even', 'government', 'twitter', 'famous', 'microblogging', 'tool', 'social', 'media', 'platform', 'people', 'express', 'their', 'emotion', 'towards', 'particular', 'person', 'event', 'product', 'what', 'makes', 'twitter', 'popular', 'content', 'data', 'that', 'readily', 'available', 'author', 'name', 'procedia', 'computer', 'science', 'zulfadzli', 'drus', 'procedia', 'computer', 'science', 'author', 'name', 'procedia', 'computer', 'science', 'title', 'sentiment', 'analysis', 'facebook', 'group', 'using', 'lexicon', 'based', 'approach', 'machine', 'learning', 'determine', 'recent', 'trends', 'facebook', 'characteristics', 'people', 'food', 'habit', 'group', 'foodbank', 'method', 'tools', 'application', 'result', 'context', 'reporting', 'result', 'sentiment', 'analysis', 'method', 'used', 'social', 'media', 'author', 'akter', 'aziz', 'tareq', 'rahaman', 'chedia', 'cynthia', 'alshehri', 'hodhod', 'omar', 'bachtiar', 'karamollaoglu', 'dogru', 'dorterler', 'utku', 'yıldız', 'strauss', 'christine', 'ragini', 'anand', 'bhaskar', 'ramanathan', 'meyyappan', 'shahare', 'mahtab', 'islam', 'sentiment', 'analysis', 'bangladesh', 'cricket', 'lexicon', 'based', 'analyze', 'people', 'sentiment', 'with', 'support', 'vector', 'machine', 'machine', 'expressed', 'towards', 'cricket', 'facebook', 'group', 'bangladesh', 'cricket', 'social', 'media', 'sentiment', 'analysis', 'lexicon', 'lexicon', 'based', 'sentiment', 'analysis', 'facebook', 'versus', 'machine', 'learning', 'consumer', 'generated', 'content', 'brand', 'pages', 'elrahman', 'alotaibi', 'sentiment', 'analysis', 'twitter', 'data', 'jawad', 'sentiment', 'analysis', 'social', 'media', 'networks', 'using', 'machine', 'learning', 'popularity', 'between', 'restaurant', 'mcdonalds', 'twitter', 'system', 'provide', 'insight', 'twitter', 'people', 'perception', 'fatyanosa', 'classification', 'method', 'comparison', 'lexicon', 'based', 'sentiment', 'jakarta', 'twitter', 'indonesian', 'social', 'media', 'sentiment', 'governor', 'election', 'sentiment', 'analysis', 'turkish', 'social', 'lexicon', 'based', 'measure', 'perception', 'twitter', 'media', 'shares', 'through', 'lexicon', 'based', 'influences', 'phenomena', 'analysis', 'approach', 'poecze', 'ebster', 'social', 'media', 'metrics', 'sentiment', 'analysis', 'evaluate', 'effectiveness', 'social', 'media', 'posts', 'optimize', 'brand', 'communication', 'understanding', 'consumer', 'feedback', 'facebook', 'page', 'youtube', 'gamers', 'data', 'analytics', 'disaster', 'response', 'lexicon', 'based', 'sentiment', 'towards', 'needs', 'twitter', 'recovery', 'through', 'sentiment', 'analysis', 'machine', 'affected', 'people', 'during', 'learning', 'disaster', 'twitter', 'text', 'mining', 'sentiment', 'lexicon', 'based', 'feedback', 'oman', 'tourism', 'twitter', 'analysis', 'people', 'feedback', 'about', 'oman', 'tourism', 'learning', 'machine', 'learning', 'machine', 'learning', 'machine', 'learning', 'machine', 'learning', 'machine', 'learning', 'machine', 'learning', 'machine', 'learning', 'vishal', 'extensive', 'study', 'sentiment', 'analysis', 'tools', 'binary', 'classification', 'tweets', 'using', 'rapid', 'miner', 'identify', 'efficient', 'tool', 'twitter', 'which', 'help', 'enterprise', 'suman', 'gupta', 'sharma', 'analysis', 'stock', 'price', 'flow', 'based', 'social', 'media', 'sentiments', 'machine', 'learning', 'relate', 'flow', 'stock', 'price', 'stock', 'twists', 'based', 'papers', 'reviewed', 'paper', 'demonstrated', 'usage', 'either', 'lexicon', 'based', 'method', 'machine', 'learning', 'method', 'both', 'method', 'when', 'implementing', 'sentiment', 'analysis', 'results', 'show', 'conducting', 'sentiment', 'analysis', 'reviewed', 'paper', 'uses', 'lexicon', 'based', 'method', 'papers', 'machine', 'learning', 'papers', 'show', 'combination', 'both', 'methods', 'lexicon', 'based', 'method', 'known', 'unsupervised', 'learning', 'method', 'lexicon', 'method', 'does', 'require', 'training', 'data', 'only', 'depends', 'dictionary', 'most', 'study', 'adapted', 'sentiwordnet', 'method', 'when', 'conducting', 'sentiment', 'analysis', 'this', 'approach', 'calculated', 'based', 'occurrences', 'terms', 'text', 'data', 'with', 'other', 'positive', 'negative', 'words', 'predeveloped', 'polarity', 'lexicons', 'like', 'sentiwordnet', 'method', 'works', 'converting', 'words', 'into', 'number', 'calculated', 'using', 'term', 'frequency', 'inverse', 'document', 'frequency', 'method', 'techniques', 'rely', 'lexical', 'resources', 'effectiveness', 'whole', 'approach', 'strongly', 'depends', 'quality', 'lexical', 'resources', 'based', 'polarity', 'piece', 'text', 'obtained', 'ground', 'polarity', 'words', 'which', 'compose', 'complexity', 'natural', 'languages', 'this', 'approach', 'designed', 'cover', 'aspect', 'language', 'especially', 'when', 'comes', 'slang', 'sarcasm', 'negation', 'using', 'sentiment', 'words', 'sufficient', 'some', 'problems', 'exist', 'such', 'some', 'words', 'have', 'different', 'meaning', 'based', 'application', 'some', 'sentence', 'containing', 'sentiment', 'words', 'express', 'sentiment', 'many', 'sentences', 'without', 'sentiment', 'words', 'also', 'imply', 'opinion', 'however', 'lexicon', 'based', 'method', 'does', 'have', 'advantage', 'such', 'provides', 'simple', 'counting', 'positive', 'negative', 'words', 'flexible', 'with', 'different', 'language', 'speed', 'complete', 'analysis', 'machine', 'learning', 'method', 'falls', 'under', 'supervise', 'learning', 'method', 'requires', 'training', 'data', 'order', 'processed', 'most', 'used', 'method', 'machine', 'learning', 'method', 'naive', 'bayes', 'model', 'different', 'machine', 'learning', 'model', 'these', 'most', 'common', 'used', 'naive', 'bayes', 'successful', 'when', 'applied', 'well', 'formed', 'text', 'corpus', 'while', 'support', 'vector', 'machine', 'gives', 'good', 'performance', 'shape', 'dataset', 'nevertheless', 'machine', 'learning', 'method', 'performs', 'poorly', 'facebook', 'with', 'people', 'post', 'random', 'length', 'lots', 'spelling', 'mistake', 'requires', 'huge', 'amount', 'training', 'sample', 'order', 'adapt', 'method', 'amount', 'dataset', 'will', 'influence', 'size', 'quality', 'output', 'furthermore', 'analyzing', 'with', 'machine', 'learning', 'time', 'consuming', 'where', 'takes', 'hours', 'complex', 'machine', 'learning', 'model', 'especially', 'training', 'required', 'process', 'faster', 'with', 'smaller', 'size', 'training', 'dataset', 'leads', 'poorer', 'classification', 'accuracy', 'interestingly', 'researchers', 'argue', 'that', 'both', 'types', 'analysis', 'method', 'perform', 'very', 'similar', 'terms', 'accuracy', 'there', 'options', 'combine', 'approaches', 'mainly', 'lexicon', 'based', 'sentiment', 'classification', 'that', 'contain', 'sentiment', 'scoring', 'function', 'naive', 'bayes', 'multinomial', 'event', 'models', 'from', 'machine', 'learning', 'approach', 'predict', 'direction', 'sentiment', 'instead', 'relying', 'method', 'studies', 'have', 'proven', 'that', 'combining', 'both', 'methods', 'better', 'efficiency', 'thus', 'order', 'improve', 'outcome', 'recommended', 'combine', 'both', 'methods', 'will', 'complement', 'each', 'other', 'result', 'improved', 'compared', 'using', 'approach', 'only', 'combine', 'approach', 'valuable', 'identify', 'phenomenon', 'also', 'improve', 'handling', 'unstructured', 'data', 'social', 'information', 'services', 'social', 'media', 'categorized', 'into', 'four', 'types', 'based', 'their', 'application', 'usage', 'content', 'communities', 'youtube', 'instagram', 'social', 'networking', 'facebook', 'linkedin', 'blogs', 'reddit', 'quora', 'micro', 'blogs', 'twitter', 'tumblr', 'based', 'reviewed', 'paper', 'among', 'four', 'types', 'social', 'media', 'services', 'micro', 'blogging', 'sites', 'specifically', 'twitter', 'social', 'media', 'platform', 'used', 'collect', 'information', 'user', 'opinion', 'reviewed', 'paper', 'uses', 'twitter', 'collect', 'information', 'sentiment', 'analysis', 'twitter', 'most', 'visited', 'websites', 'enables', 'users', 'post', 'interact', 'with', 'short', 'messages', 'twitter', 'also', 'express', 'their', 'opinion', 'provide', 'very', 'valuable', 'information', 'scholars', 'business', 'organization', 'even', 'government', 'twitter', 'famous', 'microblogging', 'tool', 'social', 'media', 'platform', 'people', 'express', 'their', 'emotion', 'towards', 'particular', 'person', 'event', 'product', 'what', 'makes', 'twitter', 'popular', 'content', 'data', 'that', 'readily', 'available', 'sentiment', 'analysis', 'data', 'based', 'social', 'media', 'process', 'identify', 'emotion', 'news', 'from', 'level', 'from', 'news', 'data', 'blogs', 'type', 'social', 'media', 'platform', 'extract', 'data', 'sentiment', 'analysis', 'zulfadzli', 'drus', 'procedia', 'computer', 'science', 'author', 'name', 'procedia', 'computer', 'science', 'author', 'name', 'procedia', 'computer', 'science', 'public', 'with', 'usage', 'people', 'access', 'copy', 'data', 'desired', 'topic', 'based', 'keywords', 'hashtag', 'twitter', 'conduct', 'real', 'time', 'analysis', 'closely', 'public', 'sentiment', 'twitter', 'about', 'million', 'tweets', 'allows', 'public', 'access', 'data', 'through', 'twitter', 'used', 'search', 'collect', 'tweet', 'from', 'different', 'countries', 'from', 'western', 'eastern', 'country', 'there', 'twitter', 'user', 'around', 'world', 'thus', 'making', 'rich', 'with', 'opinion', 'views', 'people', 'from', 'different', 'country', 'different', 'language', 'different', 'perception', 'example', 'twitter', 'used', 'collected', 'users', 'tweet', 'particular', 'president', 'candidate', 'during', 'election', 'collected', 'tweets', 'that', 'been', 'written', 'community', 'development', 'program', 'activity', 'moreover', 'twitter', 'also', 'used', 'collect', 'message', 'from', 'customer', 'energy', 'company', 'analyze', 'tweets', 'downloaded', 'from', 'london', 'heathrow', 'airport', 'official', 'twitter', 'account', 'analyzed', 'further', 'using', 'sentiment', 'analysis', 'facebook', 'largest', 'social', 'media', 'users', 'world', 'very', 'popular', 'sentiment', 'analysis', 'data', 'messy', 'structured', 'well', 'people', 'often', 'short', 'forms', 'spelling', 'error', 'this', 'makes', 'data', 'harder', 'analyzed', 'example', 'using', 'facebook', 'twitter', 'fetch', 'pages', 'status', 'updates', 'comments', 'suggesting', 'user', 'experiences', 'study', 'conducted', 'gathered', 'data', 'from', 'various', 'source', 'social', 'media', 'includes', 'forum', 'blogs', 'expedia', 'blog', 'spot', 'mainstream', 'media', 'wordpress', 'youtube', 'twitter', 'aggregator', 'facebook', 'result', 'shows', 'that', 'data', 'comes', 'from', 'twitter', 'other', 'source', 'social', 'media', 'preferable', 'because', 'number', 'data', 'opinions', 'that', 'extracted', 'limited', 'such', 'blogspot', 'youtube', 'wordpress', 'application', 'context', 'sentiment', 'analysis', 'application', 'sentiment', 'analysis', 'ranges', 'from', 'business', 'marketing', 'politic', 'health', 'public', 'action', 'sentiment', 'analysis', 'limited', 'application', 'provides', 'vast', 'application', 'different', 'areas', 'assist', 'decision', 'making', 'sentiment', 'analysis', 'applied', 'world', 'events', 'such', 'event', 'activity', 'sports', 'disaster', 'that', 'occurring', 'world', 'some', 'examples', 'study', 'conducted', 'compare', 'people', 'from', 'western', 'countries', 'eastern', 'countries', 'view', 'isis', 'result', 'shows', 'sides', 'world', 'view', 'isis', 'same', 'which', 'terrorist', 'sentiment', 'analysis', 'also', 'allows', 'raising', 'awareness', 'data', 'security', 'danger', 'security', 'breaches', 'also', 'acts', 'guideline', 'companies', 'respond', 'security', 'breaches', 'shaping', 'public', 'perception', 'furthermore', 'sentiment', 'analysis', 'also', 'conducted', 'unemployment', 'rate', 'employment', 'sentiment', 'score', 'social', 'media', 'application', 'sentiment', 'analysis', 'healthcare', 'where', 'study', 'uses', 'sentiment', 'analysis', 'service', 'framework', 'proposed', 'utilize', 'spatio', 'temporal', 'properties', 'identify', 'locations', 'disease', 'outbreaks', 'addition', 'sentiment', 'analysis', 'identify', 'sentiment', 'needs', 'people', 'during', 'disaster', 'prepare', 'appropriate', 'response', 'rescue', 'moreover', 'sentiment', 'analysis', 'allows', 'finding', 'level', 'depression', 'person', 'overserving', 'analyzing', 'emotions', 'from', 'text', 'sentiment', 'analysis', 'used', 'predict', 'political', 'election', 'where', 'shows', 'data', 'analyzed', 'from', 'twitter', 'more', 'reliable', 'platform', 'where', 'correlation', 'been', 'found', 'polling', 'data', 'have', 'potential', 'become', 'platform', 'that', 'able', 'rival', 'sophisticated', 'polling', 'techniques', 'lastly', 'feedback', 'customer', 'plays', 'utmost', 'important', 'role', 'application', 'sentiment', 'analysis', 'where', 'assist', 'business', 'organization', 'take', 'appropriate', 'action', 'improve', 'their', 'product', 'services', 'business', 'strategy', 'this', 'shown', 'study', 'where', 'concludes', 'views', 'experiences', 'drug', 'cosmetic', 'product', 'among', 'social', 'media', 'users', 'sentiment', 'analysis', 'also', 'allows', 'detecting', 'area', 'that', 'needs', 'improved', 'airport', 'service', 'quality', 'apply', 'proper', 'corrective', 'measures', 'such', 'attention', 'passenger', 'feedback', 'social', 'media', 'then', 'sentiment', 'analysis', 'able', 'analyze', 'trends', 'characteristics', 'people', 'food', 'habit', 'which', 'useful', 'business', 'organization', 'when', 'planning', 'their', 'product', 'marketing', 'strategy', 'sentiment', 'analysis', 'creates', 'advantages', 'business', 'owners', 'identify', 'their', 'popularity', 'among', 'customer', 'customer', 'think', 'about', 'their', 'product', 'service', 'assessing', 'effectiveness', 'capability', 'business', 'brand', 'communication', 'social', 'media', 'evaluate', 'their', 'business', 'flow', 'stock', 'price', 'through', 'social', 'media', 'feedback', 'given', 'consumer', 'important', 'that', 'recognize', 'weakness', 'displayed', 'study', 'that', 'compares', 'sentiment', 'data', 'consumer', 'tweets', 'britain', 'largest', 'oldest', 'electricity', 'supplier', 'entrant', 'energy', 'consumer', 'result', 'indicates', 'that', 'sentiment', 'from', 'more', 'negative', 'than', 'entrant', 'energy', 'consumer', 'addition', 'sentiment', 'analysis', 'social', 'media', 'allows', 'organization', 'evaluate', 'success', 'level', 'program', 'shown', 'study', 'where', 'high', 'positive', 'sentiment', 'obtained', 'from', 'tweet', 'community', 'development', 'program', 'activity', 'result', 'help', 'improve', 'overall', 'living', 'standard', 'community', 'conclusion', 'conducted', 'systematic', 'literature', 'review', 'provides', 'information', 'studies', 'sentiment', 'analysis', 'social', 'media', 'paper', 'makes', 'following', 'three', 'contributions', 'first', 'show', 'what', 'method', 'used', 'analyzing', 'sentiment', 'social', 'media', 'there', 'various', 'method', 'introduced', 'researches', 'still', 'most', 'common', 'method', 'uses', 'lexicon', 'based', 'method', 'sentiwordnet', 'while', 'machine', 'learning', 'naive', 'bayes', 'choosing', 'appropriate', 'method', 'sentiment', 'analysis', 'depending', 'data', 'itself', 'both', 'methods', 'demonstrated', 'similar', 'accuracy', 'things', 'that', 'need', 'take', 'into', 'consideration', 'structure', 'text', 'time', 'amount', 'data', 'data', 'structure', 'messy', 'small', 'amount', 'data', 'limited', 'time', 'available', 'analyses', 'recommended', 'lexicon', 'based', 'method', 'bigger', 'data', 'suitable', 'machine', 'learning', 'based', 'method', 'requires', 'more', 'time', 'data', 'train', 'order', 'improve', 'quality', 'accuracy', 'result', 'suggested', 'combine', 'both', 'lexicon', 'machine', 'learning', 'method', 'second', 'identify', 'what', 'most', 'common', 'type', 'social', 'media', 'site', 'extract', 'information', 'sentiment', 'analysis', 'most', 'popular', 'social', 'media', 'site', 'extract', 'information', 'twitter', 'most', 'reviewed', 'paper', 'twitter', 'their', 'social', 'media', 'context', 'this', 'availability', 'accessibility', 'richness', 'twitter', 'content', 'there', 'millions', 'tweets', 'every', 'almost', 'topic', 'this', 'indicates', 'that', 'social', 'media', 'becoming', 'precious', 'source', 'information', 'however', 'less', 'attention', 'given', 'other', 'social', 'media', 'sources', 'such', 'blogs', 'wordpress', 'youtube', 'others', 'content', 'each', 'social', 'media', 'might', 'different', 'worth', 'exploring', 'other', 'sources', 'might', 'open', 'knowledge', 'findings', 'third', 'demonstrate', 'application', 'sentiment', 'analysis', 'social', 'media', 'sentiment', 'analysis', 'broad', 'application', 'utilized', 'different', 'areas', 'such', 'improving', 'quality', 'strategy', 'business', 'political', 'forecasting', 'election', 'result', 'monitor', 'disease', 'outbreak', 'create', 'awareness', 'importance', 'data', 'security', 'perception', 'towards', 'particular', 'sport', 'improve', 'locate', 'response', 'disaster', 'this', 'shows', 'that', 'sentiment', 'analysis', 'plays', 'huge', 'role', 'understand', 'people', 'perception', 'helps', 'decision', 'making', 'future', 'recommendation', 'further', 'investigation', 'needed', 'develop', 'universal', 'model', 'sentiment', 'analysis', 'that', 'applied', 'different', 'type', 'data', 'explores', 'other', 'potential', 'social', 'networking', 'sites', 'obtain', 'users', 'opinion', 'expanding', 'context', 'sentiment', 'analysis', 'application', 'references', 'statista', 'number', 'social', 'media', 'users', 'worldwide', 'available', 'from', 'https', 'statista', 'statistics', 'number', 'giri', 'kaiser', 'towseef', 'lone', 'data', 'overview', 'challenges', 'international', 'journal', 'advanced', 'research', 'worldwide', 'social', 'network', 'users', 'computer', 'science', 'software', 'engineering', 'sivarajah', 'uthayasankar', 'muhammad', 'mustafa', 'kamal', 'zahir', 'irani', 'vishanth', 'weerakkody', 'critical', 'analysis', 'data', 'challenges', 'analytical', 'methods', 'journal', 'business', 'research', 'agarwal', 'basant', 'namita', 'mittal', 'pooja', 'bansal', 'sonal', 'garg', 'sentiment', 'analysis', 'using', 'common', 'sense', 'context', 'information', 'journal', 'computational', 'intelligence', 'neuroscience', 'gursoy', 'bulut', 'yigit', 'social', 'media', 'mining', 'sentiment', 'analysis', 'brand', 'management', 'global', 'journal', 'emerging', 'trends', 'business', 'marketing', 'consumer', 'psychology', 'mantyla', 'mika', 'daniel', 'graziotin', 'miikka', 'kuutila', 'evolution', 'sentiment', 'analysis', 'review', 'research', 'topics', 'venues', 'cited', 'papers', 'computer', 'science', 'review', 'mishra', 'classification', 'opinion', 'mining', 'techniques', 'international', 'journal', 'computer', 'applications', 'song', 'minchae', 'hyunjung', 'park', 'kyung', 'shik', 'shin', 'attention', 'based', 'long', 'short', 'term', 'memory', 'network', 'using', 'sentiment', 'lexicon', 'embedding', 'aspect', 'level', 'sentiment', 'analysis', 'korean', 'information', 'processing', 'management', 'sanguansat', 'paragraph', 'vec', 'based', 'sentiment', 'analysis', 'social', 'media', 'business', 'thailand', 'international', 'conference', 'knowledge', 'smart', 'technology', 'itani', 'maher', 'chris', 'roast', 'samir', 'khayatt', 'developing', 'resources', 'sentiment', 'analysis', 'informal', 'arabic', 'text', 'social', 'media', 'procedia', 'computer', 'science', 'chekima', 'khalifa', 'rayner', 'alfred', 'sentiment', 'analysis', 'malay', 'social', 'media', 'text', 'author', 'name', 'procedia', 'computer', 'science', 'zulfadzli', 'drus', 'procedia', 'computer', 'science', 'author', 'name', 'procedia', 'computer', 'science', 'public', 'with', 'usage', 'people', 'access', 'copy', 'data', 'desired', 'topic', 'based', 'keywords', 'hashtag', 'twitter', 'conduct', 'real', 'time', 'analysis', 'closely', 'public', 'sentiment', 'twitter', 'about', 'million', 'tweets', 'allows', 'public', 'access', 'data', 'through', 'twitter', 'used', 'search', 'collect', 'tweet', 'from', 'different', 'countries', 'from', 'western', 'eastern', 'country', 'there', 'twitter', 'user', 'around', 'world', 'thus', 'making', 'rich', 'with', 'opinion', 'views', 'people', 'from', 'different', 'country', 'different', 'language', 'different', 'perception', 'example', 'twitter', 'used', 'collected', 'users', 'tweet', 'particular', 'president', 'candidate', 'during', 'election', 'collected', 'tweets', 'that', 'been', 'written', 'community', 'development', 'program', 'activity', 'moreover', 'twitter', 'also', 'used', 'collect', 'message', 'from', 'customer', 'energy', 'company', 'analyze', 'tweets', 'downloaded', 'from', 'london', 'heathrow', 'airport', 'official', 'twitter', 'account', 'analyzed', 'further', 'using', 'sentiment', 'analysis', 'facebook', 'largest', 'social', 'media', 'users', 'world', 'very', 'popular', 'sentiment', 'analysis', 'data', 'messy', 'structured', 'well', 'people', 'often', 'short', 'forms', 'spelling', 'error', 'this', 'makes', 'data', 'harder', 'analyzed', 'example', 'using', 'facebook', 'twitter', 'fetch', 'pages', 'status', 'updates', 'comments', 'suggesting', 'user', 'experiences', 'study', 'conducted', 'gathered', 'data', 'from', 'various', 'source', 'social', 'media', 'includes', 'forum', 'blogs', 'expedia', 'blog', 'spot', 'mainstream', 'media', 'wordpress', 'youtube', 'twitter', 'aggregator', 'facebook', 'result', 'shows', 'that', 'data', 'comes', 'from', 'twitter', 'other', 'source', 'social', 'media', 'preferable', 'because', 'number', 'data', 'opinions', 'that', 'extracted', 'limited', 'such', 'blogspot', 'youtube', 'wordpress', 'application', 'context', 'sentiment', 'analysis', 'application', 'sentiment', 'analysis', 'ranges', 'from', 'business', 'marketing', 'politic', 'health', 'public', 'action', 'sentiment', 'analysis', 'limited', 'application', 'provides', 'vast', 'application', 'different', 'areas', 'assist', 'decision', 'making', 'sentiment', 'analysis', 'applied', 'world', 'events', 'such', 'event', 'activity', 'sports', 'disaster', 'that', 'occurring', 'world', 'some', 'examples', 'study', 'conducted', 'compare', 'people', 'from', 'western', 'countries', 'eastern', 'countries', 'view', 'isis', 'result', 'shows', 'sides', 'world', 'view', 'isis', 'same', 'which', 'terrorist', 'sentiment', 'analysis', 'also', 'allows', 'raising', 'awareness', 'data', 'security', 'danger', 'security', 'breaches', 'also', 'acts', 'guideline', 'companies', 'respond', 'security', 'breaches', 'shaping', 'public', 'perception', 'furthermore', 'sentiment', 'analysis', 'also', 'conducted', 'unemployment', 'rate', 'employment', 'sentiment', 'score', 'social', 'media', 'application', 'sentiment', 'analysis', 'healthcare', 'where', 'study', 'uses', 'sentiment', 'analysis', 'service', 'framework', 'proposed', 'utilize', 'spatio', 'temporal', 'properties', 'identify', 'locations', 'disease', 'outbreaks', 'addition', 'sentiment', 'analysis', 'identify', 'sentiment', 'needs', 'people', 'during', 'disaster', 'prepare', 'appropriate', 'response', 'rescue', 'moreover', 'sentiment', 'analysis', 'allows', 'finding', 'level', 'depression', 'person', 'overserving', 'analyzing', 'emotions', 'from', 'text', 'sentiment', 'analysis', 'used', 'predict', 'political', 'election', 'where', 'shows', 'data', 'analyzed', 'from', 'twitter', 'more', 'reliable', 'platform', 'where', 'correlation', 'been', 'found', 'polling', 'data', 'have', 'potential', 'become', 'platform', 'that', 'able', 'rival', 'sophisticated', 'polling', 'techniques', 'lastly', 'feedback', 'customer', 'plays', 'utmost', 'important', 'role', 'application', 'sentiment', 'analysis', 'where', 'assist', 'business', 'organization', 'take', 'appropriate', 'action', 'improve', 'their', 'product', 'services', 'business', 'strategy', 'this', 'shown', 'study', 'where', 'concludes', 'views', 'experiences', 'drug', 'cosmetic', 'product', 'among', 'social', 'media', 'users', 'sentiment', 'analysis', 'also', 'allows', 'detecting', 'area', 'that', 'needs', 'improved', 'airport', 'service', 'quality', 'apply', 'proper', 'corrective', 'measures', 'such', 'attention', 'passenger', 'feedback', 'social', 'media', 'then', 'sentiment', 'analysis', 'able', 'analyze', 'trends', 'characteristics', 'people', 'food', 'habit', 'which', 'useful', 'business', 'organization', 'when', 'planning', 'their', 'product', 'marketing', 'strategy', 'sentiment', 'analysis', 'creates', 'advantages', 'business', 'owners', 'identify', 'their', 'popularity', 'among', 'customer', 'customer', 'think', 'about', 'their', 'product', 'service', 'assessing', 'effectiveness', 'capability', 'business', 'brand', 'communication', 'social', 'media', 'evaluate', 'their', 'business', 'flow', 'stock', 'price', 'through', 'social', 'media', 'feedback', 'given', 'consumer', 'important', 'that', 'recognize', 'weakness', 'displayed', 'study', 'that', 'compares', 'sentiment', 'data', 'consumer', 'tweets', 'britain', 'largest', 'oldest', 'electricity', 'supplier', 'entrant', 'energy', 'consumer', 'result', 'indicates', 'that', 'sentiment', 'from', 'more', 'negative', 'than', 'entrant', 'energy', 'consumer', 'addition', 'sentiment', 'analysis', 'social', 'media', 'allows', 'organization', 'evaluate', 'success', 'level', 'program', 'shown', 'study', 'where', 'high', 'positive', 'sentiment', 'obtained', 'from', 'tweet', 'community', 'development', 'program', 'activity', 'result', 'help', 'improve', 'overall', 'living', 'standard', 'community', 'conclusion', 'conducted', 'systematic', 'literature', 'review', 'provides', 'information', 'studies', 'sentiment', 'analysis', 'social', 'media', 'paper', 'makes', 'following', 'three', 'contributions', 'first', 'show', 'what', 'method', 'used', 'analyzing', 'sentiment', 'social', 'media', 'there', 'various', 'method', 'introduced', 'researches', 'still', 'most', 'common', 'method', 'uses', 'lexicon', 'based', 'method', 'sentiwordnet', 'while', 'machine', 'learning', 'naive', 'bayes', 'choosing', 'appropriate', 'method', 'sentiment', 'analysis', 'depending', 'data', 'itself', 'both', 'methods', 'demonstrated', 'similar', 'accuracy', 'things', 'that', 'need', 'take', 'into', 'consideration', 'structure', 'text', 'time', 'amount', 'data', 'data', 'structure', 'messy', 'small', 'amount', 'data', 'limited', 'time', 'available', 'analyses', 'recommended', 'lexicon', 'based', 'method', 'bigger', 'data', 'suitable', 'machine', 'learning', 'based', 'method', 'requires', 'more', 'time', 'data', 'train', 'order', 'improve', 'quality', 'accuracy', 'result', 'suggested', 'combine', 'both', 'lexicon', 'machine', 'learning', 'method', 'second', 'identify', 'what', 'most', 'common', 'type', 'social', 'media', 'site', 'extract', 'information', 'sentiment', 'analysis', 'most', 'popular', 'social', 'media', 'site', 'extract', 'information', 'twitter', 'most', 'reviewed', 'paper', 'twitter', 'their', 'social', 'media', 'context', 'this', 'availability', 'accessibility', 'richness', 'twitter', 'content', 'there', 'millions', 'tweets', 'every', 'almost', 'topic', 'this', 'indicates', 'that', 'social', 'media', 'becoming', 'precious', 'source', 'information', 'however', 'less', 'attention', 'given', 'other', 'social', 'media', 'sources', 'such', 'blogs', 'wordpress', 'youtube', 'others', 'content', 'each', 'social', 'media', 'might', 'different', 'worth', 'exploring', 'other', 'sources', 'might', 'open', 'knowledge', 'findings', 'third', 'demonstrate', 'application', 'sentiment', 'analysis', 'social', 'media', 'sentiment', 'analysis', 'broad', 'application', 'utilized', 'different', 'areas', 'such', 'improving', 'quality', 'strategy', 'business', 'political', 'forecasting', 'election', 'result', 'monitor', 'disease', 'outbreak', 'create', 'awareness', 'importance', 'data', 'security', 'perception', 'towards', 'particular', 'sport', 'improve', 'locate', 'response', 'disaster', 'this', 'shows', 'that', 'sentiment', 'analysis', 'plays', 'huge', 'role', 'understand', 'people', 'perception', 'helps', 'decision', 'making', 'future', 'recommendation', 'further', 'investigation', 'needed', 'develop', 'universal', 'model', 'sentiment', 'analysis', 'that', 'applied', 'different', 'type', 'data', 'explores', 'other', 'potential', 'social', 'networking', 'sites', 'obtain', 'users', 'opinion', 'expanding', 'context', 'sentiment', 'analysis', 'application', 'references', 'statista', 'number', 'social', 'media', 'users', 'worldwide', 'available', 'from', 'https', 'statista', 'statistics', 'number', 'giri', 'kaiser', 'towseef', 'lone', 'data', 'overview', 'challenges', 'international', 'journal', 'advanced', 'research', 'worldwide', 'social', 'network', 'users', 'computer', 'science', 'software', 'engineering', 'sivarajah', 'uthayasankar', 'muhammad', 'mustafa', 'kamal', 'zahir', 'irani', 'vishanth', 'weerakkody', 'critical', 'analysis', 'data', 'challenges', 'analytical', 'methods', 'journal', 'business', 'research', 'agarwal', 'basant', 'namita', 'mittal', 'pooja', 'bansal', 'sonal', 'garg', 'sentiment', 'analysis', 'using', 'common', 'sense', 'context', 'information', 'journal', 'computational', 'intelligence', 'neuroscience', 'gursoy', 'bulut', 'yigit', 'social', 'media', 'mining', 'sentiment', 'analysis', 'brand', 'management', 'global', 'journal', 'emerging', 'trends', 'business', 'marketing', 'consumer', 'psychology', 'mantyla', 'mika', 'daniel', 'graziotin', 'miikka', 'kuutila', 'evolution', 'sentiment', 'analysis', 'review', 'research', 'topics', 'venues', 'cited', 'papers', 'computer', 'science', 'review', 'mishra', 'classification', 'opinion', 'mining', 'techniques', 'international', 'journal', 'computer', 'applications', 'song', 'minchae', 'hyunjung', 'park', 'kyung', 'shik', 'shin', 'attention', 'based', 'long', 'short', 'term', 'memory', 'network', 'using', 'sentiment', 'lexicon', 'embedding', 'aspect', 'level', 'sentiment', 'analysis', 'korean', 'information', 'processing', 'management', 'sanguansat', 'paragraph', 'vec', 'based', 'sentiment', 'analysis', 'social', 'media', 'business', 'thailand', 'international', 'conference', 'knowledge', 'smart', 'technology', 'itani', 'maher', 'chris', 'roast', 'samir', 'khayatt', 'developing', 'resources', 'sentiment', 'analysis', 'informal', 'arabic', 'text', 'social', 'media', 'procedia', 'computer', 'science', 'chekima', 'khalifa', 'rayner', 'alfred', 'sentiment', 'analysis', 'malay', 'social', 'media', 'text', 'zulfadzli', 'drus', 'procedia', 'computer', 'science', 'author', 'name', 'procedia', 'computer', 'science', 'cirqueira', 'fontes', 'pinheiro', 'jacob', 'lobato', 'santana', 'literature', 'review', 'preprocessing', 'sentiment', 'analysis', 'brazilian', 'portuguese', 'social', 'media', 'ieee', 'international', 'conference', 'intelligence', 'peng', 'haiyun', 'erik', 'cambria', 'amir', 'hussain', 'review', 'sentiment', 'analysis', 'research', 'chinese', 'language', 'cognitive', 'computation', 'ieee', 'ebrahimi', 'yazdavar', 'sheth', 'challenges', 'sentiment', 'analysis', 'dynamic', 'events', 'intelligent', 'systems', 'durach', 'christian', 'joakim', 'kembro', 'andreas', 'paradigm', 'systematic', 'literature', 'reviews', 'supply', 'chain', 'management', 'journal', 'supply', 'chain', 'management', 'wieland', 'bijoyan', 'sarit', 'chakraborty', 'improved', 'text', 'sentiment', 'classification', 'model', 'using', 'next', 'word', 'negation', 'khan', 'muhammad', 'taimoor', 'mehr', 'durrani', 'armughan', 'irum', 'inayat', 'shehzad', 'khalid', 'kamran', 'habib', 'khan', 'sentiment', 'analysis', 'complex', 'natural', 'language', 'complex', 'adaptive', 'systems', 'modeling', 'akter', 'sanjida', 'muhammad', 'tareq', 'aziz', 'sentiment', 'analysis', 'facebook', 'group', 'using', 'lexicon', 'based', 'approach', 'international', 'conference', 'electrical', 'engineering', 'information', 'communication', 'technology', 'iceeict', 'hassan', 'anees', 'jamil', 'hussain', 'musarrat', 'hussain', 'muhammad', 'sadiq', 'sungyoung', 'sentiment', 'analysis', 'social', 'networking', 'sites', 'data', 'using', 'machine', 'learning', 'approach', 'measurement', 'depression', 'international', 'conference', 'information', 'communication', 'technology', 'convergence', 'ictc', 'jeju', 'south', 'korea', 'ieee', 'mahtab', 'arafin', 'islam', 'mahfuzur', 'rahaman', 'sept', 'sentiment', 'analysis', 'bangladesh', 'cricket', 'with', 'support', 'vector', 'machine', 'international', 'conference', 'bangla', 'speech', 'language', 'processing', 'icbslp', 'dhaoui', 'chedia', 'cynthia', 'webster', 'peng', 'social', 'media', 'sentiment', 'analysis', 'lexicon', 'versus', 'machine', 'learning', 'journal', 'consumer', 'marketing', 'rahman', 'alotaibi', 'alshehri', 'april', 'sentiment', 'analysis', 'twitter', 'data', 'international', 'conference', 'computer', 'information', 'sciences', 'iccis', 'kashif', 'dong', 'athman', 'bouguettaya', 'abdelkarim', 'erradi', 'rachid', 'hadjidj', 'sentiment', 'analysis', 'service', 'social', 'media', 'based', 'sentiment', 'analysis', 'framework', 'ieee', 'international', 'conference', 'services', 'icws', 'honolulu', 'ieee', 'jianqiang', 'hongying', 'social', 'media', 'content', 'sentiment', 'analysis', 'consumer', 'security', 'breaches', 'journal', 'financial', 'crime', 'computer', 'science', 'mansour', 'samah', 'social', 'media', 'analysis', 'user', 'responses', 'terrorism', 'using', 'sentiment', 'analysis', 'text', 'mining', 'procedia', 'joyce', 'brandon', 'jing', 'deng', 'sentiment', 'analysis', 'tweets', 'presidential', 'election', 'ieee', 'undergraduate', 'research', 'technology', 'conference', 'urtc', 'cambridge', 'ieee', 'yuliyanti', 'siti', 'djatna', 'sukoco', 'taufik', 'heru', 'sentiment', 'mining', 'community', 'development', 'program', 'evaluation', 'based', 'social', 'media', 'telkomnika', 'computing', 'electronics', 'control', 'ikoro', 'victoria', 'maria', 'sharmina', 'khaleel', 'malik', 'riza', 'batista', 'navarro', 'analyzing', 'sentiments', 'expressed', 'twitter', 'energy', 'company', 'consumers', 'fifth', 'international', 'conference', 'social', 'networks', 'analysis', 'management', 'security', 'snams', 'ieee', 'martin', 'domingo', 'luis', 'juan', 'carlos', 'martin', 'glen', 'mandsberg', 'social', 'media', 'resource', 'sentiment', 'analysis', 'airport', 'service', 'quality', 'journal', 'transport', 'management', 'isah', 'haruna', 'paul', 'trundle', 'daniel', 'neagu', 'social', 'media', 'analysis', 'product', 'safety', 'using', 'text', 'mining', 'sentiment', 'analysis', 'th', 'workshop', 'computational', 'intelligence', 'ukci', 'ieee', 'shayaa', 'shahid', 'phoong', 'seuk', 'yeong', 'chung', 'ainin', 'sulaiman', 'noor', 'ismawati', 'jaafar', 'shamshul', 'bahri', 'zakaria', 'social', 'media', 'sentiment', 'analysis', 'employment', 'malaysia', 'proceedings', 'global', 'business', 'finance', 'research', 'conference', 'taipei', 'taiwan', 'karamollaoglu', 'dogru', 'dorterler', 'utku', 'yıldız', 'sept', 'sentiment', 'analysis', 'turkish', 'social', 'media', 'shares', 'through', 'lexicon', 'based', 'approach', 'international', 'conference', 'computer', 'science', 'engineering', 'ragini', 'rexiline', 'rubesh', 'anand', 'vidhyacharan', 'bhaskar', 'data', 'analytics', 'disaster', 'response', 'recovery', 'through', 'sentiment', 'analysis', 'international', 'journal', 'information', 'management', 'poecze', 'flora', 'claus', 'ebster', 'christine', 'strauss', 'social', 'media', 'metrics', 'sentiment', 'analysis', 'evaluate', 'effectiveness', 'social', 'media', 'posts', 'procedia', 'computer', 'science', 'suman', 'gupta', 'sharma', 'analysis', 'stock', 'price', 'flow', 'based', 'social', 'media', 'sentiments', 'international', 'conference', 'next', 'generation', 'computing', 'information', 'systems', 'icngcis']]\n"
     ]
    }
   ],
   "source": [
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): #'NOUN', 'ADJ', 'VERB', 'ADV'\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sciencedirect sciencedirect sciencedirect sciencedirect procedia computer science procedia computer science procedia computer science procedia procedia information system conference information system conference sentiment analysis medium application sentiment analysis medium application literature review literature review zulfadzli drus haliyana khalid zulfadzli drus haliyana khalid azman business school kuala lumpur malaysia azman business school kuala lumpur malaysia paper report review sentiment analysis medium explore method medium platform use paper report review sentiment analysis medium explore method medium platform use application medium contain amount datum upload user form text video photo application medium contain amount datum upload user form text video photo datum convert information use sentiment analysis review study datum convert information use sentiment analysis review study publish undertake use follow trust database include emerald publish undertake use follow trust database include emerald insight ieee xplore science scopus depth screen paper article insight ieee xplore science scopus depth screen paper article choose review process article review base study result show choose review process article review base study result show article apply opinion lexicon method analyse text sentiment medium extract datum microblogging site article apply opinion lexicon method analyse text sentiment medium extract datum microblogging site twitter sentiment analysis application see world event healthcare politic business twitter sentiment analysis application see world event healthcare politic business author publish author publish author publish access article license http creativecommon license access article license http creativecommon license access article license http creativecommon license peer review responsibility committee information system conference peer review responsibility committee information system conference peer review responsibility committee information system conference keyword sentiment analysis datum medium keyword sentiment analysis datum medium introduction introduction emergence change world medium medium use connect emergence change world medium medium use connect share information opinion other business communicate understand share information opinion other business communicate understand improve product service connect medium number medium user increase improve product service connect medium number medium user increase estimate medium user estimate medium user author correspond author mail address haliyana mail address haliyana author publish access article license http creativecommon license author publish peer review responsibility committee information system conference access article license http creativecommon license peer review responsibility committee information system conference author publish access article license http creativecommon license peer review responsibility committee information system conference proc proc sciencedirect zulfadzli drus procedia computer science author name procedia computer science author name procedia computer science type information upload share medium form text video photo medium data improvement technology machine learn intelligence allow datum process convert datum benefit business organization paper focus provide understand application sentiment analysis medium platform examine relate literature publish sentiment analysis approach use language processing extract convert opinion text classify sentiment study apply sentiment analysis product movie review understand customer make decision improve product service scholar conduct study sentiment analysis decade paper start appear grow year sentiment analysis divide level sentence level document level feature level purpose classify opinion sentence document feature sentiment method sentiment analysis identify machine learning approach lexicon base approach machine learning approach utilize algorithm extract detect sentiment datum lexicon base approach work count word datum scholar develop model sentiment analysis challenge arise develop model design language study show sentiment analysis model design language thailand malay application sentiment analysis report do business marketing politic action context example application commerce voting application world event datum extract study extract medium medium contain amount datum user information product service place event make sentiment analysis study review design review undertake use step guideline conduct literature review management start define research question determine require study continue retrieve literature select literature synthesize information literature step reporting result review provide overview review follow research question address method use sentiment analysis medium type medium platform use sentiment analysis application context sentiment analysis medium retrieve select literature review utilize online database publish literature cover information computer science area search string keyword use database sentiment analysis medium facebook twitter article identify database search article article identify insight result identify science result association compute machinery article scopus article identify ieee screen paper conduct base inclusion exclusion criterion screen result article screen involve read text analyze article obtain finalize article study publish article select suit purpose review datum paper extract study finding analyze integrate synthesize literature table table summary review literature author title method tool application result djatna sentiment mining community lexicon base success level development program evaluation base machine community development medium medium resource sentiment martin analysis airport service quality learning machine learning program analyse airport service quality context twitter twitter account mansour medium analysis user response lexicon base user view isis twitter terrorism use sentiment analysis threat fear text mining saragih girsang sentiment analysis customer lexicon base evaluate business facebook engagement medium transport performance transport twitter comment hussain sentiment analysis networking husain sadiq site datum use machine learn machine learning find depression level twitter person newsgroup approach measurement depression joyce deng sentiment analysis tweet lexicon base calculate sentiment twitter election machine express compare learn polling data correlation analyze sentiment express lexicon base analyze energy provider twitter harmina malik batista navarro twitter energy company consumer medium content sentiment lexicon base security breach twitter analysis consumer security breach company sentiment user show detect stage prevent destruction medium sentiment analysis lexicon base sentiment score sulaiman employment malaysia employment zakaria isah trundle bouguettaya erradi hadjidj medium analysis product safety lexicon base monitor brand order facebook use text mining sentiment analysis machine rise comment learn sentiment twitter sentiment analysis service medium base sentiment analysis framework machine learning identify location disease outbreak channel medium twitter reddit instagram news forum author name procedia computer science zulfadzli drus procedia computer science author name procedia computer science type information upload share medium form text video photo synthesize literature medium data improvement technology machine learn intelligence allow datum process convert datum benefit business organization paper focus provide understand application sentiment analysis medium platform examine relate literature publish sentiment analysis approach use language processing extract convert opinion text classify sentiment study apply sentiment analysis product movie review understand customer make decision improve product service scholar conduct study sentiment analysis decade paper start appear grow year sentiment analysis divide level sentence level document level feature level purpose classify opinion sentence document feature sentiment method sentiment analysis identify machine learning approach lexicon base approach machine learning approach utilize algorithm extract detect sentiment datum lexicon base approach work count word datum scholar develop model sentiment analysis challenge arise develop model design language study show sentiment analysis model design language thailand malay application sentiment analysis report do business marketing politic action context example application commerce voting application world event datum extract study extract medium medium contain amount datum user information product service place event make sentiment analysis study review design review undertake use step guideline conduct literature review management start define research question determine require study continue retrieve literature select literature synthesize information literature step reporting result review provide overview review follow research question address method use sentiment analysis medium type medium platform use sentiment analysis application context sentiment analysis medium retrieve select literature review utilize online database publish literature cover information computer science area search string keyword use database sentiment analysis medium facebook twitter article identify database search article article identify insight result identify science result association compute machinery article scopus article identify ieee screen paper conduct base inclusion exclusion criterion screen result article screen involve read text analyze article obtain finalize article study publish article select suit purpose review datum paper extract study finding analyze integrate table table summary review literature author title method tool application result djatna sentiment mine community development program evaluation base medium lexicon base machine learn success level community development program medium resource sentiment analysis airport service quality machine learning analyse airport service quality context twitter twitter account domingo mansour medium analysis user response terrorism use sentiment analysis text mining lexicon base user view isis twitter threat fear saragih girsang sentiment analysis customer engagement medium transport lexicon base evaluate business performance transport facebook twitter comment machine learning find depression level person twitter newsgroup hassan hussain husain sadiq sentiment analysis networking site datum use machine learning approach measurement depression joyce deng sentiment analysis tweet election harmina malik batista navarro analyze sentiment express twitter energy company consumer medium content sentiment analysis consumer security breach lexicon base lexicon base machine learning lexicon base calculate sentiment express compare polling datum correlation analyze energy provider company sentiment user show security breach detect stage prevent destruction medium sentiment analysis employment malaysia lexicon base sentiment score employment isah trundle medium analysis product safety use text mining sentiment analysis lexicon base machine learning monitor brand order rise sentiment facebook comment twitter sentiment analysis service medium base sentiment analysis framework machine learning identify location disease outbreak twitter twitter twitter channel medium twitter reddit instagram news forum shayaa sulaiman zakaria bouguettaya erradi hadjidj zulfadzli drus procedia computer science author name procedia computer science author name procedia computer science method tool application result reporting result author akter tareq mahtab islam rahaman chedia cynthia omar fatyanosa poecze ebster strauss christine ramanathan meyyappan shahare title sentiment analysis facebook group use lexicon base approach machine learning determine trend characteristic people food habit sentiment analysis bangladesh cricket support vector machine lexicon base machine learning analyze people sentiment express cricket context facebook group foodbank facebook group bangladesh cricket medium sentiment analysis lexicon machine learning lexicon base sentiment analysis consumer generate content facebook brand page elrahman alotaibi alshehri sentiment analysis twitter datum popularity restaurant mcdonald twitter sentiment analysis medium network use machine learn machine learning system provide insight people perception twitter classification method comparison medium sentiment analysis lexicon base sentiment jakarta governor election twitter karamollaoglu dogru dorterler utku yıldız sentiment analysis medium share lexicon base approach lexicon base measure perception twitter influence phenomena machine learn machine learn machine learn medium metric sentiment analysis evaluate effectiveness medium post machine learn optimize brand communication understand consumer feedback facebook page youtube gamer twitter ragini anand datum analytic disaster response recovery sentiment analysis lexicon base machine learning sentiment need affect people disaster twitter text mining sentiment analysis people feedback oman tourism lexicon base feedback oman tourism twitter sentiment analysis datum base medium machine learning process identify emotion level news datum news blog study sentiment analysis tool classification tweet use miner machine learning identify tool help enterprise twitter suman gupta sharma analysis stock price flow base medium sentiment machine learning relate flow stock price stock twist sentiment analysis method use medium base paper review paper demonstrate usage lexicon base method machine learning method method implement sentiment analysis result show conduct sentiment analysis review paper use lexicon base method paper machine learning paper show combination method lexicon base method know learning method lexicon method require training datum depend study adapt sentiwordnet method conduct sentiment analysis approach calculate base occurrence term text datum word predevelope polarity lexicon sentiwordnet method work convert word number calculate use term frequency inverse document frequency method technique rely resource effectiveness approach depend quality resource base polarity piece text obtain ground polarity word compose complexity language approach design cover aspect language come sarcasm negation use sentiment word problem exist word have meaning base application sentence contain sentiment word express sentiment sentence sentiment word imply opinion lexicon base method have advantage provide count word language speed analysis machine learning method fall supervise learning method require training datum order process use method machine learning method bayes model machine learning model use bayes apply form text corpus support vector machine give performance shape dataset machine learning method perform facebook people post length lot spelling mistake require amount train sample order adapt method amount dataset influence size quality output analyze machine learning time consume take hour machine learning model train require process size train dataset lead classification accuracy researcher argue type analysis method perform term accuracy option combine approach base sentiment classification contain sentiment score function bayes event model machine learning approach predict direction sentiment rely method study prove combine method efficiency order improve outcome recommend combine method complement result improve compare use approach combine approach identify phenomenon improve handle data type medium platform extract datum sentiment analysis information service medium categorize type base application usage content community youtube instagram networking facebook linkedin blog reddit quora micro blog twitter tumblr base review paper type medium service blogging site twitter medium platform use collect information user opinion review paper use twitter collect information sentiment analysis twitter website enable user post interact message twitter express opinion provide information scholar business organization government twitter microblogging tool medium platform people express emotion person event product make twitter content datum author name procedia computer science zulfadzli drus procedia computer science author name procedia computer science title sentiment analysis facebook group use lexicon base approach machine learning determine trend facebook characteristic people food habit group foodbank method tool application result context reporting result sentiment analysis method use medium author akter tareq rahaman chedia cynthia alshehri karamollaoglu dogru dorterler utku yıldız strauss christine ragini anand ramanathan meyyappan shahare mahtab islam sentiment analysis bangladesh cricket lexicon base analyze people sentiment support vector machine machine express cricket facebook group bangladesh cricket medium sentiment analysis lexicon lexicon base sentiment analysis facebook machine learn consumer generate content brand page elrahman alotaibi sentiment analysis twitter datum sentiment analysis medium network use machine learn popularity restaurant mcdonald twitter system provide insight twitter people perception fatyanosa classification method comparison lexicon base sentiment jakarta twitter medium sentiment governor election sentiment analysis lexicon base measure perception twitter medium share lexicon base influence phenomena analysis approach poecze ebster medium metric sentiment analysis evaluate effectiveness medium post optimize brand communication understand consumer feedback facebook page youtube gamer data analytic disaster response lexicon base sentiment need twitter recovery sentiment analysis machine affect people learn disaster twitter text mining sentiment lexicon base feedback oman tourism twitter analysis people feedback oman tourism learn machine learn machine learn machine learn machine learn machine learn machine learn machine learning study sentiment analysis tool classification tweet use miner identify tool twitter help enterprise suman gupta sharma analysis stock price flow base medium sentiment machine learning relate flow stock price stock twist base paper review paper demonstrate usage lexicon base method machine learning method method implement sentiment analysis result show conduct sentiment analysis review paper use lexicon base method paper machine learning paper show combination method lexicon base method know learning method lexicon method require training datum depend study adapt sentiwordnet method conduct sentiment analysis approach calculate base occurrence term text datum word predevelope polarity lexicon sentiwordnet method work convert word number calculate use term frequency inverse document frequency method technique rely resource effectiveness approach depend quality resource base polarity piece text obtain ground polarity word compose complexity language approach design cover aspect language come sarcasm negation use sentiment word problem exist word have meaning base application sentence contain sentiment word express sentiment sentence sentiment word imply opinion lexicon base method have advantage provide count word language speed analysis machine learning method fall supervise learning method require training datum order process use method machine learning method bayes model machine learning model use bayes apply form text corpus support vector machine give performance shape dataset machine learning method perform facebook people post length lot spelling mistake require amount train sample order adapt method amount dataset influence size quality output analyze machine learning time consume take hour machine learning model train require process size train dataset lead classification accuracy researcher argue type analysis method perform term accuracy option combine approach base sentiment classification contain sentiment score function bayes event model machine learning approach predict direction sentiment rely method study prove combine method efficiency order improve outcome recommend combine method complement result improve compare use approach combine approach identify phenomenon improve handle datum information service medium categorize type base application usage content community youtube instagram networking facebook linkedin blog reddit quora micro blog twitter tumblr base review paper type medium service blogging site twitter medium platform use collect information user opinion review paper use twitter collect information sentiment analysis twitter website enable user post interact message twitter express opinion provide information scholar business organization government twitter microblogging tool medium platform people express emotion person event product make twitter content datum sentiment analysis datum base medium process identify emotion news level news datum blog type medium platform extract datum sentiment analysis zulfadzli drus procedia computer science author name procedia computer science author name procedia computer science usage people access copy datum desire topic base keyword hashtag twitter conduct time analysis sentiment twitter tweet allow access datum twitter use search collect tweet country country twitter user world make opinion view people country language perception example twitter use collect user tweet president candidate election collect tweet write community development program activity twitter use collect message customer energy company analyze tweet download heathrow airport official twitter account analyze use sentiment analysis facebook medium user world sentiment analysis datum messy structure people form spell error make datum analyze example use facebook twitter fetch page status update comment suggest user experience study conduct gather datum source medium include forum blog expedia blog spot mainstream medium youtube twitter aggregator facebook result show datum come twitter source medium number data opinion extract blogspot youtube application context sentiment analysis application sentiment analysis range business marketing health action sentiment analysis application provide application area assist decision make sentiment analysis apply world event event activity sport disaster occur world example study conduct people country country view isis result show side world view isis sentiment analysis allow raise awareness datum security danger security breach act guideline company respond security breach shape perception sentiment analysis conduct unemployment rate employment sentiment score medium application sentiment analysis healthcare study use sentiment analysis service framework propose utilize spatio property identify location disease outbreak addition sentiment analysis identify sentiment need people disaster prepare response rescue sentiment analysis allow find level depression person overserve analyze emotion text sentiment analysis use predict election show datum analyze twitter platform correlation find polling datum have become platform polling technique feedback customer play role application sentiment analysis assist business organization take action improve product service business strategy show study conclude view experience drug product medium user sentiment analysis allow detect area need improve airport service quality apply measure attention passenger feedback medium sentiment analysis analyze trend characteristic people food habit business organization plan product marketing strategy sentiment analysis create advantage business owner identify popularity customer customer think product service assess effectiveness capability business brand communication medium evaluate business flow stock price medium feedback give consumer recognize weakness display study compare sentiment datum consumer tweet britain electricity supplier energy consumer result indicate sentiment energy consumer addition sentiment analysis medium allow organization evaluate success level program show study sentiment obtain tweet community development program activity result help improve live community conclusion conduct literature review provide information study sentiment analysis medium paper make follow contribution show method use analyze sentiment medium method introduce research method use lexicon base method sentiwordnet machine learn bayes choose method sentiment analysis depend datum method demonstrate accuracy thing take consideration structure text time amount data data structure amount datum limit time analysis recommend lexicon base method datum machine learning base method require time datum train order improve quality accuracy result suggest combine machine learning method identify type medium site extract information sentiment analysis medium site extract information twitter review paper twitter medium context availability accessibility richness twitter content million tweet topic indicate medium become source information attention give medium source blog wordpress youtube other content medium different explore source open knowledge finding demonstrate application sentiment analysis medium sentiment analysis application utilize area improve quality strategy business forecasting election result monitor disease outbreak create awareness importance datum security perception sport improve response disaster show sentiment analysis play role understand people perception help decision make recommendation investigation need develop model sentiment analysis apply type datum explore networking site obtain user opinion expand context sentiment analysis application reference statista number medium user statista statistic number giri kaiser towseef datum overview challenge journal research network user computer science software engineering sivarajah uthayasankar mustafa vishanth weerakkody analysis datum challenge method journal business research agarwal namita pooja bansal garg sentiment analysis use sense context information journal intelligence neuroscience gursoy medium mining sentiment analysis brand management journal emerge trend business marketing consumer psychology mantyla mika daniel graziotin miikka kuutila evolution sentiment analysis review research topic venue cite paper computer science review mishra classification opinion mining technique journal computer application song minchae park kyung shik shin attention base term memory network use sentiment lexicon embed aspect level sentiment analysis information processing management sanguansat paragraph vec base sentiment analysis medium business thailand conference knowledge technology maher chris roast samir khayatt develop resource sentiment analysis text medium procedia computer science chekima khalifa rayner sentiment analysis medium text author name procedia computer science zulfadzli drus procedia computer science author name procedia computer science usage people access copy datum desire topic base keyword hashtag twitter conduct time analysis sentiment twitter tweet allow access datum twitter use search collect tweet country country twitter user world make opinion view people country language perception example twitter use collect user tweet president candidate election collect tweet write community development program activity twitter use collect message customer energy company analyze tweet download heathrow airport official twitter account analyze use sentiment analysis facebook medium user world sentiment analysis datum messy structure people form spell error make datum analyze example use facebook twitter fetch page status update comment suggest user experience study conduct gather datum source medium include forum blog expedia blog spot mainstream medium youtube twitter aggregator facebook result show datum come twitter source medium number data opinion extract blogspot youtube application context sentiment analysis application sentiment analysis range business marketing health action sentiment analysis application provide application area assist decision make sentiment analysis apply world event event activity sport disaster occur world example study conduct people country country view isis result show side world view isis sentiment analysis allow raise awareness datum security danger security breach act guideline company respond security breach shape perception sentiment analysis conduct unemployment rate employment sentiment score medium application sentiment analysis healthcare study use sentiment analysis service framework propose utilize spatio property identify location disease outbreak addition sentiment analysis identify sentiment need people disaster prepare response rescue sentiment analysis allow find level depression person overserve analyze emotion text sentiment analysis use predict election show datum analyze twitter platform correlation find polling datum have become platform polling technique feedback customer play role application sentiment analysis assist business organization take action improve product service business strategy show study conclude view experience drug product medium user sentiment analysis allow detect area need improve airport service quality apply measure attention passenger feedback medium sentiment analysis analyze trend characteristic people food habit business organization plan product marketing strategy sentiment analysis create advantage business owner identify popularity customer customer think product service assess effectiveness capability business brand communication medium evaluate business flow stock price medium feedback give consumer recognize weakness display study compare sentiment datum consumer tweet britain electricity supplier energy consumer result indicate sentiment energy consumer addition sentiment analysis medium allow organization evaluate success level program show study sentiment obtain tweet community development program activity result help improve live community conclusion conduct literature review provide information study sentiment analysis medium paper make follow contribution show method use analyze sentiment medium method introduce research method use lexicon base method sentiwordnet machine learn bayes choose method sentiment analysis depend datum method demonstrate accuracy thing take consideration structure text time amount data data structure amount datum limit time analysis recommend lexicon base method datum machine learning base method require time datum train order improve quality accuracy result suggest combine machine learning method identify type medium site extract information sentiment analysis medium site extract information twitter review paper twitter medium context availability accessibility richness twitter content million tweet topic indicate medium become source information attention give medium source blog wordpress youtube other content medium different explore source open knowledge finding demonstrate application sentiment analysis medium sentiment analysis application utilize area improve quality strategy business forecasting election result monitor disease outbreak create awareness importance datum security perception sport improve response disaster show sentiment analysis play role understand people perception help decision make recommendation investigation need develop model sentiment analysis apply type datum explore networking site obtain user opinion expand context sentiment analysis application reference statista number medium user statista statistic number giri kaiser towseef datum overview challenge journal research network user computer science software engineering sivarajah uthayasankar mustafa vishanth weerakkody analysis datum challenge method journal business research agarwal namita pooja bansal garg sentiment analysis use sense context information journal intelligence neuroscience gursoy medium mining sentiment analysis brand management journal emerge trend business marketing consumer psychology mantyla mika daniel graziotin miikka kuutila evolution sentiment analysis review research topic venue cite paper computer science review mishra classification opinion mining technique journal computer application song minchae park kyung shik shin attention base term memory network use sentiment lexicon embed aspect level sentiment analysis information processing management sanguansat paragraph vec base sentiment analysis medium business thailand conference knowledge technology maher chris roast samir khayatt develop resource sentiment analysis text medium procedia computer science chekima khalifa rayner sentiment analysis medium text zulfadzli drus procedia computer science author name procedia computer science cirqueira fonte jacob lobato santana literature review preprocesse sentiment analysis medium ieee conference intelligence peng haiyun erik cambria amir hussain review sentiment analysis research language computation ieee ebrahimi yazdavar sheth challenge sentiment analysis event system durach joakim kembro andreas paradigm literature review supply chain management journal supply chain management wieland sarit chakraborty improve text sentiment classification model use word negation khan muhammad taimoor mehr armughan irum inayat shehzad khalid habib khan sentiment analysis language adaptive system model akter sanjida muhammad tareq sentiment analysis facebook group use lexicon base approach conference engineering information communication technology iceeict hassan anee jamil hussain musarrat hussain muhammad sadiq sentiment analysis networking site datum use machine learning approach measurement depression conference information communication technology convergence ictc jeju korea ieee mahtab arafin mahfuzur rahaman sept sentiment analysis bangladesh cricket support vector machine conference bangla speech language processing icbslp chedia cynthia webster medium sentiment analysis lexicon machine learning journal consumer marketing rahman alotaibi alshehri april sentiment analysis twitter datum conference computer information science iccis kashif dong athman bouguettaya abdelkarim erradi hadjidj sentiment analysis service medium base sentiment analysis framework ieee conference service icw ieee jianqiang hongye medium content sentiment analysis consumer security breach journal crime computer science mansour samah medium analysis user response terrorism use sentiment analysis text mining procedia joyce brandon je deng sentiment analysis tweet election ieee undergraduate research technology conference cambridge ieee siti djatna taufik sentiment mine community development program evaluation base medium telkomnika compute electronic control victoria maria sharmina khaleel malik riza batista navarro analyze sentiment express twitter energy company consumer conference network analysis management security snam ieee domingo luis juan martin medium resource sentiment analysis airport service quality journal transport management isah haruna paul trundle medium analysis product safety use text mining sentiment analysis workshop intelligence ieee shayaa shahid phoong seuk chung sulaiman noor ismawati shamshul bahri medium sentiment analysis employment malaysia proceeding business finance research conference taipei taiwan karamollaoglu dogru dorterler utku yıldız sept sentiment analysis medium share lexicon base approach conference computer science engineering ragini rexiline rubesh anand datum analytic disaster response recovery sentiment analysis journal information management poecze flora claus ebster christine medium metric sentiment analysis evaluate effectiveness medium post procedia computer science suman gupta sharma analysis stock price flow base medium sentiment conference generation compute information system icngcis', 'expert system application content list sciencedirect expert system application journal homepage eswa review extract software development information application review survey mining technique tool mohammadali tavakoli lipe zhao atefeh heydari nenadi school computer science university manchester manchester kingdom article history receive revise march accept june keyword application review review review mining development review mining tool review mining technique application website play appstore allow user review app review user help user make deci sion review developer contain information concern user need requirement order unleash value review bile development mining tool help discern review one provide paper survey state development tool technique gain insight maturity support mining tool paper development information tool discover challenge face result survey inform development review mining technique tool crown copyright publish access article license http creativecommon license introduction base software application distribution platform come internet user accord number mobile application load play store increase lion platform user share opinion applica tion app user perspective opinion ﬂuence decision purchase choice heydari tavakoli salim heydari provider perspective review attract customer bring gain review cause sale loss addition user review opinion contain information development improvement report user experience user requirement correspondence author computer science university manch ester manchester kingdom mail address mohammadali tavakoli manchester tavakoli lipe zhao manchester zhao atefeh heydari manchester dari manchester nenadi user review development order unleash value review development mining tool help cern review one provide year variety technique propose range sentiment analysis fernandez gavilane alvarez lopez martinez montenegro gonzalez castano spam detection heydari tavakoli salim savage chou wang mining tech nique castelli manzoni popovi lack understanding technique context mining mobile review support tool find survey assess store technique survey genc abran vide analysis datum mining technique spam tection opinion mining review evaluation feature extraction survey martin sarro harman concern store analysis analysis feature analysis review analysis none survey cover work speciﬁc mining technique tool view address paper survey state development mobile review mining technique tool https eswa crown copyright publish access article license http creativecommon license tavakoli expert system application gain insight maturity support mining tool paper development information tool discover challenge face result survey inform development review mining technique tool rest paper organize follow section scribe survey methodology section present survey result section discuss issue challenge face development review mining tool section discuss validity threat review section conclude review survey methodology methodology conducting survey erature review kitchenham charter budgen brereton turner avoid confusion use term survey review refer paper subject matter review base literature review guideline provide kitchenham follow describe step survey process research question state section goal paper state development mobile review mining technique tool survey cover study report development review mining technique tool addition paper speciﬁc development topic report tool use discover ﬁnding help evaluate maturity mining tool line goal formulate follow research question drive survey review mining technique be software tool develop support port literature technique development topic use report technique tool topic use identify study deﬁnition strategy perform enable researcher retrieve majority study section discuss search strategy detail develop search string select keyword study review domain apply alternative synonym term link expression cover search result order perform search formulate query optimize reﬁne search string iteration mull reveal result skim ming retrieve study help manipulate search string keyword exclude keyword inclusion replace one search term present table ﬁnalize search term android application feedback review opinion ment report feature request complain issue expectation analysis process mining extract discover developer development phase selection process denote number paper team requirement team software vendor requirement engineering requirement elicitation requirement analysis follow database sciencedirect construct search string use search ieeexplore scopus springerlink search return result find paper publish table summarize search result select study base search result use follow sion exclusion criterion select study selection phase show inclusion criterion tavakoli expert system application table type component search string type search term domain content review review type development requirement engineering technique mobile android application feedback review opinion comment report feature request complain requirement issue expectation developer development team requirement team software vendor requirement engineering requirement elicitation requirement analysis analysis process mining extract discover table search result database search result science scopus springerlink ieeexplore google scholar study report review application develop ment include addition study report search method case study survey experiment study include paper report study paper include exclusion criterion grey literature research output peer review report document work paper exclude paper text exclude paper page exclude inclusion exclusion criterion apply low step apply turn search result clude study apply remain study include study criterion apply study include stud step study lecte survey text port endnote library datum extraction reference study list appendix show distribution select study lishe number paper publish paper mention number paper find search period cover month year select paper select paper conference paper journal arti cle appendix summarize number paper publish channel extract synthesize datum step require datum extract study predeﬁne datum extraction form table use record datum study type datum distribution select study table datum extraction form datum item description assign paper year study publish author paper title paper publication venue study paper year author title venue technique mining technique use study tool topic support tool extract software development information software development topic discover study extract datum require answer research question datum require display information study extract datum store excel process analysis extract datum synthesize use parison method boeije step involve datum synthesis be comparison paper summarize core paper understand category diﬃcultie highlight comparison paper category paper use technique aim ceptualize produce category study comparison paper group identify effectiveness eﬃciency category technique solve issue tavakoli expert system application survey result search question section analyse review result answer review mining technique review mining technique port literature mining technique identiﬁed survey type supervise machine learn language processing feature extraction section present type technique detail supervise machine learning technique supervise machine learning technique find select study technique use classify review platzer use motivational model usage motive address text user review attach motive review apply algorithm song believe view ﬁltere help developer overcome overload feedback deﬁne category view demand request classiﬁe review category train ﬁltere model identiﬁcation category do developer analyse review content detail process number expert number analyse review selection criterion provide extract keyword review use latent dirichlet location method apply algorithm classify review author argue performance identify keyword review length order identify reason user dislike app develop system name wiscom analyse view level review level cover review rating apply regression model understand review word use ply topic modelling technique discover speciﬁc prob lem user complaint market level analyse complaint complaint aspect app lead identify trend market focus review hypothesize use enhance quality application searcher observe feature request feed developer appear review iacob iacob veerappa harrison chen xiao develop computa framework review mining miner ﬁlter review use topic modelling technique group review base topic discuss review rank scheme prioritize respect developer need miner use expectation maximization baye emnb algorithm machine learning classify review phase use grouping create rank group review base rating ﬂuctuation time review volume review report issue request thor compare performance algorithm topic elle latent dirichlet allocation aspect senti ment uniﬁcation model asum group review base content classify user feedback informative justify superiority expectation maximization bayes emnb iste algorithm testing performance classiﬁer compare outcome justiﬁcation periority approach aspect neglect approach sentiment view relate content use leverage formance tool guzman maalej employ time stamp text rating review task feature title review datum ture neglect remove stop word stem apply processing technique crawl view state approach khalid hassan show processing task increase chance lose content discriminate review base understand reviewing number forum identify developer extract user review need requirement study miner use palomba vestigate address user feedback developer inﬂuence rank propose tool name crowdsourcing review support evolution collect view post release application track rating extract review use miner check comment apply lease check effect rating release maalej nabil design apply prob technique heuristic classify view type report feature request user perience rating generate list keyword string matching word sentiment score process text review rating length use classiﬁcation task apply bayes decision tree maxent pare performance classiﬁer classi ﬁcation user feedback predeﬁne type thor extend approach maalej kurtanovi nabil stanik add bigram combination utilize classiﬁcation technique improve preprocesse phase classiﬁcation script argue datum combine text classiﬁcation language processing text classiﬁcation precision rise guzman haliby bruegge rely category find pagano maalej form taxonomy category software evolution author use taxonomy investigate performance machine learn technique bayes support vector machine svms regression network classiﬁcation review feature use include number case character length sentiment rate feature find user feed use classiﬁcation purpose order enhance throughput model tavakoli ismail salim panichella argue topic iden tiﬁed pagano maalej rele software maintenance evolution task propose method identify feedback software mainte nance evolution task author hypothesize stand intention review role tracte information developer understand intention review use sentence structure sentiment review text feature contain review form taxonomy information give information seek feature tavakoli expert system application request problem discovery review number review extract feature text analysis sentiment analysis technique train classi review accord taxonomy compare performance machine learn technique bayes regression support vector machine alternate cision tree report perform thor identiﬁe group review sentence category compare category topic pagano maalej find topic match category panichella panichella propose ardoc review orient tool classiﬁe sentence user review accord taxonomy design panichella model developer information need perform software maintenance evolution task divide review sentence extract structure sentiment sentence use machine learning algorithm classiﬁcation purpose approach improve surf summarizer user review feedback pose sorbo tool categoriza tion summarization review split review tence perform summarization task phase employ approach propose study panichella sentence category classify sentence user intention assign tention review sentence employ set word build classiﬁer assign concept topic sentence review author analyse sentence review select training discover topic discuss reviewer result identiﬁcation topic assign keyword sentence topic create ﬁnite keyword enrich wordnet synonym summarization purpose rely observation report feature request sentence cuss aspect need review discuss feature need assign score sentence observation tool categorize sentence accord topic intention category generate summary structure html approach suffer lack research need developer point serve author software developer assign relevance score category intention score observation study impact score overcome problem process terminolo gy use user language cause classiﬁcation model overﬁtte problem moud propose framenet tagging base approach classify review base notion role labelling use obtain level abstrac tion sentence classiﬁes word use sentence class describe event pant classiﬁcation task use bayes port vector machine author use frame generate review word target class report feature request indicate type information ignore author release tool mahmoud study target star star review mcilroy study extent label user table supervise machine learning technique find study algorithm description use study regression model decision tree maxent technique description use study table technique find study rule gram analyse deﬁnition pattern deﬁnition expression term vector model aspect sentiment uniﬁcation model topic modelling view review raise issue type propose approach label label user review deﬁne type issue label number review form labelling task transform problem labelling belling use label combine sult use classiﬁer support vector chine decision tree bayes labelling approach relevance leverage correlation label clas siﬁer chain palomba leverage correla tion label prune set threshold extension deﬁne threshold assign label review use fold validation evaluate result processing phase remove number character stop word expand abbreviation ﬁltere word occur ring time dataset stem word remove review consist word observation exhibit review word report bug issue camera save button suck upload picture use crease weight word occur user review decrease weight word occur user review devalue word weighting word demote issue repeat discuss user analysis technique use mention approach train classiﬁer classiﬁcation suffer fye review number class approach domain speciﬁc fact change target application aspect feature change update utilize model require table summarize technique technique technique find select study process extract information review table tavakoli expert system application iacob harrison study identiﬁcation proportion feature request user feedback develop totype mine review app retrieve feature request crawl review datum crawler extract feature request predeﬁne rule summarize extract feature request rank base frequency length visualize result author crawl review play read formulate rule identify feature request iden tify topic feature request apply model study enhancement request android user macdonell patel apply technique gram analysis identify request focus android view vary one generate application focus text ignore data feedback drawback approach moghaddam deﬁne pattern improvement defect review contain provement defect use review case train distance learning classiﬁer use sentence contain defect improvement author ply cluster sentence score importance found topic effort ﬁnde pattern ture defect improvement suﬃcient approach problem issue report deﬁne defect improve ment include modiﬁcation upgrade request pattern deﬁne extract user review form explanation use user report fect improvement miss approach pattern use label review cause inaccuracy result propose approach provide develop categorize set review need expend effort source explore set deﬁnition defect improvement term category fall group liang propose approach extract user view extract requirement information classify functional require ment approach requirement engineer iden tify classify number user review technique extract keyword review use classiﬁcation review prede ﬁne expression judgement require check keyword make approach labour corpus consist review obtain evaluation thor annotate datum extract keyword deﬁne expression base observation datum mance approach depend preference evaluation accuracy question pham nguyen develop mark analyse review keyword automate frame work assist developer search review feature use base approach search review technique use keyword extraction rank keyword categorize base mark use vector space model query view database fetch result respect keyword galvi carreno winbladh use topic modelling technique discover topic review use change create requirement release software use aspect sentiment uniﬁcation model asum extraction topic author focus table feature extraction technique find study technique description study pattern base parse collocation ﬁnde algorithm change kind information developer neglect approach author classiﬁe review build dataset acknowledge procure training datum error expertise author domain question feature extraction technique study use feature extraction tech nique mobile review table apply sentiment pattern parse review sentence elicit feature propose tool surminer split review sentence apply entropy classify review category aspect evaluation report feature request praise other identiﬁes pect opinion sentence fall category aspect evaluation designing pattern base par method design method author yse sentence review label aspect evaluation identiﬁed template tool analyse sentiment sentence assign rating mine quent item aspect word cluster aspect opinion pair item summarize output focus aspect evaluation sentence review topic feature request report ignore study study extract applica tion aspect opinion designing pattern base parse method deﬁnition pattern aspect error uncertainty aspect cuss user review aspect differ demand update predeﬁne tern guzman maalej extract feature text review collocation ﬁnding algorithm algorithm ﬁnd collection word occur battery life screen resolution apply sentistrength automate sentiment analysis technique thelwall buckley paltoglou design tackle quality text extract opinion sentiment associate ture sentistrength divide input text sentence sign value value tence group feature aggregate sentiment use topic modelling technique blei identify application ture sentiment mention user review guzman bruegge extend approach guzman maalej study identiﬁcation ﬂicting opinion develop identify user opinion concern application group review mention feature sentiment approach author use collocation ﬁnde algorithm extract feature user review sentiment analysis tool opinion experience concern feature use algorithm retrieve feature sentiment issue affect accuracy method feature approach collection word curre review include order tavakoli expert system application table study section study technique review sample analysis analysis analysis analysis analysis analysis analysis analysis regex analysis regex onym misspell limit feature consist keyword prevent approach cover type feature mention user feedback accord deﬁnition feature approach ignore feature appear review feature development team sentiment analysis score sentence review assign score ture mention sentence case sentiment review user admire feature sentence word word use favour feature sohail siddiqui example review background approach assign score review feature analysis majority select study group use analysis user feedback identify topic discuss table section study explain detail wagner analyse review application google play user talk identiﬁed category topic discuss review irritation understand recur issue user report review iacob analyse google play review author deﬁne code scheme capture recur issue coder anno tat select review result identiﬁcation class code price late request requirement issue report usability customer support versione divide review snippet text assign reﬁne code approach vague describe review process study demonstrate user review developer pagano maalej investigate user provide feedback identify classify review integrate user feedback ment software engineering infrastructure focus impact review rating community user author apply statistic investigate usage feedback analyse sample view review explore assign topic review group observe topic theme community rating requirement user experience contribution ﬁeld khalid manu ally analyse star star review app order developer list complaint discover type issue complaint app user feedback repeat approach khalid shihab gappan hassan extend approach propose review system enable user rating reply comment sort review dislike cate gorize review khalid asif shehzaib abovementione study section try iden tify topic discuss feedback analyse topic ﬁnde annotation report study show result judgment dataset study ere approach facilitate developer analyse review cost effort require approach iden tiﬁed topic hand used propose method classify feedback content dependency barrier take eration review contain sarcasm content exclude scope identiﬁcation topic user feedback analysis use study role star rating rela tion aspect application include volume download length review amount sale price manu ally analyse user review blackberry app harman conﬁrme rating give user signif impact number download understand user communicate review hoon vasa schneider mouzaki analyse review app categorize keyword appear star rating hypothesize inform focus development fort author determine distribution word charac count star rating apply expression extract word review entity monitor star rating appearance extract keyword pertain apply processing technique stem remove stop word spell checking normalize input review focus word result miss word expression author use dataset approach vasa hoon mouzaki noguchi discover relation rating review content argue review ing include feedback depth feedback category approach author discuss detail setting analysis analysis observation use processing technique mention study support tool mining mobile review software tool develop support technique support tool find select study mary present table follow provide overview tool mara tool analyse user feedback step source review crawl tool use rule mine review content ture request express user example rule add request add exit button feature request summarize accord predeﬁne rule rank extract quest base frequency length identify topic associate request latent dirichlet alloca tion model use feature request visu alization phase result summarization display user wiscom tool analyse user feedback level tail involve discover inconsistency review identify reason user dislike give identify user preference concern type app regularize regression model use discover consistency detect rating match text tavakoli expert system application table support tool mining mobile review tool name description underlie technique unit analysis study mara base frequency length term rule sentence wiscom sentence tool use rule identify extract summarize feature request tool feedback use regression model uncover cause complaint use aggregate complaint app category identify market trend regression model aspect sentiment uniﬁcation model level miner order ﬁlter review tool use topic expectation maximization sentence model technique group review review rank scheme prioritize bayes tool detect traceability link document detection conﬂicte opinion tool respond document review source code change address use link analyse impact crowd review development process surminer developer query grouping review mention feature sentiment summarize review tool classiﬁes review extract aspect use pattern base parser review feature surf combine topic extraction intention classiﬁcation technique tool categorize summarize user review miner information retrieval heuristic collocation ﬁnde algorithm sentiment analysis tool algorithm sentiment pattern entropy technique vector space model classiﬁer wordnet sentence synonym tool classiﬁe sentence user sentiment analysis text sentence review accord taxonomy analysis mark automate tool assist developer search keyword base approach document sentence feedback feedback comment app aggregate algorithm apply discover user dislike app train use word weight regression model identify complaint category app complaint feedback comment step aggregate category summarize display user miner tool analyse user feedback comment ﬁlter aggregate prioritize visualize information help developer improve app review noise text review ﬁltere apply train expectation maximization bayes remain review group use topic modelling technique pect sentiment uniﬁcation model prioritize applica tion rank model rank result visualize radar chart help developer spot feedback user tool use trace provide insight speciﬁc problem experience feature demand user review source code change monitor change impact user satisfaction measure follow rating miner use view heuristic use extract issue mit drive review technique use iden tify link review issue commit link retrieve review store database group link relate release information exploit monitoring component create report manager developer show stat view implement feature sentiment retrieval tool generate sample user review opinion experience mention review developer query review mention feature tool retrieve review represent user opinion concern ture tool apply collocation ﬁnde algorithm extract feature mention review use sentiment analysis order excerpt sentiment associate tracte feature use algorithm retrieve verse review term mention feature group view content sentiment surminer tool summarize user sentiment opinion software aspect entropy algorithm tool classiﬁes sentence user review category aspect evaluation praise feature request report other ﬁlter aspect evaluation sentence extraction aspect correspond opinion sentiment tool use pattern base parse method extract aspect timent method analyse syntax semantic review sentence result aspect opinion sentiment triplet cluster mining word aspect cluster aspect opinion pair word result graph mark tool automate review analysis framework take keyword developer input trieve list review match keyword analy tool keyword extraction component extract keyword review keyword cluster base word vec use algorithm pande base base vector base similarity word analyst speciﬁes keyword clustering expand tool query database return result employ term frequency inverse document frequency term weight scheme compute element value vector vector space model task surf tool summarize user review assist developer manage amount user review tool rely model capturing user need developer tavakoli expert system application form maintenance evolution task use summarisa tion technique summarize thousand review gener ate agenda recommend software change tool equip topic use cla siﬁer assign sentence review topic suggest speciﬁc kind maintenance task develop have accomplish classify intention review use panichella base scoring mechanism tool generate summary structure html ardoc tool classiﬁe feedback taine review deem form software maintenance evolution task tool vide review text sentence extract sentence kind feature word ture extract classiﬁer exploit tionalitie provide apache lucene extract term weight use term frequency structure feature frame tence extract classiﬁer use heuristic predeﬁne pattern detect occurrence speciﬁc keyword role speciﬁc structure sentiment extract classiﬁer use sentiment annotator provide corenlp step classiﬁer use information tracte phase approach classify view accord predeﬁne taxonomy exploit algorithm development topic discover select study development topic use report technique tool topic use find group topic discover liter ature group number speciﬁc topic table summarize topic group topic study separate line table answer part topic refer table count number study discover topic topic number study refer hottest category topic discover literature report study refer term study mention term report topic feature request discover study mention term feature request paper statistic show concern application user port bug shortcoming request feature add application show distribution topic discover select study issue section report challenge problem area provide research direction researcher issue make information extraction user feedback challenge task majority issue arise nature feedback mine interpretation searcher environment prob lem identiﬁe investigate extraction technique address challenge lecte study focus subset issue solve direction research area distribution development topic designing model solve follow issue volume user feedback volume user generate feedback insight amount review task searcher quantity review generate application ally exceed capacity analyse extract information make diﬃcultie identify recur trend issue review process amount data time consuming case plication update reveal version time mitment involve extract require change lease carreno winbladh approach process sive amount feedback take relation account demand datum nature user feedback challenge process user write review manner make interpretation diﬃcult iacob harrison tend express review use language include terminolo gy grammar punctua tion rule entity ironic sarcasm sentence review pagano maalej challenge developer deal piece text overcome problem text mining approach integrate propose model accompany order deal type user generate text annotating data datum annotation require proache training machine evaluation purpose time cost consuming task accompany mistake error obtain sult supervise approach discuss section majority case study vide approach demand type anno tation result impossibility creation gold standard dataset use evaluation approach diﬃcultie annotation datum accuracy annotate datum question domain expert article author expert employ majority case effort novel approach require alleviate problem data annotation datum processing preprocess task researcher prepare datum use input method stop word removal technique tavakoli expert system application table development topic discover select study topic category speciﬁc topic study study category comment review work doesn comment comparison report feature request user experience update price recommendation work praise rating feature strength praise description work doesn work uninstalle shortcome dispraise dissuasion complaint compatibility crash network problem interface design privacy response time unintereste content resource comparison app comparative feature functionality issue report content request feature report performance error feature removal report solution proposal problem discovery feature shortcome aesthetic request requirement improvement request promise rate improvement demand request feature request user request user requirement tip installation usage usability question user experience information seek information give opinion ask user scenario update compare version update issue versione money money price relate cost cost recommend customer support recommendation program need number content company download use device model permission preinstalle consumption resource example task propose method include task perform term time computation group approach argue use preprocesse task cause miss information user feedback mahmoud fore research require investigate impact apply processing task performance accuracy technique data interpretation approach technique extraction information user feedback propose researcher interpretation information extent veloper research begel zimmermann try discover developer designer viewpoint mull review fung jeon tavakoli expert system application support tool feedback mining describe discover select study identiﬁed discuss challenge open problem feedback mining require research ﬁnding review provide implication researcher quirement engineer tool developer gain stand study tool suitability context result improvement devel opment review mining technique tool result show quality control discover topic user feedback seem suﬃcient ensure feedback source relate information fact affect investment application vendor user feedback discover accuracy effectiveness propose technique varie apply user review type application indicate terminology feature use user review domain developer requirement engineer take domain dependency issue account select target technique mining review app result study show publish per study extraction technique compare one focus discover discuss topic consider dissemination domain dependency approach emerge topic researcher practitioner want encounter topic review research direction derive sight provide researcher propose approach user feedback mining usefulness value user feedback expert question knowledge research investigation quality usefulness user feedback expert view point depth study need address shortcoming discover user feed mining tool address challenge list section development tool assist expert mining user feedback source user feed back app user developer experi mente technique enrich datum result ﬁnding propose technique transform extract user need speciﬁcation modelling compleman component requirement extraction tool wish thank reviewer ment help improve paper president scholarship university manchester enable carry study zhou factor functionality feature use guzman haliby informativity review developer perspective investigate read forum discussion chen propose approach make data analysis author point view inconsistency tween researcher developer interpretation result ineﬃcient method technique study need extract user review require make propose approach validity threat threat validity result coverage study search cover publication lishe conduct review use bind search term study publish march include review search relate limitation review miss paper identiﬁcation stud completeness search search criterion use scope search inﬂuence limitation search engine use brereton kitchen budgen turner khalil use know reference validate search string make amendment undertake review paper be snowballing index library find search term use view aim cope construct validity eralization result concept theory study execution clae deﬁne apply construct requirement engineer feedback analysis software evolution validity threat relate selection analysis synthesis extract datum interpre tation researcher inclusion exclusion study pass selection go discussion cross check author try mitigate thread conduct selection process collect datum extractor candidate ﬁeld minimize risk researcher bias literature synthesis content jane alleviate threat form search process trial search compromise author unqualiﬁe paper exclude application exclusion criterion conclusion paper report research effort aim review analyse application feedback processing practice assist developer extraction informa tion insight review conduct guideline vide study identiﬁe analyse categorize discuss stud base technique investigate cappendix select study tavakoli expert system application study citation title venue wagner consumer communication networking conference guzman maalej requirement engineering conference galvi carreno winbladh platzer harman hoon vasa iacob khalid iacob harrison pagano maalej chen khalid guzman guzman haliby khalid palomba liang panichella opportunity automate motive base user review analysis context acceptance store mining analysis store analysis user review analysis user review people hate make sense user feedback mobile store android user write sheep examine consumer review google play complain study review application identify user complaint app facilitate developer user interaction review digest analysis user comment approach software requirement evolution retrieve analyze app feature request review user feedback study miner mining review developer marketplace user feature grain sentiment analysis review user complain part app love user retrieve opinion review method review classiﬁcation approach software evolution improve quality review know analyze post release feedback community report feature request praise classify review sentiment analysis mining defect improvement customer feedback user review matter track crowdsource review support evolution app mining user opinion review base approach identiﬁcation classiﬁcation requirement user review improve classify user review software maintenance evolution conference information system proceeding ieee work conference mining software repository proceeding computer interaction conference proceeding computer interaction conference proceeding sigkdd conference knowledge discovery data mining proceeding computer interaction conference conference software engineering abstract factor compute system proceeding conference software engineering ieee working conference mining software repository requirement engineering conference proceeding conference software engineering ieee software ieee conference automate software engineering software engineering measurement ieee symposium ieee automate software engineering journal information technology computer science proc st amer info rico ieee conference software maintenance evolution ieee conference automate software engineer conference software engineering knowledge engineering ieee conference software maintenance evolution continue page maalej nabil rd ieee requirement engineering conference conference information retrieval springer tavakoli expert system application study citation title venue sorbo proceeding symposium foundation software engineering genc abran journal system software maalej martin mcilroy panichella mahmoud mahmoud user change summarize review recommend software change literature review opinion mining study store user review classiﬁcation review survey store analysis software engineering analyze label type user issue raise review ardoc review development orient mining user requirement application store review use frame semantic application review publication channel select study journal requirement engineer ieee transaction software engineering software engineering proceeding symposium foundation software engineer work conference requirement engineering foundation software quality work conference requirement engineering foundation software quality publication channel type study study conference software engineering icse ieee requirement engineering conference ieee conference automate software engineering ieee working conference mining software repository computer interaction conference ieee conference software maintenance evolution csme symposium foundation software engineer work conference requirement engineering foundation software quality refsq journal society business study sigkdd conference knowledge discovery data mining conference consumer communication network computer interaction conference factor compute system ieee software ieee symposium software engineering measurement journal information technology computer science conference information system conference information retrieval conference software engineering knowledge engineering journal system software journal requirement engineer ieee transaction software engineering software engineering conference conference conference conference conference conference conference conference journal conference conference conference journal journal conference conference conference conference conference journal journal journal journal reference begel zimmermann analyze question datum scientist software engineering proceeding conference software engineering blei latent dirichlet allocation journal machine learn research boeije approach method analysis interview quality quantity brereton kitchenham budgen turner khalil lesson apply literature review process software gineering domain journal system software castelli manzoni popovi expert system extract knowledge customer review case amazon expert system application chen xiao miner mining review developer marketplace proceeding conference software engineering clae magnus mentation software engineering introduction book google book sorbo panichella alexandru shimagaki visaggio fora user change summarize review recommend software change proceeding symposium foundation software engineering fernandez gavilane alvarez lopez martinez montene gonzalez castano method sentiment analysis text expert system application faloutsos hong sadeh people hate make sense user feedback mobile store proceeding sigkdd conference knowledge discovery data mining carreno winbladh analysis user comment software requirement evolution proceeding conference software engineering ieee press jane recommendation system software neere recommend literature review journal system genc abran literature review opinion mining study store user review journal system software part app love user tomate software engineering ieee conference ieee guzman bruegge retrieve opinion view software engineering measurement ieee symposium ieee guzman haliby bruegge method review classiﬁcation approach software evolution automate software tavakoli expert system application engineering ieee conference ieee guzman maalej user feature grain sentiment analysis review ieee requirement engineering conference ieee wagner android user write sheep examine consumer review google play consumer communication networking conference ccnc ieee ieee harman store mining analysis store proceeding ieee work conference mining software repository ieee press tavakoli salim detection review spam survey expert system application tavakoli salim detection opinion use time series expert system application hoon vasa schneider mouzaki analysis user review proceeding computer interaction conference iacob harrison retrieve analyze app feature quest review mining software repository ieee working conference ieee iacob veerappa harrison complain study review application proceeding computer interaction conference computer society mahmoud marc application review classiﬁer work conference requirement engineering foundation software quality mahmoud mining user requirement application store review use frame semantic work conference require ment engineering foundation software quality springer khalid identify user complaint interna conference software engineering icse ieee shihab nagappan hassan user complain ieee software khalid asif shehzaib improve quality review journal information technology computer science ijitcs kitchenham charter budgen brereton turner guide line perform literature review software engineering report ebse report ebse macdonell patel savarimuthu know analyze post release feedback commu nity proc st america conference information system rico aisel fung identify review product designer perspective computer aid design maalej kurtanovi nabil stanik classiﬁca tion review requirement engineering maalej nabil report feature request praise classify review ieee requirement engineering conference ieee martin sarro harman survey store analysis software engineering ieee transaction software engineering mcilroy khalid hassan analyze bell type user issue raise review software engineering moghaddam sentiment analysis mining defect improve ment customer feedback conference information retrieval springer song facilitate developer user action review digest extend abstract factor compute system pagano maalej user feedback study st requirement engineering conference ieee palomba linare vasquez poshyvanyk user review matter track crowdsource review support evolution app software maintenance evolution ieee conference ieee panichella sorbo guzman visaggio canfora gall improve classify user review maintenance evolution software maintenance evolution ieee conference ieee panichella sorbo guzman visaggio canfora gall ardoc review development orient ceeding symposium foundation software engineering platzer opportunity automate motive base user review analysis context mobile acceptance ceciis jeon zhou mining customer requirement review product improvement perspective information management savage chou detection opinion spam base rating deviation expert system application sohail siddiqui feature extraction analysis review recommendation book use opinion mining technique spective science tavakoli ismail salim framework review spam detection research world academy science engineering technology journal computer automation control information neere thelwall buckley paltoglou sentiment strength detection journal association information science technology vasa hoon mouzaki noguchi analysis bile user review proceeding computer teraction conference nguyen pham mining user opinion review keyword base approach automate software engineering ieee conference ieee yang liang identiﬁcation classiﬁcation requirement user review conference software engineering knowledge engineering seke']\n"
     ]
    }
   ],
   "source": [
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'VERB']) #select noun and verb\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,\n",
    "# minimum reqd occurences of a word \n",
    "                             stop_words='english',             \n",
    "# remove stop words\n",
    "                             lowercase=True,                   \n",
    "# convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  )\n",
    "# num chars > 3\n",
    "                             # max_features=50000,             \n",
    "# max number of uniq words    )\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(learning_method='online', n_jobs=-1, random_state=100)\n"
     ]
    }
   ],
   "source": [
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "                                      max_iter=10,               \n",
    "# Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          \n",
    "# Random state\n",
    "                                      batch_size=128,            \n",
    "# n docs in each learning iter\n",
    "                                      evaluate_every = -1,       \n",
    "# compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               \n",
    "# Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -2506900.9547517775\n",
      "Perplexity:  623.6512813018077\n",
      "{'batch_size': 128,\n",
      " 'doc_topic_prior': None,\n",
      " 'evaluate_every': -1,\n",
      " 'learning_decay': 0.7,\n",
      " 'learning_method': 'online',\n",
      " 'learning_offset': 10.0,\n",
      " 'max_doc_update_iter': 100,\n",
      " 'max_iter': 10,\n",
      " 'mean_change_tol': 0.001,\n",
      " 'n_components': 10,\n",
      " 'n_jobs': -1,\n",
      " 'perp_tol': 0.1,\n",
      " 'random_state': 100,\n",
      " 'topic_word_prior': None,\n",
      " 'total_samples': 1000000.0,\n",
      " 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LatentDirichletAllocation(learning_method='online',\n",
       "                                                 learning_offset=50.0,\n",
       "                                                 max_iter=5, random_state=0),\n",
       "             param_grid={'learning_decay': [0.5, 0.7, 0.9],\n",
       "                         'n_components': [10, 15, 20, 25, 30]})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.9, 'n_components': 10}\n",
      "Best Log Likelihood Score:  -547393.7176522564\n",
      "Model Perplexity:  763.4917678261962\n"
     ]
    }
   ],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_0f1b6_row0_col0,#T_0f1b6_row0_col2,#T_0f1b6_row0_col3,#T_0f1b6_row0_col4,#T_0f1b6_row0_col5,#T_0f1b6_row0_col6,#T_0f1b6_row0_col7,#T_0f1b6_row0_col8,#T_0f1b6_row0_col9,#T_0f1b6_row1_col0,#T_0f1b6_row1_col2,#T_0f1b6_row1_col3,#T_0f1b6_row1_col4,#T_0f1b6_row1_col5,#T_0f1b6_row1_col6,#T_0f1b6_row1_col7,#T_0f1b6_row1_col8,#T_0f1b6_row1_col9,#T_0f1b6_row2_col0,#T_0f1b6_row2_col2,#T_0f1b6_row2_col3,#T_0f1b6_row2_col4,#T_0f1b6_row2_col5,#T_0f1b6_row2_col6,#T_0f1b6_row2_col7,#T_0f1b6_row2_col8,#T_0f1b6_row2_col9,#T_0f1b6_row3_col0,#T_0f1b6_row3_col2,#T_0f1b6_row3_col3,#T_0f1b6_row3_col4,#T_0f1b6_row3_col5,#T_0f1b6_row3_col7,#T_0f1b6_row3_col8,#T_0f1b6_row3_col9,#T_0f1b6_row4_col0,#T_0f1b6_row4_col2,#T_0f1b6_row4_col3,#T_0f1b6_row4_col4,#T_0f1b6_row4_col5,#T_0f1b6_row4_col6,#T_0f1b6_row4_col7,#T_0f1b6_row4_col8,#T_0f1b6_row4_col9,#T_0f1b6_row5_col0,#T_0f1b6_row5_col2,#T_0f1b6_row5_col3,#T_0f1b6_row5_col4,#T_0f1b6_row5_col5,#T_0f1b6_row5_col7,#T_0f1b6_row5_col8,#T_0f1b6_row5_col9,#T_0f1b6_row6_col0,#T_0f1b6_row6_col2,#T_0f1b6_row6_col3,#T_0f1b6_row6_col4,#T_0f1b6_row6_col5,#T_0f1b6_row6_col7,#T_0f1b6_row6_col8,#T_0f1b6_row6_col9,#T_0f1b6_row7_col0,#T_0f1b6_row7_col2,#T_0f1b6_row7_col3,#T_0f1b6_row7_col4,#T_0f1b6_row7_col5,#T_0f1b6_row7_col7,#T_0f1b6_row7_col8,#T_0f1b6_row7_col9,#T_0f1b6_row8_col0,#T_0f1b6_row8_col2,#T_0f1b6_row8_col3,#T_0f1b6_row8_col4,#T_0f1b6_row8_col5,#T_0f1b6_row8_col6,#T_0f1b6_row8_col7,#T_0f1b6_row8_col8,#T_0f1b6_row8_col9,#T_0f1b6_row9_col0,#T_0f1b6_row9_col2,#T_0f1b6_row9_col3,#T_0f1b6_row9_col4,#T_0f1b6_row9_col6,#T_0f1b6_row9_col7,#T_0f1b6_row9_col8,#T_0f1b6_row9_col9,#T_0f1b6_row10_col0,#T_0f1b6_row10_col2,#T_0f1b6_row10_col3,#T_0f1b6_row10_col4,#T_0f1b6_row10_col5,#T_0f1b6_row10_col6,#T_0f1b6_row10_col7,#T_0f1b6_row10_col8,#T_0f1b6_row10_col9,#T_0f1b6_row11_col0,#T_0f1b6_row11_col1,#T_0f1b6_row11_col2,#T_0f1b6_row11_col3,#T_0f1b6_row11_col4,#T_0f1b6_row11_col5,#T_0f1b6_row11_col7,#T_0f1b6_row11_col8,#T_0f1b6_row11_col9,#T_0f1b6_row12_col0,#T_0f1b6_row12_col2,#T_0f1b6_row12_col3,#T_0f1b6_row12_col4,#T_0f1b6_row12_col5,#T_0f1b6_row12_col6,#T_0f1b6_row12_col7,#T_0f1b6_row12_col8,#T_0f1b6_row12_col9,#T_0f1b6_row13_col0,#T_0f1b6_row13_col2,#T_0f1b6_row13_col3,#T_0f1b6_row13_col4,#T_0f1b6_row13_col5,#T_0f1b6_row13_col7,#T_0f1b6_row13_col8,#T_0f1b6_row13_col9,#T_0f1b6_row14_col0,#T_0f1b6_row14_col2,#T_0f1b6_row14_col3,#T_0f1b6_row14_col4,#T_0f1b6_row14_col5,#T_0f1b6_row14_col7,#T_0f1b6_row14_col8,#T_0f1b6_row14_col9,#T_0f1b6_row15_col0,#T_0f1b6_row15_col2,#T_0f1b6_row15_col3,#T_0f1b6_row15_col4,#T_0f1b6_row15_col5,#T_0f1b6_row15_col6,#T_0f1b6_row15_col7,#T_0f1b6_row15_col8,#T_0f1b6_row15_col9,#T_0f1b6_row16_col0,#T_0f1b6_row16_col2,#T_0f1b6_row16_col3,#T_0f1b6_row16_col4,#T_0f1b6_row16_col5,#T_0f1b6_row16_col6,#T_0f1b6_row16_col7,#T_0f1b6_row16_col8,#T_0f1b6_row16_col9,#T_0f1b6_row17_col0,#T_0f1b6_row17_col2,#T_0f1b6_row17_col3,#T_0f1b6_row17_col4,#T_0f1b6_row17_col5,#T_0f1b6_row17_col7,#T_0f1b6_row17_col8,#T_0f1b6_row17_col9,#T_0f1b6_row18_col0,#T_0f1b6_row18_col2,#T_0f1b6_row18_col3,#T_0f1b6_row18_col4,#T_0f1b6_row18_col5,#T_0f1b6_row18_col6,#T_0f1b6_row18_col7,#T_0f1b6_row18_col8,#T_0f1b6_row18_col9,#T_0f1b6_row19_col0,#T_0f1b6_row19_col2,#T_0f1b6_row19_col3,#T_0f1b6_row19_col4,#T_0f1b6_row19_col5,#T_0f1b6_row19_col6,#T_0f1b6_row19_col7,#T_0f1b6_row19_col8,#T_0f1b6_row19_col9,#T_0f1b6_row20_col0,#T_0f1b6_row20_col2,#T_0f1b6_row20_col3,#T_0f1b6_row20_col4,#T_0f1b6_row20_col5,#T_0f1b6_row20_col6,#T_0f1b6_row20_col7,#T_0f1b6_row20_col8,#T_0f1b6_row20_col9,#T_0f1b6_row21_col0,#T_0f1b6_row21_col2,#T_0f1b6_row21_col3,#T_0f1b6_row21_col4,#T_0f1b6_row21_col5,#T_0f1b6_row21_col6,#T_0f1b6_row21_col7,#T_0f1b6_row21_col8,#T_0f1b6_row21_col9,#T_0f1b6_row22_col0,#T_0f1b6_row22_col1,#T_0f1b6_row22_col2,#T_0f1b6_row22_col3,#T_0f1b6_row22_col5,#T_0f1b6_row22_col6,#T_0f1b6_row22_col7,#T_0f1b6_row22_col8,#T_0f1b6_row22_col9,#T_0f1b6_row23_col0,#T_0f1b6_row23_col2,#T_0f1b6_row23_col3,#T_0f1b6_row23_col4,#T_0f1b6_row23_col5,#T_0f1b6_row23_col6,#T_0f1b6_row23_col7,#T_0f1b6_row23_col8,#T_0f1b6_row23_col9,#T_0f1b6_row24_col0,#T_0f1b6_row24_col2,#T_0f1b6_row24_col3,#T_0f1b6_row24_col4,#T_0f1b6_row24_col6,#T_0f1b6_row24_col7,#T_0f1b6_row24_col8,#T_0f1b6_row24_col9,#T_0f1b6_row25_col0,#T_0f1b6_row25_col1,#T_0f1b6_row25_col2,#T_0f1b6_row25_col3,#T_0f1b6_row25_col4,#T_0f1b6_row25_col5,#T_0f1b6_row25_col7,#T_0f1b6_row25_col8,#T_0f1b6_row25_col9,#T_0f1b6_row26_col0,#T_0f1b6_row26_col2,#T_0f1b6_row26_col3,#T_0f1b6_row26_col4,#T_0f1b6_row26_col5,#T_0f1b6_row26_col6,#T_0f1b6_row26_col7,#T_0f1b6_row26_col8,#T_0f1b6_row26_col9,#T_0f1b6_row27_col0,#T_0f1b6_row27_col2,#T_0f1b6_row27_col3,#T_0f1b6_row27_col4,#T_0f1b6_row27_col5,#T_0f1b6_row27_col6,#T_0f1b6_row27_col7,#T_0f1b6_row27_col8,#T_0f1b6_row27_col9,#T_0f1b6_row28_col0,#T_0f1b6_row28_col1,#T_0f1b6_row28_col2,#T_0f1b6_row28_col3,#T_0f1b6_row28_col4,#T_0f1b6_row28_col5,#T_0f1b6_row28_col7,#T_0f1b6_row28_col8,#T_0f1b6_row28_col9,#T_0f1b6_row29_col0,#T_0f1b6_row29_col2,#T_0f1b6_row29_col3,#T_0f1b6_row29_col4,#T_0f1b6_row29_col5,#T_0f1b6_row29_col6,#T_0f1b6_row29_col7,#T_0f1b6_row29_col8,#T_0f1b6_row29_col9,#T_0f1b6_row30_col0,#T_0f1b6_row30_col2,#T_0f1b6_row30_col3,#T_0f1b6_row30_col4,#T_0f1b6_row30_col5,#T_0f1b6_row30_col7,#T_0f1b6_row30_col8,#T_0f1b6_row30_col9,#T_0f1b6_row31_col0,#T_0f1b6_row31_col2,#T_0f1b6_row31_col3,#T_0f1b6_row31_col4,#T_0f1b6_row31_col5,#T_0f1b6_row31_col7,#T_0f1b6_row31_col8,#T_0f1b6_row31_col9,#T_0f1b6_row32_col0,#T_0f1b6_row32_col2,#T_0f1b6_row32_col3,#T_0f1b6_row32_col5,#T_0f1b6_row32_col7,#T_0f1b6_row32_col8,#T_0f1b6_row32_col9,#T_0f1b6_row33_col0,#T_0f1b6_row33_col2,#T_0f1b6_row33_col3,#T_0f1b6_row33_col5,#T_0f1b6_row33_col7,#T_0f1b6_row33_col8,#T_0f1b6_row33_col9,#T_0f1b6_row34_col0,#T_0f1b6_row34_col2,#T_0f1b6_row34_col3,#T_0f1b6_row34_col4,#T_0f1b6_row34_col5,#T_0f1b6_row34_col7,#T_0f1b6_row34_col8,#T_0f1b6_row34_col9,#T_0f1b6_row35_col0,#T_0f1b6_row35_col2,#T_0f1b6_row35_col3,#T_0f1b6_row35_col4,#T_0f1b6_row35_col5,#T_0f1b6_row35_col6,#T_0f1b6_row35_col7,#T_0f1b6_row35_col8,#T_0f1b6_row35_col9,#T_0f1b6_row36_col0,#T_0f1b6_row36_col2,#T_0f1b6_row36_col3,#T_0f1b6_row36_col4,#T_0f1b6_row36_col5,#T_0f1b6_row36_col6,#T_0f1b6_row36_col7,#T_0f1b6_row36_col8,#T_0f1b6_row36_col9,#T_0f1b6_row37_col0,#T_0f1b6_row37_col1,#T_0f1b6_row37_col2,#T_0f1b6_row37_col3,#T_0f1b6_row37_col5,#T_0f1b6_row37_col7,#T_0f1b6_row37_col8,#T_0f1b6_row37_col9,#T_0f1b6_row38_col0,#T_0f1b6_row38_col2,#T_0f1b6_row38_col3,#T_0f1b6_row38_col4,#T_0f1b6_row38_col5,#T_0f1b6_row38_col7,#T_0f1b6_row38_col8,#T_0f1b6_row38_col9,#T_0f1b6_row39_col0,#T_0f1b6_row39_col2,#T_0f1b6_row39_col3,#T_0f1b6_row39_col4,#T_0f1b6_row39_col5,#T_0f1b6_row39_col6,#T_0f1b6_row39_col7,#T_0f1b6_row39_col8,#T_0f1b6_row39_col9,#T_0f1b6_row40_col0,#T_0f1b6_row40_col2,#T_0f1b6_row40_col3,#T_0f1b6_row40_col4,#T_0f1b6_row40_col5,#T_0f1b6_row40_col6,#T_0f1b6_row40_col7,#T_0f1b6_row40_col8,#T_0f1b6_row40_col9,#T_0f1b6_row41_col0,#T_0f1b6_row41_col2,#T_0f1b6_row41_col3,#T_0f1b6_row41_col4,#T_0f1b6_row41_col5,#T_0f1b6_row41_col6,#T_0f1b6_row41_col7,#T_0f1b6_row41_col8,#T_0f1b6_row41_col9,#T_0f1b6_row42_col0,#T_0f1b6_row42_col2,#T_0f1b6_row42_col3,#T_0f1b6_row42_col4,#T_0f1b6_row42_col5,#T_0f1b6_row42_col6,#T_0f1b6_row42_col7,#T_0f1b6_row42_col8,#T_0f1b6_row42_col9,#T_0f1b6_row43_col0,#T_0f1b6_row43_col1,#T_0f1b6_row43_col2,#T_0f1b6_row43_col3,#T_0f1b6_row43_col5,#T_0f1b6_row43_col6,#T_0f1b6_row43_col7,#T_0f1b6_row43_col8,#T_0f1b6_row43_col9,#T_0f1b6_row44_col0,#T_0f1b6_row44_col2,#T_0f1b6_row44_col3,#T_0f1b6_row44_col4,#T_0f1b6_row44_col5,#T_0f1b6_row44_col6,#T_0f1b6_row44_col7,#T_0f1b6_row44_col8,#T_0f1b6_row44_col9,#T_0f1b6_row45_col0,#T_0f1b6_row45_col2,#T_0f1b6_row45_col3,#T_0f1b6_row45_col4,#T_0f1b6_row45_col5,#T_0f1b6_row45_col6,#T_0f1b6_row45_col7,#T_0f1b6_row45_col8,#T_0f1b6_row45_col9,#T_0f1b6_row46_col0,#T_0f1b6_row46_col2,#T_0f1b6_row46_col3,#T_0f1b6_row46_col4,#T_0f1b6_row46_col5,#T_0f1b6_row46_col6,#T_0f1b6_row46_col7,#T_0f1b6_row46_col8,#T_0f1b6_row46_col9,#T_0f1b6_row47_col0,#T_0f1b6_row47_col2,#T_0f1b6_row47_col3,#T_0f1b6_row47_col4,#T_0f1b6_row47_col5,#T_0f1b6_row47_col6,#T_0f1b6_row47_col7,#T_0f1b6_row47_col8,#T_0f1b6_row47_col9,#T_0f1b6_row48_col0,#T_0f1b6_row48_col2,#T_0f1b6_row48_col3,#T_0f1b6_row48_col4,#T_0f1b6_row48_col5,#T_0f1b6_row48_col6,#T_0f1b6_row48_col7,#T_0f1b6_row48_col8,#T_0f1b6_row48_col9,#T_0f1b6_row49_col0,#T_0f1b6_row49_col2,#T_0f1b6_row49_col3,#T_0f1b6_row49_col5,#T_0f1b6_row49_col6,#T_0f1b6_row49_col7,#T_0f1b6_row49_col8,#T_0f1b6_row49_col9,#T_0f1b6_row50_col0,#T_0f1b6_row50_col2,#T_0f1b6_row50_col3,#T_0f1b6_row50_col4,#T_0f1b6_row50_col5,#T_0f1b6_row50_col6,#T_0f1b6_row50_col7,#T_0f1b6_row50_col8,#T_0f1b6_row50_col9,#T_0f1b6_row51_col0,#T_0f1b6_row51_col2,#T_0f1b6_row51_col3,#T_0f1b6_row51_col4,#T_0f1b6_row51_col5,#T_0f1b6_row51_col6,#T_0f1b6_row51_col7,#T_0f1b6_row51_col8,#T_0f1b6_row51_col9,#T_0f1b6_row52_col0,#T_0f1b6_row52_col2,#T_0f1b6_row52_col3,#T_0f1b6_row52_col4,#T_0f1b6_row52_col5,#T_0f1b6_row52_col6,#T_0f1b6_row52_col7,#T_0f1b6_row52_col8,#T_0f1b6_row52_col9,#T_0f1b6_row53_col0,#T_0f1b6_row53_col2,#T_0f1b6_row53_col3,#T_0f1b6_row53_col4,#T_0f1b6_row53_col5,#T_0f1b6_row53_col6,#T_0f1b6_row53_col7,#T_0f1b6_row53_col8,#T_0f1b6_row53_col9,#T_0f1b6_row54_col0,#T_0f1b6_row54_col2,#T_0f1b6_row54_col3,#T_0f1b6_row54_col4,#T_0f1b6_row54_col5,#T_0f1b6_row54_col7,#T_0f1b6_row54_col8,#T_0f1b6_row54_col9,#T_0f1b6_row55_col0,#T_0f1b6_row55_col2,#T_0f1b6_row55_col3,#T_0f1b6_row55_col4,#T_0f1b6_row55_col5,#T_0f1b6_row55_col7,#T_0f1b6_row55_col8,#T_0f1b6_row55_col9,#T_0f1b6_row56_col0,#T_0f1b6_row56_col2,#T_0f1b6_row56_col3,#T_0f1b6_row56_col4,#T_0f1b6_row56_col5,#T_0f1b6_row56_col6,#T_0f1b6_row56_col7,#T_0f1b6_row56_col8,#T_0f1b6_row56_col9,#T_0f1b6_row57_col0,#T_0f1b6_row57_col2,#T_0f1b6_row57_col3,#T_0f1b6_row57_col4,#T_0f1b6_row57_col5,#T_0f1b6_row57_col7,#T_0f1b6_row57_col8,#T_0f1b6_row57_col9,#T_0f1b6_row58_col0,#T_0f1b6_row58_col2,#T_0f1b6_row58_col3,#T_0f1b6_row58_col4,#T_0f1b6_row58_col5,#T_0f1b6_row58_col6,#T_0f1b6_row58_col7,#T_0f1b6_row58_col8,#T_0f1b6_row58_col9,#T_0f1b6_row59_col0,#T_0f1b6_row59_col1,#T_0f1b6_row59_col2,#T_0f1b6_row59_col3,#T_0f1b6_row59_col4,#T_0f1b6_row59_col5,#T_0f1b6_row59_col7,#T_0f1b6_row59_col8,#T_0f1b6_row59_col9,#T_0f1b6_row60_col0,#T_0f1b6_row60_col2,#T_0f1b6_row60_col3,#T_0f1b6_row60_col5,#T_0f1b6_row60_col7,#T_0f1b6_row60_col8,#T_0f1b6_row60_col9,#T_0f1b6_row61_col0,#T_0f1b6_row61_col2,#T_0f1b6_row61_col3,#T_0f1b6_row61_col4,#T_0f1b6_row61_col5,#T_0f1b6_row61_col6,#T_0f1b6_row61_col7,#T_0f1b6_row61_col8,#T_0f1b6_row61_col9,#T_0f1b6_row62_col0,#T_0f1b6_row62_col2,#T_0f1b6_row62_col3,#T_0f1b6_row62_col4,#T_0f1b6_row62_col5,#T_0f1b6_row62_col6,#T_0f1b6_row62_col7,#T_0f1b6_row62_col8,#T_0f1b6_row62_col9,#T_0f1b6_row63_col0,#T_0f1b6_row63_col2,#T_0f1b6_row63_col3,#T_0f1b6_row63_col4,#T_0f1b6_row63_col5,#T_0f1b6_row63_col6,#T_0f1b6_row63_col7,#T_0f1b6_row63_col8,#T_0f1b6_row63_col9,#T_0f1b6_row64_col0,#T_0f1b6_row64_col2,#T_0f1b6_row64_col3,#T_0f1b6_row64_col4,#T_0f1b6_row64_col5,#T_0f1b6_row64_col7,#T_0f1b6_row64_col8,#T_0f1b6_row64_col9,#T_0f1b6_row65_col0,#T_0f1b6_row65_col2,#T_0f1b6_row65_col3,#T_0f1b6_row65_col4,#T_0f1b6_row65_col5,#T_0f1b6_row65_col7,#T_0f1b6_row65_col8,#T_0f1b6_row65_col9,#T_0f1b6_row66_col0,#T_0f1b6_row66_col1,#T_0f1b6_row66_col2,#T_0f1b6_row66_col3,#T_0f1b6_row66_col4,#T_0f1b6_row66_col5,#T_0f1b6_row66_col7,#T_0f1b6_row66_col8,#T_0f1b6_row66_col9,#T_0f1b6_row67_col0,#T_0f1b6_row67_col2,#T_0f1b6_row67_col3,#T_0f1b6_row67_col4,#T_0f1b6_row67_col5,#T_0f1b6_row67_col6,#T_0f1b6_row67_col7,#T_0f1b6_row67_col8,#T_0f1b6_row67_col9,#T_0f1b6_row68_col0,#T_0f1b6_row68_col2,#T_0f1b6_row68_col3,#T_0f1b6_row68_col4,#T_0f1b6_row68_col5,#T_0f1b6_row68_col6,#T_0f1b6_row68_col7,#T_0f1b6_row68_col8,#T_0f1b6_row68_col9,#T_0f1b6_row69_col0,#T_0f1b6_row69_col1,#T_0f1b6_row69_col2,#T_0f1b6_row69_col3,#T_0f1b6_row69_col4,#T_0f1b6_row69_col5,#T_0f1b6_row69_col7,#T_0f1b6_row69_col8,#T_0f1b6_row69_col9,#T_0f1b6_row70_col0,#T_0f1b6_row70_col2,#T_0f1b6_row70_col3,#T_0f1b6_row70_col4,#T_0f1b6_row70_col5,#T_0f1b6_row70_col6,#T_0f1b6_row70_col7,#T_0f1b6_row70_col8,#T_0f1b6_row70_col9,#T_0f1b6_row71_col0,#T_0f1b6_row71_col2,#T_0f1b6_row71_col3,#T_0f1b6_row71_col4,#T_0f1b6_row71_col5,#T_0f1b6_row71_col6,#T_0f1b6_row71_col7,#T_0f1b6_row71_col8,#T_0f1b6_row71_col9,#T_0f1b6_row72_col0,#T_0f1b6_row72_col2,#T_0f1b6_row72_col3,#T_0f1b6_row72_col4,#T_0f1b6_row72_col5,#T_0f1b6_row72_col6,#T_0f1b6_row72_col7,#T_0f1b6_row72_col8,#T_0f1b6_row72_col9,#T_0f1b6_row73_col0,#T_0f1b6_row73_col2,#T_0f1b6_row73_col3,#T_0f1b6_row73_col4,#T_0f1b6_row73_col5,#T_0f1b6_row73_col6,#T_0f1b6_row73_col7,#T_0f1b6_row73_col8,#T_0f1b6_row73_col9,#T_0f1b6_row74_col0,#T_0f1b6_row74_col2,#T_0f1b6_row74_col3,#T_0f1b6_row74_col4,#T_0f1b6_row74_col6,#T_0f1b6_row74_col7,#T_0f1b6_row74_col8,#T_0f1b6_row74_col9,#T_0f1b6_row75_col0,#T_0f1b6_row75_col2,#T_0f1b6_row75_col3,#T_0f1b6_row75_col5,#T_0f1b6_row75_col6,#T_0f1b6_row75_col7,#T_0f1b6_row75_col8,#T_0f1b6_row75_col9,#T_0f1b6_row76_col0,#T_0f1b6_row76_col2,#T_0f1b6_row76_col3,#T_0f1b6_row76_col4,#T_0f1b6_row76_col5,#T_0f1b6_row76_col6,#T_0f1b6_row76_col7,#T_0f1b6_row76_col8,#T_0f1b6_row76_col9,#T_0f1b6_row77_col0,#T_0f1b6_row77_col2,#T_0f1b6_row77_col3,#T_0f1b6_row77_col4,#T_0f1b6_row77_col5,#T_0f1b6_row77_col7,#T_0f1b6_row77_col8,#T_0f1b6_row77_col9,#T_0f1b6_row78_col0,#T_0f1b6_row78_col2,#T_0f1b6_row78_col3,#T_0f1b6_row78_col4,#T_0f1b6_row78_col5,#T_0f1b6_row78_col6,#T_0f1b6_row78_col7,#T_0f1b6_row78_col8,#T_0f1b6_row78_col9,#T_0f1b6_row79_col0,#T_0f1b6_row79_col2,#T_0f1b6_row79_col3,#T_0f1b6_row79_col4,#T_0f1b6_row79_col5,#T_0f1b6_row79_col6,#T_0f1b6_row79_col7,#T_0f1b6_row79_col8,#T_0f1b6_row79_col9,#T_0f1b6_row80_col0,#T_0f1b6_row80_col2,#T_0f1b6_row80_col3,#T_0f1b6_row80_col4,#T_0f1b6_row80_col5,#T_0f1b6_row80_col7,#T_0f1b6_row80_col8,#T_0f1b6_row80_col9,#T_0f1b6_row81_col0,#T_0f1b6_row81_col2,#T_0f1b6_row81_col3,#T_0f1b6_row81_col4,#T_0f1b6_row81_col5,#T_0f1b6_row81_col7,#T_0f1b6_row81_col8,#T_0f1b6_row81_col9,#T_0f1b6_row82_col0,#T_0f1b6_row82_col2,#T_0f1b6_row82_col3,#T_0f1b6_row82_col4,#T_0f1b6_row82_col5,#T_0f1b6_row82_col6,#T_0f1b6_row82_col7,#T_0f1b6_row82_col8,#T_0f1b6_row82_col9,#T_0f1b6_row83_col0,#T_0f1b6_row83_col1,#T_0f1b6_row83_col2,#T_0f1b6_row83_col3,#T_0f1b6_row83_col4,#T_0f1b6_row83_col5,#T_0f1b6_row83_col7,#T_0f1b6_row83_col8,#T_0f1b6_row83_col9,#T_0f1b6_row84_col0,#T_0f1b6_row84_col2,#T_0f1b6_row84_col3,#T_0f1b6_row84_col4,#T_0f1b6_row84_col5,#T_0f1b6_row84_col7,#T_0f1b6_row84_col8,#T_0f1b6_row84_col9,#T_0f1b6_row85_col0,#T_0f1b6_row85_col2,#T_0f1b6_row85_col3,#T_0f1b6_row85_col4,#T_0f1b6_row85_col7,#T_0f1b6_row85_col8,#T_0f1b6_row85_col9,#T_0f1b6_row86_col0,#T_0f1b6_row86_col1,#T_0f1b6_row86_col2,#T_0f1b6_row86_col3,#T_0f1b6_row86_col4,#T_0f1b6_row86_col5,#T_0f1b6_row86_col7,#T_0f1b6_row86_col8,#T_0f1b6_row86_col9,#T_0f1b6_row87_col0,#T_0f1b6_row87_col2,#T_0f1b6_row87_col3,#T_0f1b6_row87_col4,#T_0f1b6_row87_col5,#T_0f1b6_row87_col6,#T_0f1b6_row87_col7,#T_0f1b6_row87_col8,#T_0f1b6_row87_col9{\n",
       "            color:  black;\n",
       "            font-weight:  400;\n",
       "        }#T_0f1b6_row0_col1,#T_0f1b6_row0_col10,#T_0f1b6_row1_col1,#T_0f1b6_row1_col10,#T_0f1b6_row2_col1,#T_0f1b6_row2_col10,#T_0f1b6_row3_col1,#T_0f1b6_row3_col6,#T_0f1b6_row3_col10,#T_0f1b6_row4_col1,#T_0f1b6_row4_col10,#T_0f1b6_row5_col1,#T_0f1b6_row5_col6,#T_0f1b6_row5_col10,#T_0f1b6_row6_col1,#T_0f1b6_row6_col6,#T_0f1b6_row6_col10,#T_0f1b6_row7_col1,#T_0f1b6_row7_col6,#T_0f1b6_row7_col10,#T_0f1b6_row8_col1,#T_0f1b6_row8_col10,#T_0f1b6_row9_col1,#T_0f1b6_row9_col5,#T_0f1b6_row9_col10,#T_0f1b6_row10_col1,#T_0f1b6_row10_col10,#T_0f1b6_row11_col6,#T_0f1b6_row11_col10,#T_0f1b6_row12_col1,#T_0f1b6_row12_col10,#T_0f1b6_row13_col1,#T_0f1b6_row13_col6,#T_0f1b6_row13_col10,#T_0f1b6_row14_col1,#T_0f1b6_row14_col6,#T_0f1b6_row14_col10,#T_0f1b6_row15_col1,#T_0f1b6_row15_col10,#T_0f1b6_row16_col1,#T_0f1b6_row16_col10,#T_0f1b6_row17_col1,#T_0f1b6_row17_col6,#T_0f1b6_row17_col10,#T_0f1b6_row18_col1,#T_0f1b6_row18_col10,#T_0f1b6_row19_col1,#T_0f1b6_row19_col10,#T_0f1b6_row20_col1,#T_0f1b6_row20_col10,#T_0f1b6_row21_col1,#T_0f1b6_row21_col10,#T_0f1b6_row22_col4,#T_0f1b6_row22_col10,#T_0f1b6_row23_col1,#T_0f1b6_row23_col10,#T_0f1b6_row24_col1,#T_0f1b6_row24_col5,#T_0f1b6_row24_col10,#T_0f1b6_row25_col6,#T_0f1b6_row25_col10,#T_0f1b6_row26_col1,#T_0f1b6_row26_col10,#T_0f1b6_row27_col1,#T_0f1b6_row27_col10,#T_0f1b6_row28_col6,#T_0f1b6_row28_col10,#T_0f1b6_row29_col1,#T_0f1b6_row29_col10,#T_0f1b6_row30_col1,#T_0f1b6_row30_col6,#T_0f1b6_row30_col10,#T_0f1b6_row31_col1,#T_0f1b6_row31_col6,#T_0f1b6_row31_col10,#T_0f1b6_row32_col1,#T_0f1b6_row32_col4,#T_0f1b6_row32_col6,#T_0f1b6_row32_col10,#T_0f1b6_row33_col1,#T_0f1b6_row33_col4,#T_0f1b6_row33_col6,#T_0f1b6_row33_col10,#T_0f1b6_row34_col1,#T_0f1b6_row34_col6,#T_0f1b6_row34_col10,#T_0f1b6_row35_col1,#T_0f1b6_row35_col10,#T_0f1b6_row36_col1,#T_0f1b6_row36_col10,#T_0f1b6_row37_col4,#T_0f1b6_row37_col6,#T_0f1b6_row37_col10,#T_0f1b6_row38_col1,#T_0f1b6_row38_col6,#T_0f1b6_row38_col10,#T_0f1b6_row39_col1,#T_0f1b6_row39_col10,#T_0f1b6_row40_col1,#T_0f1b6_row40_col10,#T_0f1b6_row41_col1,#T_0f1b6_row41_col10,#T_0f1b6_row42_col1,#T_0f1b6_row42_col10,#T_0f1b6_row43_col4,#T_0f1b6_row43_col10,#T_0f1b6_row44_col1,#T_0f1b6_row44_col10,#T_0f1b6_row45_col1,#T_0f1b6_row45_col10,#T_0f1b6_row46_col1,#T_0f1b6_row46_col10,#T_0f1b6_row47_col1,#T_0f1b6_row47_col10,#T_0f1b6_row48_col1,#T_0f1b6_row48_col10,#T_0f1b6_row49_col1,#T_0f1b6_row49_col4,#T_0f1b6_row49_col10,#T_0f1b6_row50_col1,#T_0f1b6_row50_col10,#T_0f1b6_row51_col1,#T_0f1b6_row51_col10,#T_0f1b6_row52_col1,#T_0f1b6_row52_col10,#T_0f1b6_row53_col1,#T_0f1b6_row53_col10,#T_0f1b6_row54_col1,#T_0f1b6_row54_col6,#T_0f1b6_row54_col10,#T_0f1b6_row55_col1,#T_0f1b6_row55_col6,#T_0f1b6_row55_col10,#T_0f1b6_row56_col1,#T_0f1b6_row56_col10,#T_0f1b6_row57_col1,#T_0f1b6_row57_col6,#T_0f1b6_row57_col10,#T_0f1b6_row58_col1,#T_0f1b6_row58_col10,#T_0f1b6_row59_col6,#T_0f1b6_row59_col10,#T_0f1b6_row60_col1,#T_0f1b6_row60_col4,#T_0f1b6_row60_col6,#T_0f1b6_row60_col10,#T_0f1b6_row61_col1,#T_0f1b6_row61_col10,#T_0f1b6_row62_col1,#T_0f1b6_row62_col10,#T_0f1b6_row63_col1,#T_0f1b6_row63_col10,#T_0f1b6_row64_col1,#T_0f1b6_row64_col6,#T_0f1b6_row64_col10,#T_0f1b6_row65_col1,#T_0f1b6_row65_col6,#T_0f1b6_row65_col10,#T_0f1b6_row66_col6,#T_0f1b6_row66_col10,#T_0f1b6_row67_col1,#T_0f1b6_row67_col10,#T_0f1b6_row68_col1,#T_0f1b6_row68_col10,#T_0f1b6_row69_col6,#T_0f1b6_row69_col10,#T_0f1b6_row70_col1,#T_0f1b6_row70_col10,#T_0f1b6_row71_col1,#T_0f1b6_row71_col10,#T_0f1b6_row72_col1,#T_0f1b6_row72_col10,#T_0f1b6_row73_col1,#T_0f1b6_row73_col10,#T_0f1b6_row74_col1,#T_0f1b6_row74_col5,#T_0f1b6_row74_col10,#T_0f1b6_row75_col1,#T_0f1b6_row75_col4,#T_0f1b6_row75_col10,#T_0f1b6_row76_col1,#T_0f1b6_row76_col10,#T_0f1b6_row77_col1,#T_0f1b6_row77_col6,#T_0f1b6_row77_col10,#T_0f1b6_row78_col1,#T_0f1b6_row78_col10,#T_0f1b6_row79_col1,#T_0f1b6_row79_col10,#T_0f1b6_row80_col1,#T_0f1b6_row80_col6,#T_0f1b6_row80_col10,#T_0f1b6_row81_col1,#T_0f1b6_row81_col6,#T_0f1b6_row81_col10,#T_0f1b6_row82_col1,#T_0f1b6_row82_col10,#T_0f1b6_row83_col6,#T_0f1b6_row83_col10,#T_0f1b6_row84_col1,#T_0f1b6_row84_col6,#T_0f1b6_row84_col10,#T_0f1b6_row85_col1,#T_0f1b6_row85_col5,#T_0f1b6_row85_col6,#T_0f1b6_row85_col10,#T_0f1b6_row86_col6,#T_0f1b6_row86_col10,#T_0f1b6_row87_col1,#T_0f1b6_row87_col10{\n",
       "            color:  green;\n",
       "            font-weight:  700;\n",
       "        }</style><table id=\"T_0f1b6_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Topic0</th>        <th class=\"col_heading level0 col1\" >Topic1</th>        <th class=\"col_heading level0 col2\" >Topic2</th>        <th class=\"col_heading level0 col3\" >Topic3</th>        <th class=\"col_heading level0 col4\" >Topic4</th>        <th class=\"col_heading level0 col5\" >Topic5</th>        <th class=\"col_heading level0 col6\" >Topic6</th>        <th class=\"col_heading level0 col7\" >Topic7</th>        <th class=\"col_heading level0 col8\" >Topic8</th>        <th class=\"col_heading level0 col9\" >Topic9</th>        <th class=\"col_heading level0 col10\" >dominant_topic</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row0\" class=\"row_heading level0 row0\" >Doc0</th>\n",
       "                        <td id=\"T_0f1b6_row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row0_col8\" class=\"data row0 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row0_col9\" class=\"data row0 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row0_col10\" class=\"data row0 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row1\" class=\"row_heading level0 row1\" >Doc1</th>\n",
       "                        <td id=\"T_0f1b6_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row1_col7\" class=\"data row1 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row1_col8\" class=\"data row1 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row1_col9\" class=\"data row1 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row1_col10\" class=\"data row1 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row2\" class=\"row_heading level0 row2\" >Doc2</th>\n",
       "                        <td id=\"T_0f1b6_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row2_col1\" class=\"data row2 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row2_col8\" class=\"data row2 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row2_col9\" class=\"data row2 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row2_col10\" class=\"data row2 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row3\" class=\"row_heading level0 row3\" >Doc3</th>\n",
       "                        <td id=\"T_0f1b6_row3_col0\" class=\"data row3 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row3_col1\" class=\"data row3 col1\" >0.210000</td>\n",
       "                        <td id=\"T_0f1b6_row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row3_col4\" class=\"data row3 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row3_col6\" class=\"data row3 col6\" >0.790000</td>\n",
       "                        <td id=\"T_0f1b6_row3_col7\" class=\"data row3 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row3_col8\" class=\"data row3 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row3_col9\" class=\"data row3 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row3_col10\" class=\"data row3 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row4\" class=\"row_heading level0 row4\" >Doc4</th>\n",
       "                        <td id=\"T_0f1b6_row4_col0\" class=\"data row4 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row4_col1\" class=\"data row4 col1\" >0.900000</td>\n",
       "                        <td id=\"T_0f1b6_row4_col2\" class=\"data row4 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row4_col4\" class=\"data row4 col4\" >0.100000</td>\n",
       "                        <td id=\"T_0f1b6_row4_col5\" class=\"data row4 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row4_col6\" class=\"data row4 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row4_col7\" class=\"data row4 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row4_col8\" class=\"data row4 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row4_col9\" class=\"data row4 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row4_col10\" class=\"data row4 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row5\" class=\"row_heading level0 row5\" >Doc5</th>\n",
       "                        <td id=\"T_0f1b6_row5_col0\" class=\"data row5 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row5_col1\" class=\"data row5 col1\" >0.120000</td>\n",
       "                        <td id=\"T_0f1b6_row5_col2\" class=\"data row5 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row5_col3\" class=\"data row5 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row5_col4\" class=\"data row5 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row5_col5\" class=\"data row5 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row5_col6\" class=\"data row5 col6\" >0.880000</td>\n",
       "                        <td id=\"T_0f1b6_row5_col7\" class=\"data row5 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row5_col8\" class=\"data row5 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row5_col9\" class=\"data row5 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row5_col10\" class=\"data row5 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row6\" class=\"row_heading level0 row6\" >Doc6</th>\n",
       "                        <td id=\"T_0f1b6_row6_col0\" class=\"data row6 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row6_col1\" class=\"data row6 col1\" >0.440000</td>\n",
       "                        <td id=\"T_0f1b6_row6_col2\" class=\"data row6 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row6_col4\" class=\"data row6 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row6_col5\" class=\"data row6 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row6_col6\" class=\"data row6 col6\" >0.560000</td>\n",
       "                        <td id=\"T_0f1b6_row6_col7\" class=\"data row6 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row6_col8\" class=\"data row6 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row6_col9\" class=\"data row6 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row6_col10\" class=\"data row6 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row7\" class=\"row_heading level0 row7\" >Doc7</th>\n",
       "                        <td id=\"T_0f1b6_row7_col0\" class=\"data row7 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row7_col1\" class=\"data row7 col1\" >0.120000</td>\n",
       "                        <td id=\"T_0f1b6_row7_col2\" class=\"data row7 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row7_col3\" class=\"data row7 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row7_col4\" class=\"data row7 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row7_col5\" class=\"data row7 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row7_col6\" class=\"data row7 col6\" >0.880000</td>\n",
       "                        <td id=\"T_0f1b6_row7_col7\" class=\"data row7 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row7_col8\" class=\"data row7 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row7_col9\" class=\"data row7 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row7_col10\" class=\"data row7 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row8\" class=\"row_heading level0 row8\" >Doc8</th>\n",
       "                        <td id=\"T_0f1b6_row8_col0\" class=\"data row8 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row8_col1\" class=\"data row8 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row8_col2\" class=\"data row8 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row8_col3\" class=\"data row8 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row8_col4\" class=\"data row8 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row8_col5\" class=\"data row8 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row8_col6\" class=\"data row8 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row8_col7\" class=\"data row8 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row8_col8\" class=\"data row8 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row8_col9\" class=\"data row8 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row8_col10\" class=\"data row8 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row9\" class=\"row_heading level0 row9\" >Doc9</th>\n",
       "                        <td id=\"T_0f1b6_row9_col0\" class=\"data row9 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row9_col1\" class=\"data row9 col1\" >0.430000</td>\n",
       "                        <td id=\"T_0f1b6_row9_col2\" class=\"data row9 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row9_col3\" class=\"data row9 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row9_col4\" class=\"data row9 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row9_col5\" class=\"data row9 col5\" >0.500000</td>\n",
       "                        <td id=\"T_0f1b6_row9_col6\" class=\"data row9 col6\" >0.070000</td>\n",
       "                        <td id=\"T_0f1b6_row9_col7\" class=\"data row9 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row9_col8\" class=\"data row9 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row9_col9\" class=\"data row9 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row9_col10\" class=\"data row9 col10\" >5</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row10\" class=\"row_heading level0 row10\" >Doc10</th>\n",
       "                        <td id=\"T_0f1b6_row10_col0\" class=\"data row10 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row10_col1\" class=\"data row10 col1\" >0.920000</td>\n",
       "                        <td id=\"T_0f1b6_row10_col2\" class=\"data row10 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row10_col3\" class=\"data row10 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row10_col4\" class=\"data row10 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row10_col5\" class=\"data row10 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row10_col6\" class=\"data row10 col6\" >0.080000</td>\n",
       "                        <td id=\"T_0f1b6_row10_col7\" class=\"data row10 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row10_col8\" class=\"data row10 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row10_col9\" class=\"data row10 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row10_col10\" class=\"data row10 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row11\" class=\"row_heading level0 row11\" >Doc11</th>\n",
       "                        <td id=\"T_0f1b6_row11_col0\" class=\"data row11 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row11_col1\" class=\"data row11 col1\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row11_col2\" class=\"data row11 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row11_col3\" class=\"data row11 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row11_col4\" class=\"data row11 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row11_col5\" class=\"data row11 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row11_col6\" class=\"data row11 col6\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row11_col7\" class=\"data row11 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row11_col8\" class=\"data row11 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row11_col9\" class=\"data row11 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row11_col10\" class=\"data row11 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row12\" class=\"row_heading level0 row12\" >Doc12</th>\n",
       "                        <td id=\"T_0f1b6_row12_col0\" class=\"data row12 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row12_col1\" class=\"data row12 col1\" >0.970000</td>\n",
       "                        <td id=\"T_0f1b6_row12_col2\" class=\"data row12 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row12_col3\" class=\"data row12 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row12_col4\" class=\"data row12 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row12_col5\" class=\"data row12 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row12_col6\" class=\"data row12 col6\" >0.030000</td>\n",
       "                        <td id=\"T_0f1b6_row12_col7\" class=\"data row12 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row12_col8\" class=\"data row12 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row12_col9\" class=\"data row12 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row12_col10\" class=\"data row12 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row13\" class=\"row_heading level0 row13\" >Doc13</th>\n",
       "                        <td id=\"T_0f1b6_row13_col0\" class=\"data row13 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row13_col1\" class=\"data row13 col1\" >0.290000</td>\n",
       "                        <td id=\"T_0f1b6_row13_col2\" class=\"data row13 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row13_col3\" class=\"data row13 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row13_col4\" class=\"data row13 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row13_col5\" class=\"data row13 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row13_col6\" class=\"data row13 col6\" >0.710000</td>\n",
       "                        <td id=\"T_0f1b6_row13_col7\" class=\"data row13 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row13_col8\" class=\"data row13 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row13_col9\" class=\"data row13 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row13_col10\" class=\"data row13 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row14\" class=\"row_heading level0 row14\" >Doc14</th>\n",
       "                        <td id=\"T_0f1b6_row14_col0\" class=\"data row14 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row14_col1\" class=\"data row14 col1\" >0.440000</td>\n",
       "                        <td id=\"T_0f1b6_row14_col2\" class=\"data row14 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row14_col3\" class=\"data row14 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row14_col4\" class=\"data row14 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row14_col5\" class=\"data row14 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row14_col6\" class=\"data row14 col6\" >0.560000</td>\n",
       "                        <td id=\"T_0f1b6_row14_col7\" class=\"data row14 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row14_col8\" class=\"data row14 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row14_col9\" class=\"data row14 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row14_col10\" class=\"data row14 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row15\" class=\"row_heading level0 row15\" >Doc15</th>\n",
       "                        <td id=\"T_0f1b6_row15_col0\" class=\"data row15 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row15_col1\" class=\"data row15 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row15_col2\" class=\"data row15 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row15_col3\" class=\"data row15 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row15_col4\" class=\"data row15 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row15_col5\" class=\"data row15 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row15_col6\" class=\"data row15 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row15_col7\" class=\"data row15 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row15_col8\" class=\"data row15 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row15_col9\" class=\"data row15 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row15_col10\" class=\"data row15 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row16\" class=\"row_heading level0 row16\" >Doc16</th>\n",
       "                        <td id=\"T_0f1b6_row16_col0\" class=\"data row16 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row16_col1\" class=\"data row16 col1\" >0.980000</td>\n",
       "                        <td id=\"T_0f1b6_row16_col2\" class=\"data row16 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row16_col3\" class=\"data row16 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row16_col4\" class=\"data row16 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row16_col5\" class=\"data row16 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row16_col6\" class=\"data row16 col6\" >0.020000</td>\n",
       "                        <td id=\"T_0f1b6_row16_col7\" class=\"data row16 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row16_col8\" class=\"data row16 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row16_col9\" class=\"data row16 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row16_col10\" class=\"data row16 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row17\" class=\"row_heading level0 row17\" >Doc17</th>\n",
       "                        <td id=\"T_0f1b6_row17_col0\" class=\"data row17 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row17_col1\" class=\"data row17 col1\" >0.710000</td>\n",
       "                        <td id=\"T_0f1b6_row17_col2\" class=\"data row17 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row17_col3\" class=\"data row17 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row17_col4\" class=\"data row17 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row17_col5\" class=\"data row17 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row17_col6\" class=\"data row17 col6\" >0.290000</td>\n",
       "                        <td id=\"T_0f1b6_row17_col7\" class=\"data row17 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row17_col8\" class=\"data row17 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row17_col9\" class=\"data row17 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row17_col10\" class=\"data row17 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row18\" class=\"row_heading level0 row18\" >Doc18</th>\n",
       "                        <td id=\"T_0f1b6_row18_col0\" class=\"data row18 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row18_col1\" class=\"data row18 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row18_col2\" class=\"data row18 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row18_col3\" class=\"data row18 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row18_col4\" class=\"data row18 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row18_col5\" class=\"data row18 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row18_col6\" class=\"data row18 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row18_col7\" class=\"data row18 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row18_col8\" class=\"data row18 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row18_col9\" class=\"data row18 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row18_col10\" class=\"data row18 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row19\" class=\"row_heading level0 row19\" >Doc19</th>\n",
       "                        <td id=\"T_0f1b6_row19_col0\" class=\"data row19 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row19_col1\" class=\"data row19 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row19_col2\" class=\"data row19 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row19_col3\" class=\"data row19 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row19_col4\" class=\"data row19 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row19_col5\" class=\"data row19 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row19_col6\" class=\"data row19 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row19_col7\" class=\"data row19 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row19_col8\" class=\"data row19 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row19_col9\" class=\"data row19 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row19_col10\" class=\"data row19 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row20\" class=\"row_heading level0 row20\" >Doc20</th>\n",
       "                        <td id=\"T_0f1b6_row20_col0\" class=\"data row20 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row20_col1\" class=\"data row20 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row20_col2\" class=\"data row20 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row20_col3\" class=\"data row20 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row20_col4\" class=\"data row20 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row20_col5\" class=\"data row20 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row20_col6\" class=\"data row20 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row20_col7\" class=\"data row20 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row20_col8\" class=\"data row20 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row20_col9\" class=\"data row20 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row20_col10\" class=\"data row20 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row21\" class=\"row_heading level0 row21\" >Doc21</th>\n",
       "                        <td id=\"T_0f1b6_row21_col0\" class=\"data row21 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row21_col1\" class=\"data row21 col1\" >0.960000</td>\n",
       "                        <td id=\"T_0f1b6_row21_col2\" class=\"data row21 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row21_col3\" class=\"data row21 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row21_col4\" class=\"data row21 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row21_col5\" class=\"data row21 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row21_col6\" class=\"data row21 col6\" >0.040000</td>\n",
       "                        <td id=\"T_0f1b6_row21_col7\" class=\"data row21 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row21_col8\" class=\"data row21 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row21_col9\" class=\"data row21 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row21_col10\" class=\"data row21 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row22\" class=\"row_heading level0 row22\" >Doc22</th>\n",
       "                        <td id=\"T_0f1b6_row22_col0\" class=\"data row22 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row22_col1\" class=\"data row22 col1\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row22_col2\" class=\"data row22 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row22_col3\" class=\"data row22 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row22_col4\" class=\"data row22 col4\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row22_col5\" class=\"data row22 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row22_col6\" class=\"data row22 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row22_col7\" class=\"data row22 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row22_col8\" class=\"data row22 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row22_col9\" class=\"data row22 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row22_col10\" class=\"data row22 col10\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row23\" class=\"row_heading level0 row23\" >Doc23</th>\n",
       "                        <td id=\"T_0f1b6_row23_col0\" class=\"data row23 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row23_col1\" class=\"data row23 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row23_col2\" class=\"data row23 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row23_col3\" class=\"data row23 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row23_col4\" class=\"data row23 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row23_col5\" class=\"data row23 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row23_col6\" class=\"data row23 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row23_col7\" class=\"data row23 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row23_col8\" class=\"data row23 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row23_col9\" class=\"data row23 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row23_col10\" class=\"data row23 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row24\" class=\"row_heading level0 row24\" >Doc24</th>\n",
       "                        <td id=\"T_0f1b6_row24_col0\" class=\"data row24 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row24_col1\" class=\"data row24 col1\" >0.520000</td>\n",
       "                        <td id=\"T_0f1b6_row24_col2\" class=\"data row24 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row24_col3\" class=\"data row24 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row24_col4\" class=\"data row24 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row24_col5\" class=\"data row24 col5\" >0.480000</td>\n",
       "                        <td id=\"T_0f1b6_row24_col6\" class=\"data row24 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row24_col7\" class=\"data row24 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row24_col8\" class=\"data row24 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row24_col9\" class=\"data row24 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row24_col10\" class=\"data row24 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row25\" class=\"row_heading level0 row25\" >Doc25</th>\n",
       "                        <td id=\"T_0f1b6_row25_col0\" class=\"data row25 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row25_col1\" class=\"data row25 col1\" >0.030000</td>\n",
       "                        <td id=\"T_0f1b6_row25_col2\" class=\"data row25 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row25_col3\" class=\"data row25 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row25_col4\" class=\"data row25 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row25_col5\" class=\"data row25 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row25_col6\" class=\"data row25 col6\" >0.970000</td>\n",
       "                        <td id=\"T_0f1b6_row25_col7\" class=\"data row25 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row25_col8\" class=\"data row25 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row25_col9\" class=\"data row25 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row25_col10\" class=\"data row25 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row26\" class=\"row_heading level0 row26\" >Doc26</th>\n",
       "                        <td id=\"T_0f1b6_row26_col0\" class=\"data row26 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row26_col1\" class=\"data row26 col1\" >0.970000</td>\n",
       "                        <td id=\"T_0f1b6_row26_col2\" class=\"data row26 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row26_col3\" class=\"data row26 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row26_col4\" class=\"data row26 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row26_col5\" class=\"data row26 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row26_col6\" class=\"data row26 col6\" >0.030000</td>\n",
       "                        <td id=\"T_0f1b6_row26_col7\" class=\"data row26 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row26_col8\" class=\"data row26 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row26_col9\" class=\"data row26 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row26_col10\" class=\"data row26 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row27\" class=\"row_heading level0 row27\" >Doc27</th>\n",
       "                        <td id=\"T_0f1b6_row27_col0\" class=\"data row27 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row27_col1\" class=\"data row27 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row27_col2\" class=\"data row27 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row27_col3\" class=\"data row27 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row27_col4\" class=\"data row27 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row27_col5\" class=\"data row27 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row27_col6\" class=\"data row27 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row27_col7\" class=\"data row27 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row27_col8\" class=\"data row27 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row27_col9\" class=\"data row27 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row27_col10\" class=\"data row27 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row28\" class=\"row_heading level0 row28\" >Doc28</th>\n",
       "                        <td id=\"T_0f1b6_row28_col0\" class=\"data row28 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row28_col1\" class=\"data row28 col1\" >0.080000</td>\n",
       "                        <td id=\"T_0f1b6_row28_col2\" class=\"data row28 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row28_col3\" class=\"data row28 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row28_col4\" class=\"data row28 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row28_col5\" class=\"data row28 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row28_col6\" class=\"data row28 col6\" >0.920000</td>\n",
       "                        <td id=\"T_0f1b6_row28_col7\" class=\"data row28 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row28_col8\" class=\"data row28 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row28_col9\" class=\"data row28 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row28_col10\" class=\"data row28 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row29\" class=\"row_heading level0 row29\" >Doc29</th>\n",
       "                        <td id=\"T_0f1b6_row29_col0\" class=\"data row29 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row29_col1\" class=\"data row29 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row29_col2\" class=\"data row29 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row29_col3\" class=\"data row29 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row29_col4\" class=\"data row29 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row29_col5\" class=\"data row29 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row29_col6\" class=\"data row29 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row29_col7\" class=\"data row29 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row29_col8\" class=\"data row29 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row29_col9\" class=\"data row29 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row29_col10\" class=\"data row29 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row30\" class=\"row_heading level0 row30\" >Doc30</th>\n",
       "                        <td id=\"T_0f1b6_row30_col0\" class=\"data row30 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row30_col1\" class=\"data row30 col1\" >0.680000</td>\n",
       "                        <td id=\"T_0f1b6_row30_col2\" class=\"data row30 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row30_col3\" class=\"data row30 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row30_col4\" class=\"data row30 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row30_col5\" class=\"data row30 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row30_col6\" class=\"data row30 col6\" >0.320000</td>\n",
       "                        <td id=\"T_0f1b6_row30_col7\" class=\"data row30 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row30_col8\" class=\"data row30 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row30_col9\" class=\"data row30 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row30_col10\" class=\"data row30 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row31\" class=\"row_heading level0 row31\" >Doc31</th>\n",
       "                        <td id=\"T_0f1b6_row31_col0\" class=\"data row31 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row31_col1\" class=\"data row31 col1\" >0.530000</td>\n",
       "                        <td id=\"T_0f1b6_row31_col2\" class=\"data row31 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row31_col3\" class=\"data row31 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row31_col4\" class=\"data row31 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row31_col5\" class=\"data row31 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row31_col6\" class=\"data row31 col6\" >0.470000</td>\n",
       "                        <td id=\"T_0f1b6_row31_col7\" class=\"data row31 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row31_col8\" class=\"data row31 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row31_col9\" class=\"data row31 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row31_col10\" class=\"data row31 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row32\" class=\"row_heading level0 row32\" >Doc32</th>\n",
       "                        <td id=\"T_0f1b6_row32_col0\" class=\"data row32 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row32_col1\" class=\"data row32 col1\" >0.510000</td>\n",
       "                        <td id=\"T_0f1b6_row32_col2\" class=\"data row32 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row32_col3\" class=\"data row32 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row32_col4\" class=\"data row32 col4\" >0.310000</td>\n",
       "                        <td id=\"T_0f1b6_row32_col5\" class=\"data row32 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row32_col6\" class=\"data row32 col6\" >0.180000</td>\n",
       "                        <td id=\"T_0f1b6_row32_col7\" class=\"data row32 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row32_col8\" class=\"data row32 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row32_col9\" class=\"data row32 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row32_col10\" class=\"data row32 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row33\" class=\"row_heading level0 row33\" >Doc33</th>\n",
       "                        <td id=\"T_0f1b6_row33_col0\" class=\"data row33 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row33_col1\" class=\"data row33 col1\" >0.730000</td>\n",
       "                        <td id=\"T_0f1b6_row33_col2\" class=\"data row33 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row33_col3\" class=\"data row33 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row33_col4\" class=\"data row33 col4\" >0.110000</td>\n",
       "                        <td id=\"T_0f1b6_row33_col5\" class=\"data row33 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row33_col6\" class=\"data row33 col6\" >0.160000</td>\n",
       "                        <td id=\"T_0f1b6_row33_col7\" class=\"data row33 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row33_col8\" class=\"data row33 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row33_col9\" class=\"data row33 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row33_col10\" class=\"data row33 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row34\" class=\"row_heading level0 row34\" >Doc34</th>\n",
       "                        <td id=\"T_0f1b6_row34_col0\" class=\"data row34 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row34_col1\" class=\"data row34 col1\" >0.790000</td>\n",
       "                        <td id=\"T_0f1b6_row34_col2\" class=\"data row34 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row34_col3\" class=\"data row34 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row34_col4\" class=\"data row34 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row34_col5\" class=\"data row34 col5\" >0.050000</td>\n",
       "                        <td id=\"T_0f1b6_row34_col6\" class=\"data row34 col6\" >0.160000</td>\n",
       "                        <td id=\"T_0f1b6_row34_col7\" class=\"data row34 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row34_col8\" class=\"data row34 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row34_col9\" class=\"data row34 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row34_col10\" class=\"data row34 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row35\" class=\"row_heading level0 row35\" >Doc35</th>\n",
       "                        <td id=\"T_0f1b6_row35_col0\" class=\"data row35 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row35_col1\" class=\"data row35 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row35_col2\" class=\"data row35 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row35_col3\" class=\"data row35 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row35_col4\" class=\"data row35 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row35_col5\" class=\"data row35 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row35_col6\" class=\"data row35 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row35_col7\" class=\"data row35 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row35_col8\" class=\"data row35 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row35_col9\" class=\"data row35 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row35_col10\" class=\"data row35 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row36\" class=\"row_heading level0 row36\" >Doc36</th>\n",
       "                        <td id=\"T_0f1b6_row36_col0\" class=\"data row36 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row36_col1\" class=\"data row36 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row36_col2\" class=\"data row36 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row36_col3\" class=\"data row36 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row36_col4\" class=\"data row36 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row36_col5\" class=\"data row36 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row36_col6\" class=\"data row36 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row36_col7\" class=\"data row36 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row36_col8\" class=\"data row36 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row36_col9\" class=\"data row36 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row36_col10\" class=\"data row36 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row37\" class=\"row_heading level0 row37\" >Doc37</th>\n",
       "                        <td id=\"T_0f1b6_row37_col0\" class=\"data row37 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row37_col1\" class=\"data row37 col1\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row37_col2\" class=\"data row37 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row37_col3\" class=\"data row37 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row37_col4\" class=\"data row37 col4\" >0.510000</td>\n",
       "                        <td id=\"T_0f1b6_row37_col5\" class=\"data row37 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row37_col6\" class=\"data row37 col6\" >0.480000</td>\n",
       "                        <td id=\"T_0f1b6_row37_col7\" class=\"data row37 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row37_col8\" class=\"data row37 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row37_col9\" class=\"data row37 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row37_col10\" class=\"data row37 col10\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row38\" class=\"row_heading level0 row38\" >Doc38</th>\n",
       "                        <td id=\"T_0f1b6_row38_col0\" class=\"data row38 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row38_col1\" class=\"data row38 col1\" >0.530000</td>\n",
       "                        <td id=\"T_0f1b6_row38_col2\" class=\"data row38 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row38_col3\" class=\"data row38 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row38_col4\" class=\"data row38 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row38_col5\" class=\"data row38 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row38_col6\" class=\"data row38 col6\" >0.470000</td>\n",
       "                        <td id=\"T_0f1b6_row38_col7\" class=\"data row38 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row38_col8\" class=\"data row38 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row38_col9\" class=\"data row38 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row38_col10\" class=\"data row38 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row39\" class=\"row_heading level0 row39\" >Doc39</th>\n",
       "                        <td id=\"T_0f1b6_row39_col0\" class=\"data row39 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row39_col1\" class=\"data row39 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row39_col2\" class=\"data row39 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row39_col3\" class=\"data row39 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row39_col4\" class=\"data row39 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row39_col5\" class=\"data row39 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row39_col6\" class=\"data row39 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row39_col7\" class=\"data row39 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row39_col8\" class=\"data row39 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row39_col9\" class=\"data row39 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row39_col10\" class=\"data row39 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row40\" class=\"row_heading level0 row40\" >Doc40</th>\n",
       "                        <td id=\"T_0f1b6_row40_col0\" class=\"data row40 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row40_col1\" class=\"data row40 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row40_col2\" class=\"data row40 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row40_col3\" class=\"data row40 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row40_col4\" class=\"data row40 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row40_col5\" class=\"data row40 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row40_col6\" class=\"data row40 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row40_col7\" class=\"data row40 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row40_col8\" class=\"data row40 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row40_col9\" class=\"data row40 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row40_col10\" class=\"data row40 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row41\" class=\"row_heading level0 row41\" >Doc41</th>\n",
       "                        <td id=\"T_0f1b6_row41_col0\" class=\"data row41 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row41_col1\" class=\"data row41 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row41_col2\" class=\"data row41 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row41_col3\" class=\"data row41 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row41_col4\" class=\"data row41 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row41_col5\" class=\"data row41 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row41_col6\" class=\"data row41 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row41_col7\" class=\"data row41 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row41_col8\" class=\"data row41 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row41_col9\" class=\"data row41 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row41_col10\" class=\"data row41 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row42\" class=\"row_heading level0 row42\" >Doc42</th>\n",
       "                        <td id=\"T_0f1b6_row42_col0\" class=\"data row42 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row42_col1\" class=\"data row42 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row42_col2\" class=\"data row42 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row42_col3\" class=\"data row42 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row42_col4\" class=\"data row42 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row42_col5\" class=\"data row42 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row42_col6\" class=\"data row42 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row42_col7\" class=\"data row42 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row42_col8\" class=\"data row42 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row42_col9\" class=\"data row42 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row42_col10\" class=\"data row42 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row43\" class=\"row_heading level0 row43\" >Doc43</th>\n",
       "                        <td id=\"T_0f1b6_row43_col0\" class=\"data row43 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row43_col1\" class=\"data row43 col1\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row43_col2\" class=\"data row43 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row43_col3\" class=\"data row43 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row43_col4\" class=\"data row43 col4\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row43_col5\" class=\"data row43 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row43_col6\" class=\"data row43 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row43_col7\" class=\"data row43 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row43_col8\" class=\"data row43 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row43_col9\" class=\"data row43 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row43_col10\" class=\"data row43 col10\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row44\" class=\"row_heading level0 row44\" >Doc44</th>\n",
       "                        <td id=\"T_0f1b6_row44_col0\" class=\"data row44 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row44_col1\" class=\"data row44 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row44_col2\" class=\"data row44 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row44_col3\" class=\"data row44 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row44_col4\" class=\"data row44 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row44_col5\" class=\"data row44 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row44_col6\" class=\"data row44 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row44_col7\" class=\"data row44 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row44_col8\" class=\"data row44 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row44_col9\" class=\"data row44 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row44_col10\" class=\"data row44 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row45\" class=\"row_heading level0 row45\" >Doc45</th>\n",
       "                        <td id=\"T_0f1b6_row45_col0\" class=\"data row45 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row45_col1\" class=\"data row45 col1\" >0.950000</td>\n",
       "                        <td id=\"T_0f1b6_row45_col2\" class=\"data row45 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row45_col3\" class=\"data row45 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row45_col4\" class=\"data row45 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row45_col5\" class=\"data row45 col5\" >0.020000</td>\n",
       "                        <td id=\"T_0f1b6_row45_col6\" class=\"data row45 col6\" >0.030000</td>\n",
       "                        <td id=\"T_0f1b6_row45_col7\" class=\"data row45 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row45_col8\" class=\"data row45 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row45_col9\" class=\"data row45 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row45_col10\" class=\"data row45 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row46\" class=\"row_heading level0 row46\" >Doc46</th>\n",
       "                        <td id=\"T_0f1b6_row46_col0\" class=\"data row46 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row46_col1\" class=\"data row46 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row46_col2\" class=\"data row46 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row46_col3\" class=\"data row46 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row46_col4\" class=\"data row46 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row46_col5\" class=\"data row46 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row46_col6\" class=\"data row46 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row46_col7\" class=\"data row46 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row46_col8\" class=\"data row46 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row46_col9\" class=\"data row46 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row46_col10\" class=\"data row46 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row47\" class=\"row_heading level0 row47\" >Doc47</th>\n",
       "                        <td id=\"T_0f1b6_row47_col0\" class=\"data row47 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row47_col1\" class=\"data row47 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row47_col2\" class=\"data row47 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row47_col3\" class=\"data row47 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row47_col4\" class=\"data row47 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row47_col5\" class=\"data row47 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row47_col6\" class=\"data row47 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row47_col7\" class=\"data row47 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row47_col8\" class=\"data row47 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row47_col9\" class=\"data row47 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row47_col10\" class=\"data row47 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row48\" class=\"row_heading level0 row48\" >Doc48</th>\n",
       "                        <td id=\"T_0f1b6_row48_col0\" class=\"data row48 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row48_col1\" class=\"data row48 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row48_col2\" class=\"data row48 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row48_col3\" class=\"data row48 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row48_col4\" class=\"data row48 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row48_col5\" class=\"data row48 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row48_col6\" class=\"data row48 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row48_col7\" class=\"data row48 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row48_col8\" class=\"data row48 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row48_col9\" class=\"data row48 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row48_col10\" class=\"data row48 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row49\" class=\"row_heading level0 row49\" >Doc49</th>\n",
       "                        <td id=\"T_0f1b6_row49_col0\" class=\"data row49 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row49_col1\" class=\"data row49 col1\" >0.400000</td>\n",
       "                        <td id=\"T_0f1b6_row49_col2\" class=\"data row49 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row49_col3\" class=\"data row49 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row49_col4\" class=\"data row49 col4\" >0.600000</td>\n",
       "                        <td id=\"T_0f1b6_row49_col5\" class=\"data row49 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row49_col6\" class=\"data row49 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row49_col7\" class=\"data row49 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row49_col8\" class=\"data row49 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row49_col9\" class=\"data row49 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row49_col10\" class=\"data row49 col10\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row50\" class=\"row_heading level0 row50\" >Doc50</th>\n",
       "                        <td id=\"T_0f1b6_row50_col0\" class=\"data row50 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row50_col1\" class=\"data row50 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row50_col2\" class=\"data row50 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row50_col3\" class=\"data row50 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row50_col4\" class=\"data row50 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row50_col5\" class=\"data row50 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row50_col6\" class=\"data row50 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row50_col7\" class=\"data row50 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row50_col8\" class=\"data row50 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row50_col9\" class=\"data row50 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row50_col10\" class=\"data row50 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row51\" class=\"row_heading level0 row51\" >Doc51</th>\n",
       "                        <td id=\"T_0f1b6_row51_col0\" class=\"data row51 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row51_col1\" class=\"data row51 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row51_col2\" class=\"data row51 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row51_col3\" class=\"data row51 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row51_col4\" class=\"data row51 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row51_col5\" class=\"data row51 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row51_col6\" class=\"data row51 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row51_col7\" class=\"data row51 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row51_col8\" class=\"data row51 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row51_col9\" class=\"data row51 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row51_col10\" class=\"data row51 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row52\" class=\"row_heading level0 row52\" >Doc52</th>\n",
       "                        <td id=\"T_0f1b6_row52_col0\" class=\"data row52 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row52_col1\" class=\"data row52 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row52_col2\" class=\"data row52 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row52_col3\" class=\"data row52 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row52_col4\" class=\"data row52 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row52_col5\" class=\"data row52 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row52_col6\" class=\"data row52 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row52_col7\" class=\"data row52 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row52_col8\" class=\"data row52 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row52_col9\" class=\"data row52 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row52_col10\" class=\"data row52 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row53\" class=\"row_heading level0 row53\" >Doc53</th>\n",
       "                        <td id=\"T_0f1b6_row53_col0\" class=\"data row53 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row53_col1\" class=\"data row53 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row53_col2\" class=\"data row53 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row53_col3\" class=\"data row53 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row53_col4\" class=\"data row53 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row53_col5\" class=\"data row53 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row53_col6\" class=\"data row53 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row53_col7\" class=\"data row53 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row53_col8\" class=\"data row53 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row53_col9\" class=\"data row53 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row53_col10\" class=\"data row53 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row54\" class=\"row_heading level0 row54\" >Doc54</th>\n",
       "                        <td id=\"T_0f1b6_row54_col0\" class=\"data row54 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row54_col1\" class=\"data row54 col1\" >0.810000</td>\n",
       "                        <td id=\"T_0f1b6_row54_col2\" class=\"data row54 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row54_col3\" class=\"data row54 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row54_col4\" class=\"data row54 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row54_col5\" class=\"data row54 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row54_col6\" class=\"data row54 col6\" >0.190000</td>\n",
       "                        <td id=\"T_0f1b6_row54_col7\" class=\"data row54 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row54_col8\" class=\"data row54 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row54_col9\" class=\"data row54 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row54_col10\" class=\"data row54 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row55\" class=\"row_heading level0 row55\" >Doc55</th>\n",
       "                        <td id=\"T_0f1b6_row55_col0\" class=\"data row55 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row55_col1\" class=\"data row55 col1\" >0.870000</td>\n",
       "                        <td id=\"T_0f1b6_row55_col2\" class=\"data row55 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row55_col3\" class=\"data row55 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row55_col4\" class=\"data row55 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row55_col5\" class=\"data row55 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row55_col6\" class=\"data row55 col6\" >0.130000</td>\n",
       "                        <td id=\"T_0f1b6_row55_col7\" class=\"data row55 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row55_col8\" class=\"data row55 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row55_col9\" class=\"data row55 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row55_col10\" class=\"data row55 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row56\" class=\"row_heading level0 row56\" >Doc56</th>\n",
       "                        <td id=\"T_0f1b6_row56_col0\" class=\"data row56 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row56_col1\" class=\"data row56 col1\" >0.990000</td>\n",
       "                        <td id=\"T_0f1b6_row56_col2\" class=\"data row56 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row56_col3\" class=\"data row56 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row56_col4\" class=\"data row56 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row56_col5\" class=\"data row56 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row56_col6\" class=\"data row56 col6\" >0.010000</td>\n",
       "                        <td id=\"T_0f1b6_row56_col7\" class=\"data row56 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row56_col8\" class=\"data row56 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row56_col9\" class=\"data row56 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row56_col10\" class=\"data row56 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row57\" class=\"row_heading level0 row57\" >Doc57</th>\n",
       "                        <td id=\"T_0f1b6_row57_col0\" class=\"data row57 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row57_col1\" class=\"data row57 col1\" >0.530000</td>\n",
       "                        <td id=\"T_0f1b6_row57_col2\" class=\"data row57 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row57_col3\" class=\"data row57 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row57_col4\" class=\"data row57 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row57_col5\" class=\"data row57 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row57_col6\" class=\"data row57 col6\" >0.470000</td>\n",
       "                        <td id=\"T_0f1b6_row57_col7\" class=\"data row57 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row57_col8\" class=\"data row57 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row57_col9\" class=\"data row57 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row57_col10\" class=\"data row57 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row58\" class=\"row_heading level0 row58\" >Doc58</th>\n",
       "                        <td id=\"T_0f1b6_row58_col0\" class=\"data row58 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row58_col1\" class=\"data row58 col1\" >0.990000</td>\n",
       "                        <td id=\"T_0f1b6_row58_col2\" class=\"data row58 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row58_col3\" class=\"data row58 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row58_col4\" class=\"data row58 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row58_col5\" class=\"data row58 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row58_col6\" class=\"data row58 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row58_col7\" class=\"data row58 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row58_col8\" class=\"data row58 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row58_col9\" class=\"data row58 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row58_col10\" class=\"data row58 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row59\" class=\"row_heading level0 row59\" >Doc59</th>\n",
       "                        <td id=\"T_0f1b6_row59_col0\" class=\"data row59 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row59_col1\" class=\"data row59 col1\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row59_col2\" class=\"data row59 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row59_col3\" class=\"data row59 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row59_col4\" class=\"data row59 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row59_col5\" class=\"data row59 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row59_col6\" class=\"data row59 col6\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row59_col7\" class=\"data row59 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row59_col8\" class=\"data row59 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row59_col9\" class=\"data row59 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row59_col10\" class=\"data row59 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row60\" class=\"row_heading level0 row60\" >Doc60</th>\n",
       "                        <td id=\"T_0f1b6_row60_col0\" class=\"data row60 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row60_col1\" class=\"data row60 col1\" >0.580000</td>\n",
       "                        <td id=\"T_0f1b6_row60_col2\" class=\"data row60 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row60_col3\" class=\"data row60 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row60_col4\" class=\"data row60 col4\" >0.180000</td>\n",
       "                        <td id=\"T_0f1b6_row60_col5\" class=\"data row60 col5\" >0.010000</td>\n",
       "                        <td id=\"T_0f1b6_row60_col6\" class=\"data row60 col6\" >0.230000</td>\n",
       "                        <td id=\"T_0f1b6_row60_col7\" class=\"data row60 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row60_col8\" class=\"data row60 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row60_col9\" class=\"data row60 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row60_col10\" class=\"data row60 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row61\" class=\"row_heading level0 row61\" >Doc61</th>\n",
       "                        <td id=\"T_0f1b6_row61_col0\" class=\"data row61 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row61_col1\" class=\"data row61 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row61_col2\" class=\"data row61 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row61_col3\" class=\"data row61 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row61_col4\" class=\"data row61 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row61_col5\" class=\"data row61 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row61_col6\" class=\"data row61 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row61_col7\" class=\"data row61 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row61_col8\" class=\"data row61 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row61_col9\" class=\"data row61 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row61_col10\" class=\"data row61 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row62\" class=\"row_heading level0 row62\" >Doc62</th>\n",
       "                        <td id=\"T_0f1b6_row62_col0\" class=\"data row62 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row62_col1\" class=\"data row62 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row62_col2\" class=\"data row62 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row62_col3\" class=\"data row62 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row62_col4\" class=\"data row62 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row62_col5\" class=\"data row62 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row62_col6\" class=\"data row62 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row62_col7\" class=\"data row62 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row62_col8\" class=\"data row62 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row62_col9\" class=\"data row62 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row62_col10\" class=\"data row62 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row63\" class=\"row_heading level0 row63\" >Doc63</th>\n",
       "                        <td id=\"T_0f1b6_row63_col0\" class=\"data row63 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row63_col1\" class=\"data row63 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row63_col2\" class=\"data row63 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row63_col3\" class=\"data row63 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row63_col4\" class=\"data row63 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row63_col5\" class=\"data row63 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row63_col6\" class=\"data row63 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row63_col7\" class=\"data row63 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row63_col8\" class=\"data row63 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row63_col9\" class=\"data row63 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row63_col10\" class=\"data row63 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row64\" class=\"row_heading level0 row64\" >Doc64</th>\n",
       "                        <td id=\"T_0f1b6_row64_col0\" class=\"data row64 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row64_col1\" class=\"data row64 col1\" >0.380000</td>\n",
       "                        <td id=\"T_0f1b6_row64_col2\" class=\"data row64 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row64_col3\" class=\"data row64 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row64_col4\" class=\"data row64 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row64_col5\" class=\"data row64 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row64_col6\" class=\"data row64 col6\" >0.620000</td>\n",
       "                        <td id=\"T_0f1b6_row64_col7\" class=\"data row64 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row64_col8\" class=\"data row64 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row64_col9\" class=\"data row64 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row64_col10\" class=\"data row64 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row65\" class=\"row_heading level0 row65\" >Doc65</th>\n",
       "                        <td id=\"T_0f1b6_row65_col0\" class=\"data row65 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row65_col1\" class=\"data row65 col1\" >0.380000</td>\n",
       "                        <td id=\"T_0f1b6_row65_col2\" class=\"data row65 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row65_col3\" class=\"data row65 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row65_col4\" class=\"data row65 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row65_col5\" class=\"data row65 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row65_col6\" class=\"data row65 col6\" >0.620000</td>\n",
       "                        <td id=\"T_0f1b6_row65_col7\" class=\"data row65 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row65_col8\" class=\"data row65 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row65_col9\" class=\"data row65 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row65_col10\" class=\"data row65 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row66\" class=\"row_heading level0 row66\" >Doc66</th>\n",
       "                        <td id=\"T_0f1b6_row66_col0\" class=\"data row66 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row66_col1\" class=\"data row66 col1\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row66_col2\" class=\"data row66 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row66_col3\" class=\"data row66 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row66_col4\" class=\"data row66 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row66_col5\" class=\"data row66 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row66_col6\" class=\"data row66 col6\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row66_col7\" class=\"data row66 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row66_col8\" class=\"data row66 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row66_col9\" class=\"data row66 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row66_col10\" class=\"data row66 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row67\" class=\"row_heading level0 row67\" >Doc67</th>\n",
       "                        <td id=\"T_0f1b6_row67_col0\" class=\"data row67 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row67_col1\" class=\"data row67 col1\" >0.950000</td>\n",
       "                        <td id=\"T_0f1b6_row67_col2\" class=\"data row67 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row67_col3\" class=\"data row67 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row67_col4\" class=\"data row67 col4\" >0.050000</td>\n",
       "                        <td id=\"T_0f1b6_row67_col5\" class=\"data row67 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row67_col6\" class=\"data row67 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row67_col7\" class=\"data row67 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row67_col8\" class=\"data row67 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row67_col9\" class=\"data row67 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row67_col10\" class=\"data row67 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row68\" class=\"row_heading level0 row68\" >Doc68</th>\n",
       "                        <td id=\"T_0f1b6_row68_col0\" class=\"data row68 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row68_col1\" class=\"data row68 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row68_col2\" class=\"data row68 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row68_col3\" class=\"data row68 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row68_col4\" class=\"data row68 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row68_col5\" class=\"data row68 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row68_col6\" class=\"data row68 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row68_col7\" class=\"data row68 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row68_col8\" class=\"data row68 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row68_col9\" class=\"data row68 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row68_col10\" class=\"data row68 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row69\" class=\"row_heading level0 row69\" >Doc69</th>\n",
       "                        <td id=\"T_0f1b6_row69_col0\" class=\"data row69 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row69_col1\" class=\"data row69 col1\" >0.020000</td>\n",
       "                        <td id=\"T_0f1b6_row69_col2\" class=\"data row69 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row69_col3\" class=\"data row69 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row69_col4\" class=\"data row69 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row69_col5\" class=\"data row69 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row69_col6\" class=\"data row69 col6\" >0.980000</td>\n",
       "                        <td id=\"T_0f1b6_row69_col7\" class=\"data row69 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row69_col8\" class=\"data row69 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row69_col9\" class=\"data row69 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row69_col10\" class=\"data row69 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row70\" class=\"row_heading level0 row70\" >Doc70</th>\n",
       "                        <td id=\"T_0f1b6_row70_col0\" class=\"data row70 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row70_col1\" class=\"data row70 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row70_col2\" class=\"data row70 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row70_col3\" class=\"data row70 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row70_col4\" class=\"data row70 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row70_col5\" class=\"data row70 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row70_col6\" class=\"data row70 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row70_col7\" class=\"data row70 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row70_col8\" class=\"data row70 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row70_col9\" class=\"data row70 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row70_col10\" class=\"data row70 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row71\" class=\"row_heading level0 row71\" >Doc71</th>\n",
       "                        <td id=\"T_0f1b6_row71_col0\" class=\"data row71 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row71_col1\" class=\"data row71 col1\" >0.990000</td>\n",
       "                        <td id=\"T_0f1b6_row71_col2\" class=\"data row71 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row71_col3\" class=\"data row71 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row71_col4\" class=\"data row71 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row71_col5\" class=\"data row71 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row71_col6\" class=\"data row71 col6\" >0.010000</td>\n",
       "                        <td id=\"T_0f1b6_row71_col7\" class=\"data row71 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row71_col8\" class=\"data row71 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row71_col9\" class=\"data row71 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row71_col10\" class=\"data row71 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row72\" class=\"row_heading level0 row72\" >Doc72</th>\n",
       "                        <td id=\"T_0f1b6_row72_col0\" class=\"data row72 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row72_col1\" class=\"data row72 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row72_col2\" class=\"data row72 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row72_col3\" class=\"data row72 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row72_col4\" class=\"data row72 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row72_col5\" class=\"data row72 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row72_col6\" class=\"data row72 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row72_col7\" class=\"data row72 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row72_col8\" class=\"data row72 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row72_col9\" class=\"data row72 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row72_col10\" class=\"data row72 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row73\" class=\"row_heading level0 row73\" >Doc73</th>\n",
       "                        <td id=\"T_0f1b6_row73_col0\" class=\"data row73 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row73_col1\" class=\"data row73 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row73_col2\" class=\"data row73 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row73_col3\" class=\"data row73 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row73_col4\" class=\"data row73 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row73_col5\" class=\"data row73 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row73_col6\" class=\"data row73 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row73_col7\" class=\"data row73 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row73_col8\" class=\"data row73 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row73_col9\" class=\"data row73 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row73_col10\" class=\"data row73 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row74\" class=\"row_heading level0 row74\" >Doc74</th>\n",
       "                        <td id=\"T_0f1b6_row74_col0\" class=\"data row74 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row74_col1\" class=\"data row74 col1\" >0.500000</td>\n",
       "                        <td id=\"T_0f1b6_row74_col2\" class=\"data row74 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row74_col3\" class=\"data row74 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row74_col4\" class=\"data row74 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row74_col5\" class=\"data row74 col5\" >0.500000</td>\n",
       "                        <td id=\"T_0f1b6_row74_col6\" class=\"data row74 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row74_col7\" class=\"data row74 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row74_col8\" class=\"data row74 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row74_col9\" class=\"data row74 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row74_col10\" class=\"data row74 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row75\" class=\"row_heading level0 row75\" >Doc75</th>\n",
       "                        <td id=\"T_0f1b6_row75_col0\" class=\"data row75 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row75_col1\" class=\"data row75 col1\" >0.560000</td>\n",
       "                        <td id=\"T_0f1b6_row75_col2\" class=\"data row75 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row75_col3\" class=\"data row75 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row75_col4\" class=\"data row75 col4\" >0.440000</td>\n",
       "                        <td id=\"T_0f1b6_row75_col5\" class=\"data row75 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row75_col6\" class=\"data row75 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row75_col7\" class=\"data row75 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row75_col8\" class=\"data row75 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row75_col9\" class=\"data row75 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row75_col10\" class=\"data row75 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row76\" class=\"row_heading level0 row76\" >Doc76</th>\n",
       "                        <td id=\"T_0f1b6_row76_col0\" class=\"data row76 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row76_col1\" class=\"data row76 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row76_col2\" class=\"data row76 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row76_col3\" class=\"data row76 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row76_col4\" class=\"data row76 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row76_col5\" class=\"data row76 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row76_col6\" class=\"data row76 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row76_col7\" class=\"data row76 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row76_col8\" class=\"data row76 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row76_col9\" class=\"data row76 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row76_col10\" class=\"data row76 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row77\" class=\"row_heading level0 row77\" >Doc77</th>\n",
       "                        <td id=\"T_0f1b6_row77_col0\" class=\"data row77 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row77_col1\" class=\"data row77 col1\" >0.830000</td>\n",
       "                        <td id=\"T_0f1b6_row77_col2\" class=\"data row77 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row77_col3\" class=\"data row77 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row77_col4\" class=\"data row77 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row77_col5\" class=\"data row77 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row77_col6\" class=\"data row77 col6\" >0.170000</td>\n",
       "                        <td id=\"T_0f1b6_row77_col7\" class=\"data row77 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row77_col8\" class=\"data row77 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row77_col9\" class=\"data row77 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row77_col10\" class=\"data row77 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row78\" class=\"row_heading level0 row78\" >Doc78</th>\n",
       "                        <td id=\"T_0f1b6_row78_col0\" class=\"data row78 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row78_col1\" class=\"data row78 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row78_col2\" class=\"data row78 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row78_col3\" class=\"data row78 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row78_col4\" class=\"data row78 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row78_col5\" class=\"data row78 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row78_col6\" class=\"data row78 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row78_col7\" class=\"data row78 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row78_col8\" class=\"data row78 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row78_col9\" class=\"data row78 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row78_col10\" class=\"data row78 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row79\" class=\"row_heading level0 row79\" >Doc79</th>\n",
       "                        <td id=\"T_0f1b6_row79_col0\" class=\"data row79 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row79_col1\" class=\"data row79 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row79_col2\" class=\"data row79 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row79_col3\" class=\"data row79 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row79_col4\" class=\"data row79 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row79_col5\" class=\"data row79 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row79_col6\" class=\"data row79 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row79_col7\" class=\"data row79 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row79_col8\" class=\"data row79 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row79_col9\" class=\"data row79 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row79_col10\" class=\"data row79 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row80\" class=\"row_heading level0 row80\" >Doc80</th>\n",
       "                        <td id=\"T_0f1b6_row80_col0\" class=\"data row80 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row80_col1\" class=\"data row80 col1\" >0.740000</td>\n",
       "                        <td id=\"T_0f1b6_row80_col2\" class=\"data row80 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row80_col3\" class=\"data row80 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row80_col4\" class=\"data row80 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row80_col5\" class=\"data row80 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row80_col6\" class=\"data row80 col6\" >0.260000</td>\n",
       "                        <td id=\"T_0f1b6_row80_col7\" class=\"data row80 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row80_col8\" class=\"data row80 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row80_col9\" class=\"data row80 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row80_col10\" class=\"data row80 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row81\" class=\"row_heading level0 row81\" >Doc81</th>\n",
       "                        <td id=\"T_0f1b6_row81_col0\" class=\"data row81 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row81_col1\" class=\"data row81 col1\" >0.460000</td>\n",
       "                        <td id=\"T_0f1b6_row81_col2\" class=\"data row81 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row81_col3\" class=\"data row81 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row81_col4\" class=\"data row81 col4\" >0.040000</td>\n",
       "                        <td id=\"T_0f1b6_row81_col5\" class=\"data row81 col5\" >0.060000</td>\n",
       "                        <td id=\"T_0f1b6_row81_col6\" class=\"data row81 col6\" >0.440000</td>\n",
       "                        <td id=\"T_0f1b6_row81_col7\" class=\"data row81 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row81_col8\" class=\"data row81 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row81_col9\" class=\"data row81 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row81_col10\" class=\"data row81 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row82\" class=\"row_heading level0 row82\" >Doc82</th>\n",
       "                        <td id=\"T_0f1b6_row82_col0\" class=\"data row82 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row82_col1\" class=\"data row82 col1\" >0.980000</td>\n",
       "                        <td id=\"T_0f1b6_row82_col2\" class=\"data row82 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row82_col3\" class=\"data row82 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row82_col4\" class=\"data row82 col4\" >0.020000</td>\n",
       "                        <td id=\"T_0f1b6_row82_col5\" class=\"data row82 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row82_col6\" class=\"data row82 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row82_col7\" class=\"data row82 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row82_col8\" class=\"data row82 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row82_col9\" class=\"data row82 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row82_col10\" class=\"data row82 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row83\" class=\"row_heading level0 row83\" >Doc83</th>\n",
       "                        <td id=\"T_0f1b6_row83_col0\" class=\"data row83 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row83_col1\" class=\"data row83 col1\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row83_col2\" class=\"data row83 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row83_col3\" class=\"data row83 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row83_col4\" class=\"data row83 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row83_col5\" class=\"data row83 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row83_col6\" class=\"data row83 col6\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row83_col7\" class=\"data row83 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row83_col8\" class=\"data row83 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row83_col9\" class=\"data row83 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row83_col10\" class=\"data row83 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row84\" class=\"row_heading level0 row84\" >Doc84</th>\n",
       "                        <td id=\"T_0f1b6_row84_col0\" class=\"data row84 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row84_col1\" class=\"data row84 col1\" >0.190000</td>\n",
       "                        <td id=\"T_0f1b6_row84_col2\" class=\"data row84 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row84_col3\" class=\"data row84 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row84_col4\" class=\"data row84 col4\" >0.100000</td>\n",
       "                        <td id=\"T_0f1b6_row84_col5\" class=\"data row84 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row84_col6\" class=\"data row84 col6\" >0.710000</td>\n",
       "                        <td id=\"T_0f1b6_row84_col7\" class=\"data row84 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row84_col8\" class=\"data row84 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row84_col9\" class=\"data row84 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row84_col10\" class=\"data row84 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row85\" class=\"row_heading level0 row85\" >Doc85</th>\n",
       "                        <td id=\"T_0f1b6_row85_col0\" class=\"data row85 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row85_col1\" class=\"data row85 col1\" >0.110000</td>\n",
       "                        <td id=\"T_0f1b6_row85_col2\" class=\"data row85 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row85_col3\" class=\"data row85 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row85_col4\" class=\"data row85 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row85_col5\" class=\"data row85 col5\" >0.170000</td>\n",
       "                        <td id=\"T_0f1b6_row85_col6\" class=\"data row85 col6\" >0.720000</td>\n",
       "                        <td id=\"T_0f1b6_row85_col7\" class=\"data row85 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row85_col8\" class=\"data row85 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row85_col9\" class=\"data row85 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row85_col10\" class=\"data row85 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row86\" class=\"row_heading level0 row86\" >Doc86</th>\n",
       "                        <td id=\"T_0f1b6_row86_col0\" class=\"data row86 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row86_col1\" class=\"data row86 col1\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row86_col2\" class=\"data row86 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row86_col3\" class=\"data row86 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row86_col4\" class=\"data row86 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row86_col5\" class=\"data row86 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row86_col6\" class=\"data row86 col6\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row86_col7\" class=\"data row86 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row86_col8\" class=\"data row86 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row86_col9\" class=\"data row86 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row86_col10\" class=\"data row86 col10\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f1b6_level0_row87\" class=\"row_heading level0 row87\" >Doc87</th>\n",
       "                        <td id=\"T_0f1b6_row87_col0\" class=\"data row87 col0\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row87_col1\" class=\"data row87 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f1b6_row87_col2\" class=\"data row87 col2\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row87_col3\" class=\"data row87 col3\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row87_col4\" class=\"data row87 col4\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row87_col5\" class=\"data row87 col5\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row87_col6\" class=\"data row87 col6\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row87_col7\" class=\"data row87 col7\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row87_col8\" class=\"data row87 col8\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row87_col9\" class=\"data row87 col9\" >0.000000</td>\n",
       "                        <td id=\"T_0f1b6_row87_col10\" class=\"data row87 col10\" >1</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9d2fa36370>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Document — Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "# index names\n",
    "\n",
    "print(len(data))\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(88).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>ability</th>\n",
       "      <th>absence</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstraction</th>\n",
       "      <th>abuse</th>\n",
       "      <th>academia</th>\n",
       "      <th>academic</th>\n",
       "      <th>academy</th>\n",
       "      <th>accelerate</th>\n",
       "      <th>...</th>\n",
       "      <th>yoon</th>\n",
       "      <th>york</th>\n",
       "      <th>ysis</th>\n",
       "      <th>yuan</th>\n",
       "      <th>zeng</th>\n",
       "      <th>zhai</th>\n",
       "      <th>zhang</th>\n",
       "      <th>zhao</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zhou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic0</th>\n",
       "      <td>1.054996</td>\n",
       "      <td>1.066450</td>\n",
       "      <td>1.067664</td>\n",
       "      <td>0.869770</td>\n",
       "      <td>0.878954</td>\n",
       "      <td>0.952991</td>\n",
       "      <td>0.949451</td>\n",
       "      <td>0.897556</td>\n",
       "      <td>0.927982</td>\n",
       "      <td>0.920965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872500</td>\n",
       "      <td>0.882906</td>\n",
       "      <td>1.139216</td>\n",
       "      <td>1.058935</td>\n",
       "      <td>0.977944</td>\n",
       "      <td>0.852891</td>\n",
       "      <td>0.836502</td>\n",
       "      <td>0.981387</td>\n",
       "      <td>0.948720</td>\n",
       "      <td>0.913859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>3.609603</td>\n",
       "      <td>13.522816</td>\n",
       "      <td>3.405321</td>\n",
       "      <td>21.273157</td>\n",
       "      <td>2.237529</td>\n",
       "      <td>2.211563</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>2.881636</td>\n",
       "      <td>4.621709</td>\n",
       "      <td>3.160580</td>\n",
       "      <td>...</td>\n",
       "      <td>2.387094</td>\n",
       "      <td>14.122318</td>\n",
       "      <td>2.429478</td>\n",
       "      <td>5.417883</td>\n",
       "      <td>4.534261</td>\n",
       "      <td>3.496922</td>\n",
       "      <td>5.390712</td>\n",
       "      <td>15.615870</td>\n",
       "      <td>3.261350</td>\n",
       "      <td>13.259086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>0.838018</td>\n",
       "      <td>1.084548</td>\n",
       "      <td>0.814479</td>\n",
       "      <td>1.084807</td>\n",
       "      <td>0.912964</td>\n",
       "      <td>1.063327</td>\n",
       "      <td>0.925191</td>\n",
       "      <td>0.789590</td>\n",
       "      <td>0.868767</td>\n",
       "      <td>0.811172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737433</td>\n",
       "      <td>1.177387</td>\n",
       "      <td>0.831745</td>\n",
       "      <td>0.830315</td>\n",
       "      <td>0.971620</td>\n",
       "      <td>1.055237</td>\n",
       "      <td>0.907475</td>\n",
       "      <td>0.926176</td>\n",
       "      <td>1.008409</td>\n",
       "      <td>1.266659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>1.049135</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.738870</td>\n",
       "      <td>1.122561</td>\n",
       "      <td>0.783811</td>\n",
       "      <td>1.007381</td>\n",
       "      <td>0.873292</td>\n",
       "      <td>0.958532</td>\n",
       "      <td>0.974741</td>\n",
       "      <td>1.042597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.948779</td>\n",
       "      <td>0.839654</td>\n",
       "      <td>0.840653</td>\n",
       "      <td>1.102615</td>\n",
       "      <td>0.969379</td>\n",
       "      <td>0.846232</td>\n",
       "      <td>0.886997</td>\n",
       "      <td>1.072033</td>\n",
       "      <td>0.823835</td>\n",
       "      <td>1.044894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>1.547054</td>\n",
       "      <td>2.066137</td>\n",
       "      <td>1.279330</td>\n",
       "      <td>5.330401</td>\n",
       "      <td>1.328044</td>\n",
       "      <td>1.582540</td>\n",
       "      <td>0.838270</td>\n",
       "      <td>0.864341</td>\n",
       "      <td>1.150777</td>\n",
       "      <td>1.128017</td>\n",
       "      <td>...</td>\n",
       "      <td>1.015327</td>\n",
       "      <td>1.574977</td>\n",
       "      <td>0.962506</td>\n",
       "      <td>0.874919</td>\n",
       "      <td>1.036042</td>\n",
       "      <td>1.166660</td>\n",
       "      <td>0.887557</td>\n",
       "      <td>1.622921</td>\n",
       "      <td>0.969050</td>\n",
       "      <td>1.493889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>1.430965</td>\n",
       "      <td>2.012039</td>\n",
       "      <td>1.181689</td>\n",
       "      <td>4.412731</td>\n",
       "      <td>1.030906</td>\n",
       "      <td>1.247141</td>\n",
       "      <td>1.046099</td>\n",
       "      <td>1.046575</td>\n",
       "      <td>1.141384</td>\n",
       "      <td>0.877054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976949</td>\n",
       "      <td>1.744591</td>\n",
       "      <td>0.690071</td>\n",
       "      <td>1.437621</td>\n",
       "      <td>0.901836</td>\n",
       "      <td>1.078547</td>\n",
       "      <td>1.240224</td>\n",
       "      <td>1.583021</td>\n",
       "      <td>0.860020</td>\n",
       "      <td>2.013042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>1.808494</td>\n",
       "      <td>4.992445</td>\n",
       "      <td>2.079185</td>\n",
       "      <td>14.013678</td>\n",
       "      <td>1.330085</td>\n",
       "      <td>0.890894</td>\n",
       "      <td>0.968127</td>\n",
       "      <td>0.878374</td>\n",
       "      <td>1.397568</td>\n",
       "      <td>1.883870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871131</td>\n",
       "      <td>3.339024</td>\n",
       "      <td>1.044310</td>\n",
       "      <td>1.041608</td>\n",
       "      <td>1.577248</td>\n",
       "      <td>2.669331</td>\n",
       "      <td>1.820477</td>\n",
       "      <td>3.646023</td>\n",
       "      <td>1.064525</td>\n",
       "      <td>4.980415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>0.808918</td>\n",
       "      <td>0.847949</td>\n",
       "      <td>0.885615</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>1.021855</td>\n",
       "      <td>0.980299</td>\n",
       "      <td>0.834588</td>\n",
       "      <td>0.888630</td>\n",
       "      <td>0.923083</td>\n",
       "      <td>0.821998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863983</td>\n",
       "      <td>0.860781</td>\n",
       "      <td>0.826731</td>\n",
       "      <td>0.830842</td>\n",
       "      <td>1.021100</td>\n",
       "      <td>0.888345</td>\n",
       "      <td>0.891197</td>\n",
       "      <td>1.007810</td>\n",
       "      <td>0.892581</td>\n",
       "      <td>1.008698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>1.000646</td>\n",
       "      <td>1.617499</td>\n",
       "      <td>1.097025</td>\n",
       "      <td>1.688844</td>\n",
       "      <td>1.014185</td>\n",
       "      <td>0.920724</td>\n",
       "      <td>0.849665</td>\n",
       "      <td>0.884063</td>\n",
       "      <td>1.129861</td>\n",
       "      <td>0.905808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821690</td>\n",
       "      <td>1.597825</td>\n",
       "      <td>1.024878</td>\n",
       "      <td>1.139745</td>\n",
       "      <td>0.988052</td>\n",
       "      <td>1.119047</td>\n",
       "      <td>1.097812</td>\n",
       "      <td>1.126821</td>\n",
       "      <td>0.974398</td>\n",
       "      <td>1.226490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>0.907713</td>\n",
       "      <td>1.138250</td>\n",
       "      <td>0.941675</td>\n",
       "      <td>1.827146</td>\n",
       "      <td>0.858276</td>\n",
       "      <td>0.904482</td>\n",
       "      <td>0.844608</td>\n",
       "      <td>0.869307</td>\n",
       "      <td>0.800188</td>\n",
       "      <td>0.953022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815541</td>\n",
       "      <td>1.400704</td>\n",
       "      <td>0.940657</td>\n",
       "      <td>1.038711</td>\n",
       "      <td>1.145261</td>\n",
       "      <td>1.148574</td>\n",
       "      <td>1.073894</td>\n",
       "      <td>1.001945</td>\n",
       "      <td>0.968805</td>\n",
       "      <td>1.213524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2060 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        abbreviation    ability   absence   abstract  abstraction     abuse  \\\n",
       "Topic0      1.054996   1.066450  1.067664   0.869770     0.878954  0.952991   \n",
       "Topic1      3.609603  13.522816  3.405321  21.273157     2.237529  2.211563   \n",
       "Topic2      0.838018   1.084548  0.814479   1.084807     0.912964  1.063327   \n",
       "Topic3      1.049135   0.867500  0.738870   1.122561     0.783811  1.007381   \n",
       "Topic4      1.547054   2.066137  1.279330   5.330401     1.328044  1.582540   \n",
       "Topic5      1.430965   2.012039  1.181689   4.412731     1.030906  1.247141   \n",
       "Topic6      1.808494   4.992445  2.079185  14.013678     1.330085  0.890894   \n",
       "Topic7      0.808918   0.847949  0.885615   0.908377     1.021855  0.980299   \n",
       "Topic8      1.000646   1.617499  1.097025   1.688844     1.014185  0.920724   \n",
       "Topic9      0.907713   1.138250  0.941675   1.827146     0.858276  0.904482   \n",
       "\n",
       "        academia  academic   academy  accelerate  ...      yoon       york  \\\n",
       "Topic0  0.949451  0.897556  0.927982    0.920965  ...  0.872500   0.882906   \n",
       "Topic1  2.214286  2.881636  4.621709    3.160580  ...  2.387094  14.122318   \n",
       "Topic2  0.925191  0.789590  0.868767    0.811172  ...  0.737433   1.177387   \n",
       "Topic3  0.873292  0.958532  0.974741    1.042597  ...  0.948779   0.839654   \n",
       "Topic4  0.838270  0.864341  1.150777    1.128017  ...  1.015327   1.574977   \n",
       "Topic5  1.046099  1.046575  1.141384    0.877054  ...  0.976949   1.744591   \n",
       "Topic6  0.968127  0.878374  1.397568    1.883870  ...  0.871131   3.339024   \n",
       "Topic7  0.834588  0.888630  0.923083    0.821998  ...  0.863983   0.860781   \n",
       "Topic8  0.849665  0.884063  1.129861    0.905808  ...  0.821690   1.597825   \n",
       "Topic9  0.844608  0.869307  0.800188    0.953022  ...  0.815541   1.400704   \n",
       "\n",
       "            ysis      yuan      zeng      zhai     zhang       zhao     zhong  \\\n",
       "Topic0  1.139216  1.058935  0.977944  0.852891  0.836502   0.981387  0.948720   \n",
       "Topic1  2.429478  5.417883  4.534261  3.496922  5.390712  15.615870  3.261350   \n",
       "Topic2  0.831745  0.830315  0.971620  1.055237  0.907475   0.926176  1.008409   \n",
       "Topic3  0.840653  1.102615  0.969379  0.846232  0.886997   1.072033  0.823835   \n",
       "Topic4  0.962506  0.874919  1.036042  1.166660  0.887557   1.622921  0.969050   \n",
       "Topic5  0.690071  1.437621  0.901836  1.078547  1.240224   1.583021  0.860020   \n",
       "Topic6  1.044310  1.041608  1.577248  2.669331  1.820477   3.646023  1.064525   \n",
       "Topic7  0.826731  0.830842  1.021100  0.888345  0.891197   1.007810  0.892581   \n",
       "Topic8  1.024878  1.139745  0.988052  1.119047  1.097812   1.126821  0.974398   \n",
       "Topic9  0.940657  1.038711  1.145261  1.148574  1.073894   1.001945  0.968805   \n",
       "\n",
       "             zhou  \n",
       "Topic0   0.913859  \n",
       "Topic1  13.259086  \n",
       "Topic2   1.266659  \n",
       "Topic3   1.044894  \n",
       "Topic4   1.493889  \n",
       "Topic5   2.013042  \n",
       "Topic6   4.980415  \n",
       "Topic7   1.008698  \n",
       "Topic8   1.226490  \n",
       "Topic9   1.213524  \n",
       "\n",
       "[10 rows x 2060 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "# View\n",
    "df_topic_keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>process</td>\n",
       "      <td>datum</td>\n",
       "      <td>use</td>\n",
       "      <td>time</td>\n",
       "      <td>base</td>\n",
       "      <td>information</td>\n",
       "      <td>mining</td>\n",
       "      <td>application</td>\n",
       "      <td>case</td>\n",
       "      <td>practice</td>\n",
       "      <td>number</td>\n",
       "      <td>business</td>\n",
       "      <td>model</td>\n",
       "      <td>activity</td>\n",
       "      <td>research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>review</td>\n",
       "      <td>datum</td>\n",
       "      <td>study</td>\n",
       "      <td>use</td>\n",
       "      <td>research</td>\n",
       "      <td>mining</td>\n",
       "      <td>base</td>\n",
       "      <td>model</td>\n",
       "      <td>analysis</td>\n",
       "      <td>process</td>\n",
       "      <td>information</td>\n",
       "      <td>method</td>\n",
       "      <td>text</td>\n",
       "      <td>journal</td>\n",
       "      <td>application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>use</td>\n",
       "      <td>study</td>\n",
       "      <td>research</td>\n",
       "      <td>datum</td>\n",
       "      <td>information</td>\n",
       "      <td>model</td>\n",
       "      <td>process</td>\n",
       "      <td>review</td>\n",
       "      <td>base</td>\n",
       "      <td>support</td>\n",
       "      <td>journal</td>\n",
       "      <td>topic</td>\n",
       "      <td>result</td>\n",
       "      <td>analysis</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>review</td>\n",
       "      <td>use</td>\n",
       "      <td>process</td>\n",
       "      <td>analysis</td>\n",
       "      <td>information</td>\n",
       "      <td>study</td>\n",
       "      <td>user</td>\n",
       "      <td>base</td>\n",
       "      <td>mining</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>model</td>\n",
       "      <td>datum</td>\n",
       "      <td>medium</td>\n",
       "      <td>conference</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>study</td>\n",
       "      <td>datum</td>\n",
       "      <td>use</td>\n",
       "      <td>review</td>\n",
       "      <td>patient</td>\n",
       "      <td>text</td>\n",
       "      <td>care</td>\n",
       "      <td>health</td>\n",
       "      <td>model</td>\n",
       "      <td>personality</td>\n",
       "      <td>analysis</td>\n",
       "      <td>result</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>include</td>\n",
       "      <td>research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>use</td>\n",
       "      <td>datum</td>\n",
       "      <td>review</td>\n",
       "      <td>mining</td>\n",
       "      <td>study</td>\n",
       "      <td>fraud</td>\n",
       "      <td>model</td>\n",
       "      <td>base</td>\n",
       "      <td>research</td>\n",
       "      <td>text</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>method</td>\n",
       "      <td>information</td>\n",
       "      <td>journal</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>use</td>\n",
       "      <td>study</td>\n",
       "      <td>text</td>\n",
       "      <td>datum</td>\n",
       "      <td>review</td>\n",
       "      <td>feature</td>\n",
       "      <td>base</td>\n",
       "      <td>technique</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>method</td>\n",
       "      <td>document</td>\n",
       "      <td>research</td>\n",
       "      <td>learn</td>\n",
       "      <td>approach</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>study</td>\n",
       "      <td>approach</td>\n",
       "      <td>base</td>\n",
       "      <td>personality</td>\n",
       "      <td>use</td>\n",
       "      <td>model</td>\n",
       "      <td>datum</td>\n",
       "      <td>research</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>process</td>\n",
       "      <td>application</td>\n",
       "      <td>journal</td>\n",
       "      <td>report</td>\n",
       "      <td>mining</td>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>datum</td>\n",
       "      <td>use</td>\n",
       "      <td>study</td>\n",
       "      <td>model</td>\n",
       "      <td>mining</td>\n",
       "      <td>article</td>\n",
       "      <td>review</td>\n",
       "      <td>research</td>\n",
       "      <td>information</td>\n",
       "      <td>base</td>\n",
       "      <td>quality</td>\n",
       "      <td>technique</td>\n",
       "      <td>text</td>\n",
       "      <td>journal</td>\n",
       "      <td>approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>use</td>\n",
       "      <td>method</td>\n",
       "      <td>text</td>\n",
       "      <td>review</td>\n",
       "      <td>study</td>\n",
       "      <td>datum</td>\n",
       "      <td>base</td>\n",
       "      <td>research</td>\n",
       "      <td>feature</td>\n",
       "      <td>document</td>\n",
       "      <td>information</td>\n",
       "      <td>approach</td>\n",
       "      <td>technique</td>\n",
       "      <td>journal</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word 0    Word 1    Word 2       Word 3       Word 4       Word 5  \\\n",
       "Topic 0  process     datum       use         time         base  information   \n",
       "Topic 1   review     datum     study          use     research       mining   \n",
       "Topic 2      use     study  research        datum  information        model   \n",
       "Topic 3   review       use   process     analysis  information        study   \n",
       "Topic 4    study     datum       use       review      patient         text   \n",
       "Topic 5      use     datum    review       mining        study        fraud   \n",
       "Topic 6      use     study      text        datum       review      feature   \n",
       "Topic 7    study  approach      base  personality          use        model   \n",
       "Topic 8    datum       use     study        model       mining      article   \n",
       "Topic 9      use    method      text       review        study        datum   \n",
       "\n",
       "          Word 6       Word 7       Word 8       Word 9      Word 10  \\\n",
       "Topic 0   mining  application         case     practice       number   \n",
       "Topic 1     base        model     analysis      process  information   \n",
       "Topic 2  process       review         base      support      journal   \n",
       "Topic 3     user         base       mining    sentiment        model   \n",
       "Topic 4     care       health        model  personality     analysis   \n",
       "Topic 5    model         base     research         text    algorithm   \n",
       "Topic 6     base    technique    algorithm       method     document   \n",
       "Topic 7    datum     research    sentiment      process  application   \n",
       "Topic 8   review     research  information         base      quality   \n",
       "Topic 9     base     research      feature     document  information   \n",
       "\n",
       "           Word 11      Word 12     Word 13         Word 14  \n",
       "Topic 0   business        model    activity        research  \n",
       "Topic 1     method         text     journal     application  \n",
       "Topic 2      topic       result    analysis            text  \n",
       "Topic 3      datum       medium  conference            text  \n",
       "Topic 4     result   healthcare     include        research  \n",
       "Topic 5     method  information     journal         machine  \n",
       "Topic 6   research        learn    approach  classification  \n",
       "Topic 7    journal       report      mining          review  \n",
       "Topic 8  technique         text     journal        approach  \n",
       "Topic 9   approach    technique     journal           model  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component's weights: \n",
      " [[ 0.    0.99  0.    0.    0.02  0.01  0.1   0.    0.    0.  ]\n",
      " [ 0.   -0.1   0.    0.    0.03  0.01  0.99  0.    0.    0.  ]]\n",
      "Perc of Variance Explained: \n",
      " [0.39 0.48]\n"
     ]
    }
   ],
   "source": [
    "# Construct the k-means clusters\n",
    "from sklearn.cluster import KMeans\n",
    "clusters = KMeans(n_clusters=5, random_state=100).fit_predict(lda_output)\n",
    "# Build the Singular Value Decomposition(SVD) model\n",
    "svd_model = TruncatedSVD(n_components=2)  # 2 components\n",
    "lda_output_svd = svd_model.fit_transform(lda_output)\n",
    "# X and Y axes of the plot using SVD decomposition\n",
    "x = lda_output_svd[:, 0]\n",
    "y = lda_output_svd[:, 1]\n",
    "# Weights for the 15 columns of lda_output, for each component\n",
    "print(\"Component's weights: \\n\", np.round(svd_model.components_, 2))\n",
    "# Percentage of total information in 'lda_output' explained by the two components\n",
    "print(\"Perc of Variance Explained: \\n\", np.round(svd_model.explained_variance_ratio_, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Segregation of Topic Clusters')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.scatter(x, y, c=clusters)\n",
    "plt.xlabel('Component 2')\n",
    "plt.xlabel('Component 1')\n",
    "plt.title(\"Segregation of Topic Clusters\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('Segregation of topic clusters.png', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['study', 'text', 'datum', 'review', 'feature', 'base', 'technique', 'algorithm', 'method', 'document', 'research', 'learn', 'approach']\n",
      "classification\n"
     ]
    }
   ],
   "source": [
    "# Define function to predict topic for a given text document.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "def predict_topic(text, nlp=nlp):\n",
    "    global sent_to_words\n",
    "    global lemmatization\n",
    "# Step 1: Clean with simple_preprocess\n",
    "    mytext_2 = list(sent_to_words(text))\n",
    "# Step 2: Lemmatize\n",
    "    mytext_3 = lemmatization(mytext_2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "# Step 3: Vectorize transform\n",
    "    mytext_4 = vectorizer.transform(mytext_3)\n",
    "# Step 4: LDA Transform\n",
    "    topic_probability_scores = best_lda_model.transform(mytext_4)\n",
    "    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), 1:14].values.tolist()\n",
    "    \n",
    "    # Step 5: Infer Topic\n",
    "    infer_topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), -1]\n",
    "    \n",
    "    #topic_guess = df_topic_keywords.iloc[np.argmax(topic_probability_scores), Topics]\n",
    "    return infer_topic, topic, topic_probability_scores\n",
    "# Predict the topic\n",
    "mytext = [\"Machine learning algorith future technology\"]\n",
    "infer_topic, topic, prob_scores = predict_topic(text = mytext)\n",
    "print(topic)\n",
    "print(infer_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
